{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 26500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0018867924528301887,
      "grad_norm": 0.4329908788204193,
      "learning_rate": 0.0004998301886792453,
      "loss": 2.8038,
      "step": 10
    },
    {
      "epoch": 0.0037735849056603774,
      "grad_norm": 0.5217053294181824,
      "learning_rate": 0.0004996415094339622,
      "loss": 2.5666,
      "step": 20
    },
    {
      "epoch": 0.005660377358490566,
      "grad_norm": 0.6584075689315796,
      "learning_rate": 0.0004994528301886793,
      "loss": 2.4505,
      "step": 30
    },
    {
      "epoch": 0.007547169811320755,
      "grad_norm": 0.5237374305725098,
      "learning_rate": 0.0004992641509433963,
      "loss": 2.495,
      "step": 40
    },
    {
      "epoch": 0.009433962264150943,
      "grad_norm": 0.5433014631271362,
      "learning_rate": 0.0004990754716981133,
      "loss": 2.3784,
      "step": 50
    },
    {
      "epoch": 0.011320754716981131,
      "grad_norm": 0.6052990555763245,
      "learning_rate": 0.0004988867924528302,
      "loss": 2.3863,
      "step": 60
    },
    {
      "epoch": 0.013207547169811321,
      "grad_norm": 0.6654027700424194,
      "learning_rate": 0.0004986981132075472,
      "loss": 2.2606,
      "step": 70
    },
    {
      "epoch": 0.01509433962264151,
      "grad_norm": 0.7887279987335205,
      "learning_rate": 0.0004985094339622642,
      "loss": 2.2494,
      "step": 80
    },
    {
      "epoch": 0.016981132075471698,
      "grad_norm": 0.5692384839057922,
      "learning_rate": 0.0004983207547169811,
      "loss": 2.2898,
      "step": 90
    },
    {
      "epoch": 0.018867924528301886,
      "grad_norm": 0.6065964102745056,
      "learning_rate": 0.0004981320754716981,
      "loss": 2.3017,
      "step": 100
    },
    {
      "epoch": 0.020754716981132074,
      "grad_norm": 0.6356387138366699,
      "learning_rate": 0.0004979433962264151,
      "loss": 2.1961,
      "step": 110
    },
    {
      "epoch": 0.022641509433962263,
      "grad_norm": 0.6024534106254578,
      "learning_rate": 0.000497754716981132,
      "loss": 2.303,
      "step": 120
    },
    {
      "epoch": 0.024528301886792454,
      "grad_norm": 0.5940855741500854,
      "learning_rate": 0.0004975660377358491,
      "loss": 2.1881,
      "step": 130
    },
    {
      "epoch": 0.026415094339622643,
      "grad_norm": 0.8111462593078613,
      "learning_rate": 0.000497377358490566,
      "loss": 2.2001,
      "step": 140
    },
    {
      "epoch": 0.02830188679245283,
      "grad_norm": 0.823147714138031,
      "learning_rate": 0.000497188679245283,
      "loss": 2.1845,
      "step": 150
    },
    {
      "epoch": 0.03018867924528302,
      "grad_norm": 0.6648862957954407,
      "learning_rate": 0.000497,
      "loss": 2.2736,
      "step": 160
    },
    {
      "epoch": 0.03207547169811321,
      "grad_norm": 0.6599627137184143,
      "learning_rate": 0.000496811320754717,
      "loss": 2.1487,
      "step": 170
    },
    {
      "epoch": 0.033962264150943396,
      "grad_norm": 0.7250626683235168,
      "learning_rate": 0.000496622641509434,
      "loss": 2.2715,
      "step": 180
    },
    {
      "epoch": 0.035849056603773584,
      "grad_norm": 0.7913130521774292,
      "learning_rate": 0.000496433962264151,
      "loss": 2.1516,
      "step": 190
    },
    {
      "epoch": 0.03773584905660377,
      "grad_norm": 0.6797475218772888,
      "learning_rate": 0.000496245283018868,
      "loss": 2.1528,
      "step": 200
    },
    {
      "epoch": 0.03962264150943396,
      "grad_norm": 0.6994564533233643,
      "learning_rate": 0.0004960566037735849,
      "loss": 2.2122,
      "step": 210
    },
    {
      "epoch": 0.04150943396226415,
      "grad_norm": 0.6238529086112976,
      "learning_rate": 0.0004958679245283019,
      "loss": 2.2076,
      "step": 220
    },
    {
      "epoch": 0.04339622641509434,
      "grad_norm": 0.6667708158493042,
      "learning_rate": 0.0004956792452830189,
      "loss": 2.1468,
      "step": 230
    },
    {
      "epoch": 0.045283018867924525,
      "grad_norm": 0.6362404227256775,
      "learning_rate": 0.0004954905660377358,
      "loss": 2.1609,
      "step": 240
    },
    {
      "epoch": 0.04716981132075472,
      "grad_norm": 0.6205162405967712,
      "learning_rate": 0.0004953018867924529,
      "loss": 2.2056,
      "step": 250
    },
    {
      "epoch": 0.04905660377358491,
      "grad_norm": 0.7212279438972473,
      "learning_rate": 0.0004951132075471698,
      "loss": 2.1392,
      "step": 260
    },
    {
      "epoch": 0.0509433962264151,
      "grad_norm": 0.7404685020446777,
      "learning_rate": 0.0004949245283018868,
      "loss": 2.1023,
      "step": 270
    },
    {
      "epoch": 0.052830188679245285,
      "grad_norm": 0.617750883102417,
      "learning_rate": 0.0004947358490566038,
      "loss": 2.1694,
      "step": 280
    },
    {
      "epoch": 0.05471698113207547,
      "grad_norm": 0.692915141582489,
      "learning_rate": 0.0004945471698113207,
      "loss": 2.0952,
      "step": 290
    },
    {
      "epoch": 0.05660377358490566,
      "grad_norm": 0.7894744277000427,
      "learning_rate": 0.0004943584905660377,
      "loss": 2.1239,
      "step": 300
    },
    {
      "epoch": 0.05849056603773585,
      "grad_norm": 0.8011015057563782,
      "learning_rate": 0.0004941698113207548,
      "loss": 2.0934,
      "step": 310
    },
    {
      "epoch": 0.06037735849056604,
      "grad_norm": 0.7109010219573975,
      "learning_rate": 0.0004939811320754717,
      "loss": 2.117,
      "step": 320
    },
    {
      "epoch": 0.062264150943396226,
      "grad_norm": 0.6491928100585938,
      "learning_rate": 0.0004937924528301887,
      "loss": 2.154,
      "step": 330
    },
    {
      "epoch": 0.06415094339622641,
      "grad_norm": 0.8640464544296265,
      "learning_rate": 0.0004936037735849057,
      "loss": 2.068,
      "step": 340
    },
    {
      "epoch": 0.0660377358490566,
      "grad_norm": 0.7572539448738098,
      "learning_rate": 0.0004934150943396227,
      "loss": 2.122,
      "step": 350
    },
    {
      "epoch": 0.06792452830188679,
      "grad_norm": 0.8139703273773193,
      "learning_rate": 0.0004932264150943396,
      "loss": 2.1313,
      "step": 360
    },
    {
      "epoch": 0.06981132075471698,
      "grad_norm": 0.6318525075912476,
      "learning_rate": 0.0004930377358490566,
      "loss": 2.1142,
      "step": 370
    },
    {
      "epoch": 0.07169811320754717,
      "grad_norm": 0.5916596055030823,
      "learning_rate": 0.0004928490566037736,
      "loss": 2.121,
      "step": 380
    },
    {
      "epoch": 0.07358490566037736,
      "grad_norm": 0.7407673001289368,
      "learning_rate": 0.0004926603773584906,
      "loss": 2.2595,
      "step": 390
    },
    {
      "epoch": 0.07547169811320754,
      "grad_norm": 0.5900154709815979,
      "learning_rate": 0.0004924716981132076,
      "loss": 2.181,
      "step": 400
    },
    {
      "epoch": 0.07735849056603773,
      "grad_norm": 0.6433875560760498,
      "learning_rate": 0.0004922830188679245,
      "loss": 2.2151,
      "step": 410
    },
    {
      "epoch": 0.07924528301886792,
      "grad_norm": 0.5971184968948364,
      "learning_rate": 0.0004920943396226415,
      "loss": 2.1733,
      "step": 420
    },
    {
      "epoch": 0.08113207547169811,
      "grad_norm": 0.7337995767593384,
      "learning_rate": 0.0004919056603773585,
      "loss": 2.1286,
      "step": 430
    },
    {
      "epoch": 0.0830188679245283,
      "grad_norm": 0.6341373920440674,
      "learning_rate": 0.0004917169811320754,
      "loss": 2.12,
      "step": 440
    },
    {
      "epoch": 0.08490566037735849,
      "grad_norm": 0.6479591131210327,
      "learning_rate": 0.0004915283018867924,
      "loss": 2.0315,
      "step": 450
    },
    {
      "epoch": 0.08679245283018867,
      "grad_norm": 0.6608479619026184,
      "learning_rate": 0.0004913396226415095,
      "loss": 2.1391,
      "step": 460
    },
    {
      "epoch": 0.08867924528301886,
      "grad_norm": 0.9329798221588135,
      "learning_rate": 0.0004911509433962264,
      "loss": 2.0354,
      "step": 470
    },
    {
      "epoch": 0.09056603773584905,
      "grad_norm": 0.6774038076400757,
      "learning_rate": 0.0004909622641509434,
      "loss": 2.1151,
      "step": 480
    },
    {
      "epoch": 0.09245283018867924,
      "grad_norm": 0.6503973603248596,
      "learning_rate": 0.0004907735849056604,
      "loss": 2.1236,
      "step": 490
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 0.7442273497581482,
      "learning_rate": 0.0004905849056603774,
      "loss": 2.0544,
      "step": 500
    },
    {
      "epoch": 0.09622641509433963,
      "grad_norm": 0.7993296980857849,
      "learning_rate": 0.0004903962264150944,
      "loss": 2.1682,
      "step": 510
    },
    {
      "epoch": 0.09811320754716982,
      "grad_norm": 0.6605177521705627,
      "learning_rate": 0.0004902075471698113,
      "loss": 2.1265,
      "step": 520
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.6250860691070557,
      "learning_rate": 0.0004900188679245283,
      "loss": 2.0747,
      "step": 530
    },
    {
      "epoch": 0.1018867924528302,
      "grad_norm": 0.6716232895851135,
      "learning_rate": 0.0004898301886792453,
      "loss": 2.0425,
      "step": 540
    },
    {
      "epoch": 0.10377358490566038,
      "grad_norm": 0.6738515496253967,
      "learning_rate": 0.0004896415094339623,
      "loss": 2.0356,
      "step": 550
    },
    {
      "epoch": 0.10566037735849057,
      "grad_norm": 0.6080332398414612,
      "learning_rate": 0.0004894528301886792,
      "loss": 2.061,
      "step": 560
    },
    {
      "epoch": 0.10754716981132076,
      "grad_norm": 0.7015767097473145,
      "learning_rate": 0.0004892641509433962,
      "loss": 2.1775,
      "step": 570
    },
    {
      "epoch": 0.10943396226415095,
      "grad_norm": 0.6045324206352234,
      "learning_rate": 0.0004890754716981132,
      "loss": 2.143,
      "step": 580
    },
    {
      "epoch": 0.11132075471698114,
      "grad_norm": 0.6237601041793823,
      "learning_rate": 0.0004888867924528301,
      "loss": 2.0613,
      "step": 590
    },
    {
      "epoch": 0.11320754716981132,
      "grad_norm": 0.771350622177124,
      "learning_rate": 0.0004886981132075472,
      "loss": 2.1839,
      "step": 600
    },
    {
      "epoch": 0.11509433962264151,
      "grad_norm": 0.7436115741729736,
      "learning_rate": 0.0004885094339622642,
      "loss": 2.0665,
      "step": 610
    },
    {
      "epoch": 0.1169811320754717,
      "grad_norm": 0.7759642601013184,
      "learning_rate": 0.0004883207547169812,
      "loss": 2.1149,
      "step": 620
    },
    {
      "epoch": 0.11886792452830189,
      "grad_norm": 0.7326511740684509,
      "learning_rate": 0.0004881320754716981,
      "loss": 2.1103,
      "step": 630
    },
    {
      "epoch": 0.12075471698113208,
      "grad_norm": 0.6966639757156372,
      "learning_rate": 0.0004879433962264151,
      "loss": 2.1407,
      "step": 640
    },
    {
      "epoch": 0.12264150943396226,
      "grad_norm": 0.6835142374038696,
      "learning_rate": 0.0004877547169811321,
      "loss": 2.0456,
      "step": 650
    },
    {
      "epoch": 0.12452830188679245,
      "grad_norm": 0.7153153419494629,
      "learning_rate": 0.00048756603773584905,
      "loss": 2.167,
      "step": 660
    },
    {
      "epoch": 0.12641509433962264,
      "grad_norm": 0.6384267807006836,
      "learning_rate": 0.000487377358490566,
      "loss": 2.0948,
      "step": 670
    },
    {
      "epoch": 0.12830188679245283,
      "grad_norm": 0.6769301891326904,
      "learning_rate": 0.00048718867924528306,
      "loss": 2.0533,
      "step": 680
    },
    {
      "epoch": 0.13018867924528302,
      "grad_norm": 0.853183925151825,
      "learning_rate": 0.000487,
      "loss": 2.0755,
      "step": 690
    },
    {
      "epoch": 0.1320754716981132,
      "grad_norm": 0.6044803261756897,
      "learning_rate": 0.00048681132075471703,
      "loss": 2.0641,
      "step": 700
    },
    {
      "epoch": 0.1339622641509434,
      "grad_norm": 0.7080063223838806,
      "learning_rate": 0.000486622641509434,
      "loss": 2.0889,
      "step": 710
    },
    {
      "epoch": 0.13584905660377358,
      "grad_norm": 0.6377431154251099,
      "learning_rate": 0.00048643396226415094,
      "loss": 2.0747,
      "step": 720
    },
    {
      "epoch": 0.13773584905660377,
      "grad_norm": 0.7512010931968689,
      "learning_rate": 0.00048624528301886795,
      "loss": 1.9838,
      "step": 730
    },
    {
      "epoch": 0.13962264150943396,
      "grad_norm": 0.6343730092048645,
      "learning_rate": 0.0004860566037735849,
      "loss": 2.1806,
      "step": 740
    },
    {
      "epoch": 0.14150943396226415,
      "grad_norm": 0.7129020094871521,
      "learning_rate": 0.0004858679245283019,
      "loss": 2.017,
      "step": 750
    },
    {
      "epoch": 0.14339622641509434,
      "grad_norm": 0.675538957118988,
      "learning_rate": 0.00048567924528301887,
      "loss": 2.0496,
      "step": 760
    },
    {
      "epoch": 0.14528301886792452,
      "grad_norm": 0.7197304368019104,
      "learning_rate": 0.0004854905660377358,
      "loss": 2.0797,
      "step": 770
    },
    {
      "epoch": 0.1471698113207547,
      "grad_norm": 0.691943883895874,
      "learning_rate": 0.00048530188679245283,
      "loss": 2.0601,
      "step": 780
    },
    {
      "epoch": 0.1490566037735849,
      "grad_norm": 0.9082642197608948,
      "learning_rate": 0.0004851132075471698,
      "loss": 2.1078,
      "step": 790
    },
    {
      "epoch": 0.1509433962264151,
      "grad_norm": 0.6595541834831238,
      "learning_rate": 0.00048492452830188685,
      "loss": 2.0617,
      "step": 800
    },
    {
      "epoch": 0.15283018867924528,
      "grad_norm": 0.6111382842063904,
      "learning_rate": 0.0004847358490566038,
      "loss": 2.1334,
      "step": 810
    },
    {
      "epoch": 0.15471698113207547,
      "grad_norm": 0.6538617014884949,
      "learning_rate": 0.00048454716981132076,
      "loss": 2.0168,
      "step": 820
    },
    {
      "epoch": 0.15660377358490565,
      "grad_norm": 0.6402589082717896,
      "learning_rate": 0.00048435849056603777,
      "loss": 2.0413,
      "step": 830
    },
    {
      "epoch": 0.15849056603773584,
      "grad_norm": 0.630989134311676,
      "learning_rate": 0.0004841698113207547,
      "loss": 2.0948,
      "step": 840
    },
    {
      "epoch": 0.16037735849056603,
      "grad_norm": 0.647290825843811,
      "learning_rate": 0.00048398113207547173,
      "loss": 2.0729,
      "step": 850
    },
    {
      "epoch": 0.16226415094339622,
      "grad_norm": 0.6277814507484436,
      "learning_rate": 0.0004837924528301887,
      "loss": 2.0488,
      "step": 860
    },
    {
      "epoch": 0.1641509433962264,
      "grad_norm": 0.696637749671936,
      "learning_rate": 0.00048360377358490564,
      "loss": 2.1209,
      "step": 870
    },
    {
      "epoch": 0.1660377358490566,
      "grad_norm": 0.8454914689064026,
      "learning_rate": 0.00048341509433962265,
      "loss": 2.0234,
      "step": 880
    },
    {
      "epoch": 0.16792452830188678,
      "grad_norm": 0.6024648547172546,
      "learning_rate": 0.0004832264150943396,
      "loss": 2.0957,
      "step": 890
    },
    {
      "epoch": 0.16981132075471697,
      "grad_norm": 0.6406837701797485,
      "learning_rate": 0.0004830377358490566,
      "loss": 2.0772,
      "step": 900
    },
    {
      "epoch": 0.17169811320754716,
      "grad_norm": 0.6748171448707581,
      "learning_rate": 0.0004828490566037736,
      "loss": 2.0764,
      "step": 910
    },
    {
      "epoch": 0.17358490566037735,
      "grad_norm": 0.6386995315551758,
      "learning_rate": 0.00048266037735849053,
      "loss": 2.2248,
      "step": 920
    },
    {
      "epoch": 0.17547169811320754,
      "grad_norm": 0.6353575587272644,
      "learning_rate": 0.0004824716981132076,
      "loss": 2.1207,
      "step": 930
    },
    {
      "epoch": 0.17735849056603772,
      "grad_norm": 0.6546456813812256,
      "learning_rate": 0.00048228301886792455,
      "loss": 2.0631,
      "step": 940
    },
    {
      "epoch": 0.1792452830188679,
      "grad_norm": 0.6766573190689087,
      "learning_rate": 0.00048209433962264156,
      "loss": 2.0304,
      "step": 950
    },
    {
      "epoch": 0.1811320754716981,
      "grad_norm": 0.6867911219596863,
      "learning_rate": 0.0004819056603773585,
      "loss": 2.072,
      "step": 960
    },
    {
      "epoch": 0.1830188679245283,
      "grad_norm": 0.6896935701370239,
      "learning_rate": 0.00048171698113207547,
      "loss": 2.1051,
      "step": 970
    },
    {
      "epoch": 0.18490566037735848,
      "grad_norm": 0.7829967737197876,
      "learning_rate": 0.0004815283018867925,
      "loss": 2.0739,
      "step": 980
    },
    {
      "epoch": 0.18679245283018867,
      "grad_norm": 0.7724447250366211,
      "learning_rate": 0.00048133962264150943,
      "loss": 2.0776,
      "step": 990
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 0.6826165318489075,
      "learning_rate": 0.00048115094339622644,
      "loss": 2.011,
      "step": 1000
    },
    {
      "epoch": 0.19056603773584907,
      "grad_norm": 0.7042605876922607,
      "learning_rate": 0.0004809622641509434,
      "loss": 2.0326,
      "step": 1010
    },
    {
      "epoch": 0.19245283018867926,
      "grad_norm": 0.7916215062141418,
      "learning_rate": 0.00048077358490566035,
      "loss": 2.0164,
      "step": 1020
    },
    {
      "epoch": 0.19433962264150945,
      "grad_norm": 0.7050279974937439,
      "learning_rate": 0.00048058490566037736,
      "loss": 2.059,
      "step": 1030
    },
    {
      "epoch": 0.19622641509433963,
      "grad_norm": 0.664303719997406,
      "learning_rate": 0.0004803962264150943,
      "loss": 2.075,
      "step": 1040
    },
    {
      "epoch": 0.19811320754716982,
      "grad_norm": 0.7910596132278442,
      "learning_rate": 0.0004802075471698114,
      "loss": 2.0879,
      "step": 1050
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.6554771661758423,
      "learning_rate": 0.00048001886792452833,
      "loss": 2.1199,
      "step": 1060
    },
    {
      "epoch": 0.2018867924528302,
      "grad_norm": 0.6407450437545776,
      "learning_rate": 0.0004798301886792453,
      "loss": 2.073,
      "step": 1070
    },
    {
      "epoch": 0.2037735849056604,
      "grad_norm": 0.7521471381187439,
      "learning_rate": 0.0004796415094339623,
      "loss": 2.0771,
      "step": 1080
    },
    {
      "epoch": 0.20566037735849058,
      "grad_norm": 0.6669948101043701,
      "learning_rate": 0.00047945283018867925,
      "loss": 1.9633,
      "step": 1090
    },
    {
      "epoch": 0.20754716981132076,
      "grad_norm": 0.6292591094970703,
      "learning_rate": 0.00047926415094339626,
      "loss": 2.0742,
      "step": 1100
    },
    {
      "epoch": 0.20943396226415095,
      "grad_norm": 0.7149105072021484,
      "learning_rate": 0.0004790754716981132,
      "loss": 2.0095,
      "step": 1110
    },
    {
      "epoch": 0.21132075471698114,
      "grad_norm": 0.7250792384147644,
      "learning_rate": 0.00047888679245283017,
      "loss": 2.0374,
      "step": 1120
    },
    {
      "epoch": 0.21320754716981133,
      "grad_norm": 0.6802382469177246,
      "learning_rate": 0.0004786981132075472,
      "loss": 2.0649,
      "step": 1130
    },
    {
      "epoch": 0.21509433962264152,
      "grad_norm": 0.681174635887146,
      "learning_rate": 0.00047850943396226414,
      "loss": 2.0521,
      "step": 1140
    },
    {
      "epoch": 0.2169811320754717,
      "grad_norm": 0.6658188104629517,
      "learning_rate": 0.00047832075471698115,
      "loss": 2.0449,
      "step": 1150
    },
    {
      "epoch": 0.2188679245283019,
      "grad_norm": 0.7006884217262268,
      "learning_rate": 0.0004781320754716981,
      "loss": 2.0453,
      "step": 1160
    },
    {
      "epoch": 0.22075471698113208,
      "grad_norm": 0.6990952491760254,
      "learning_rate": 0.0004779433962264151,
      "loss": 1.9266,
      "step": 1170
    },
    {
      "epoch": 0.22264150943396227,
      "grad_norm": 0.6470546722412109,
      "learning_rate": 0.0004777547169811321,
      "loss": 2.0096,
      "step": 1180
    },
    {
      "epoch": 0.22452830188679246,
      "grad_norm": 0.7449393272399902,
      "learning_rate": 0.0004775660377358491,
      "loss": 2.069,
      "step": 1190
    },
    {
      "epoch": 0.22641509433962265,
      "grad_norm": 0.7476711869239807,
      "learning_rate": 0.0004773773584905661,
      "loss": 2.0648,
      "step": 1200
    },
    {
      "epoch": 0.22830188679245284,
      "grad_norm": 0.6796756982803345,
      "learning_rate": 0.00047718867924528304,
      "loss": 2.0595,
      "step": 1210
    },
    {
      "epoch": 0.23018867924528302,
      "grad_norm": 0.6542447805404663,
      "learning_rate": 0.000477,
      "loss": 2.0474,
      "step": 1220
    },
    {
      "epoch": 0.2320754716981132,
      "grad_norm": 0.7169197797775269,
      "learning_rate": 0.000476811320754717,
      "loss": 1.9818,
      "step": 1230
    },
    {
      "epoch": 0.2339622641509434,
      "grad_norm": 0.6486346125602722,
      "learning_rate": 0.00047662264150943396,
      "loss": 2.0628,
      "step": 1240
    },
    {
      "epoch": 0.2358490566037736,
      "grad_norm": 0.6746475100517273,
      "learning_rate": 0.00047643396226415097,
      "loss": 2.0298,
      "step": 1250
    },
    {
      "epoch": 0.23773584905660378,
      "grad_norm": 0.6327669620513916,
      "learning_rate": 0.0004762452830188679,
      "loss": 2.0601,
      "step": 1260
    },
    {
      "epoch": 0.23962264150943396,
      "grad_norm": 0.7076253890991211,
      "learning_rate": 0.0004760566037735849,
      "loss": 2.0536,
      "step": 1270
    },
    {
      "epoch": 0.24150943396226415,
      "grad_norm": 0.7262818813323975,
      "learning_rate": 0.0004758679245283019,
      "loss": 2.0502,
      "step": 1280
    },
    {
      "epoch": 0.24339622641509434,
      "grad_norm": 0.6681029796600342,
      "learning_rate": 0.0004756792452830189,
      "loss": 2.0488,
      "step": 1290
    },
    {
      "epoch": 0.24528301886792453,
      "grad_norm": 0.7640947699546814,
      "learning_rate": 0.0004754905660377359,
      "loss": 2.0543,
      "step": 1300
    },
    {
      "epoch": 0.24716981132075472,
      "grad_norm": 0.7594863772392273,
      "learning_rate": 0.00047530188679245286,
      "loss": 2.032,
      "step": 1310
    },
    {
      "epoch": 0.2490566037735849,
      "grad_norm": 0.8363176584243774,
      "learning_rate": 0.0004751132075471698,
      "loss": 1.9726,
      "step": 1320
    },
    {
      "epoch": 0.2509433962264151,
      "grad_norm": 0.7131612300872803,
      "learning_rate": 0.0004749245283018868,
      "loss": 2.0885,
      "step": 1330
    },
    {
      "epoch": 0.2528301886792453,
      "grad_norm": 0.6202197670936584,
      "learning_rate": 0.0004747358490566038,
      "loss": 1.9929,
      "step": 1340
    },
    {
      "epoch": 0.25471698113207547,
      "grad_norm": 0.7241735458374023,
      "learning_rate": 0.0004745471698113208,
      "loss": 2.0022,
      "step": 1350
    },
    {
      "epoch": 0.25660377358490566,
      "grad_norm": 0.6157837510108948,
      "learning_rate": 0.00047435849056603774,
      "loss": 1.9669,
      "step": 1360
    },
    {
      "epoch": 0.25849056603773585,
      "grad_norm": 0.660758912563324,
      "learning_rate": 0.0004741698113207547,
      "loss": 1.9696,
      "step": 1370
    },
    {
      "epoch": 0.26037735849056604,
      "grad_norm": 0.6871370077133179,
      "learning_rate": 0.0004739811320754717,
      "loss": 2.0934,
      "step": 1380
    },
    {
      "epoch": 0.2622641509433962,
      "grad_norm": 0.7662531733512878,
      "learning_rate": 0.00047379245283018866,
      "loss": 2.0695,
      "step": 1390
    },
    {
      "epoch": 0.2641509433962264,
      "grad_norm": 0.73453289270401,
      "learning_rate": 0.00047360377358490567,
      "loss": 2.0289,
      "step": 1400
    },
    {
      "epoch": 0.2660377358490566,
      "grad_norm": 0.7183501124382019,
      "learning_rate": 0.00047341509433962263,
      "loss": 2.0367,
      "step": 1410
    },
    {
      "epoch": 0.2679245283018868,
      "grad_norm": 0.7940992116928101,
      "learning_rate": 0.00047322641509433964,
      "loss": 2.0941,
      "step": 1420
    },
    {
      "epoch": 0.269811320754717,
      "grad_norm": 0.6828907132148743,
      "learning_rate": 0.00047303773584905665,
      "loss": 2.0845,
      "step": 1430
    },
    {
      "epoch": 0.27169811320754716,
      "grad_norm": 0.7683840394020081,
      "learning_rate": 0.0004728490566037736,
      "loss": 2.0197,
      "step": 1440
    },
    {
      "epoch": 0.27358490566037735,
      "grad_norm": 0.6602147221565247,
      "learning_rate": 0.0004726603773584906,
      "loss": 2.0587,
      "step": 1450
    },
    {
      "epoch": 0.27547169811320754,
      "grad_norm": 0.6364971399307251,
      "learning_rate": 0.00047247169811320757,
      "loss": 2.0087,
      "step": 1460
    },
    {
      "epoch": 0.27735849056603773,
      "grad_norm": 0.6422655582427979,
      "learning_rate": 0.0004722830188679245,
      "loss": 2.0485,
      "step": 1470
    },
    {
      "epoch": 0.2792452830188679,
      "grad_norm": 0.9825193285942078,
      "learning_rate": 0.00047209433962264153,
      "loss": 2.0207,
      "step": 1480
    },
    {
      "epoch": 0.2811320754716981,
      "grad_norm": 0.7331013083457947,
      "learning_rate": 0.0004719056603773585,
      "loss": 1.9677,
      "step": 1490
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 0.7133809328079224,
      "learning_rate": 0.0004717169811320755,
      "loss": 2.0229,
      "step": 1500
    },
    {
      "epoch": 0.2849056603773585,
      "grad_norm": 0.6783145666122437,
      "learning_rate": 0.00047152830188679245,
      "loss": 2.0839,
      "step": 1510
    },
    {
      "epoch": 0.28679245283018867,
      "grad_norm": 0.7049760818481445,
      "learning_rate": 0.0004713396226415094,
      "loss": 1.984,
      "step": 1520
    },
    {
      "epoch": 0.28867924528301886,
      "grad_norm": 0.6426525115966797,
      "learning_rate": 0.0004711509433962264,
      "loss": 2.0194,
      "step": 1530
    },
    {
      "epoch": 0.29056603773584905,
      "grad_norm": 0.8279142379760742,
      "learning_rate": 0.0004709622641509434,
      "loss": 1.9901,
      "step": 1540
    },
    {
      "epoch": 0.29245283018867924,
      "grad_norm": 0.647354781627655,
      "learning_rate": 0.00047077358490566043,
      "loss": 1.9295,
      "step": 1550
    },
    {
      "epoch": 0.2943396226415094,
      "grad_norm": 0.8310421705245972,
      "learning_rate": 0.0004705849056603774,
      "loss": 2.0154,
      "step": 1560
    },
    {
      "epoch": 0.2962264150943396,
      "grad_norm": 0.6199637651443481,
      "learning_rate": 0.00047039622641509434,
      "loss": 2.0608,
      "step": 1570
    },
    {
      "epoch": 0.2981132075471698,
      "grad_norm": 0.6286132335662842,
      "learning_rate": 0.00047020754716981135,
      "loss": 1.9732,
      "step": 1580
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.6967529654502869,
      "learning_rate": 0.0004700188679245283,
      "loss": 2.0554,
      "step": 1590
    },
    {
      "epoch": 0.3018867924528302,
      "grad_norm": 0.651172399520874,
      "learning_rate": 0.0004698301886792453,
      "loss": 2.0495,
      "step": 1600
    },
    {
      "epoch": 0.30377358490566037,
      "grad_norm": 0.7221119999885559,
      "learning_rate": 0.00046964150943396227,
      "loss": 2.0307,
      "step": 1610
    },
    {
      "epoch": 0.30566037735849055,
      "grad_norm": 0.6522427797317505,
      "learning_rate": 0.0004694528301886792,
      "loss": 1.979,
      "step": 1620
    },
    {
      "epoch": 0.30754716981132074,
      "grad_norm": 0.7742286324501038,
      "learning_rate": 0.00046926415094339624,
      "loss": 2.0186,
      "step": 1630
    },
    {
      "epoch": 0.30943396226415093,
      "grad_norm": 0.7654798030853271,
      "learning_rate": 0.0004690754716981132,
      "loss": 2.024,
      "step": 1640
    },
    {
      "epoch": 0.3113207547169811,
      "grad_norm": 0.7883172631263733,
      "learning_rate": 0.0004688867924528302,
      "loss": 2.0332,
      "step": 1650
    },
    {
      "epoch": 0.3132075471698113,
      "grad_norm": 0.6851376891136169,
      "learning_rate": 0.0004686981132075472,
      "loss": 2.0468,
      "step": 1660
    },
    {
      "epoch": 0.3150943396226415,
      "grad_norm": 0.7214307188987732,
      "learning_rate": 0.00046850943396226416,
      "loss": 2.0898,
      "step": 1670
    },
    {
      "epoch": 0.3169811320754717,
      "grad_norm": 0.6723283529281616,
      "learning_rate": 0.0004683207547169812,
      "loss": 1.9683,
      "step": 1680
    },
    {
      "epoch": 0.31886792452830187,
      "grad_norm": 0.6638851165771484,
      "learning_rate": 0.00046813207547169813,
      "loss": 1.9694,
      "step": 1690
    },
    {
      "epoch": 0.32075471698113206,
      "grad_norm": 0.7381642460823059,
      "learning_rate": 0.00046794339622641514,
      "loss": 1.9793,
      "step": 1700
    },
    {
      "epoch": 0.32264150943396225,
      "grad_norm": 0.6604354381561279,
      "learning_rate": 0.0004677547169811321,
      "loss": 2.0432,
      "step": 1710
    },
    {
      "epoch": 0.32452830188679244,
      "grad_norm": 0.6812741160392761,
      "learning_rate": 0.00046756603773584905,
      "loss": 1.9589,
      "step": 1720
    },
    {
      "epoch": 0.3264150943396226,
      "grad_norm": 0.6968291997909546,
      "learning_rate": 0.00046737735849056606,
      "loss": 2.0987,
      "step": 1730
    },
    {
      "epoch": 0.3283018867924528,
      "grad_norm": 0.8094509243965149,
      "learning_rate": 0.000467188679245283,
      "loss": 2.0236,
      "step": 1740
    },
    {
      "epoch": 0.330188679245283,
      "grad_norm": 0.6511431932449341,
      "learning_rate": 0.000467,
      "loss": 2.0514,
      "step": 1750
    },
    {
      "epoch": 0.3320754716981132,
      "grad_norm": 0.746353268623352,
      "learning_rate": 0.000466811320754717,
      "loss": 2.0377,
      "step": 1760
    },
    {
      "epoch": 0.3339622641509434,
      "grad_norm": 0.7112246155738831,
      "learning_rate": 0.00046662264150943393,
      "loss": 2.0258,
      "step": 1770
    },
    {
      "epoch": 0.33584905660377357,
      "grad_norm": 0.6727315187454224,
      "learning_rate": 0.000466433962264151,
      "loss": 1.9673,
      "step": 1780
    },
    {
      "epoch": 0.33773584905660375,
      "grad_norm": 0.658154308795929,
      "learning_rate": 0.00046624528301886795,
      "loss": 2.0699,
      "step": 1790
    },
    {
      "epoch": 0.33962264150943394,
      "grad_norm": 0.6612434983253479,
      "learning_rate": 0.00046605660377358496,
      "loss": 1.9339,
      "step": 1800
    },
    {
      "epoch": 0.34150943396226413,
      "grad_norm": 0.7371506094932556,
      "learning_rate": 0.0004658679245283019,
      "loss": 1.9473,
      "step": 1810
    },
    {
      "epoch": 0.3433962264150943,
      "grad_norm": 0.7532640099525452,
      "learning_rate": 0.00046567924528301887,
      "loss": 1.9999,
      "step": 1820
    },
    {
      "epoch": 0.3452830188679245,
      "grad_norm": 0.6548904180526733,
      "learning_rate": 0.0004654905660377359,
      "loss": 2.1341,
      "step": 1830
    },
    {
      "epoch": 0.3471698113207547,
      "grad_norm": 0.6820883750915527,
      "learning_rate": 0.00046530188679245283,
      "loss": 2.0216,
      "step": 1840
    },
    {
      "epoch": 0.3490566037735849,
      "grad_norm": 0.6557729840278625,
      "learning_rate": 0.00046511320754716984,
      "loss": 2.0292,
      "step": 1850
    },
    {
      "epoch": 0.35094339622641507,
      "grad_norm": 0.7124652862548828,
      "learning_rate": 0.0004649245283018868,
      "loss": 2.0635,
      "step": 1860
    },
    {
      "epoch": 0.35283018867924526,
      "grad_norm": 0.6795564293861389,
      "learning_rate": 0.00046473584905660375,
      "loss": 2.0108,
      "step": 1870
    },
    {
      "epoch": 0.35471698113207545,
      "grad_norm": 0.6937212347984314,
      "learning_rate": 0.00046454716981132076,
      "loss": 2.0218,
      "step": 1880
    },
    {
      "epoch": 0.35660377358490564,
      "grad_norm": 0.7873280644416809,
      "learning_rate": 0.0004643584905660377,
      "loss": 1.9569,
      "step": 1890
    },
    {
      "epoch": 0.3584905660377358,
      "grad_norm": 0.6964331865310669,
      "learning_rate": 0.0004641698113207547,
      "loss": 1.9654,
      "step": 1900
    },
    {
      "epoch": 0.360377358490566,
      "grad_norm": 0.6660976409912109,
      "learning_rate": 0.00046398113207547174,
      "loss": 2.0336,
      "step": 1910
    },
    {
      "epoch": 0.3622641509433962,
      "grad_norm": 0.6973721981048584,
      "learning_rate": 0.0004637924528301887,
      "loss": 2.0962,
      "step": 1920
    },
    {
      "epoch": 0.3641509433962264,
      "grad_norm": 0.6942188143730164,
      "learning_rate": 0.0004636037735849057,
      "loss": 2.0967,
      "step": 1930
    },
    {
      "epoch": 0.3660377358490566,
      "grad_norm": 0.7443339824676514,
      "learning_rate": 0.00046341509433962266,
      "loss": 2.0892,
      "step": 1940
    },
    {
      "epoch": 0.36792452830188677,
      "grad_norm": 0.687900185585022,
      "learning_rate": 0.00046322641509433966,
      "loss": 1.9578,
      "step": 1950
    },
    {
      "epoch": 0.36981132075471695,
      "grad_norm": 0.674553394317627,
      "learning_rate": 0.0004630377358490566,
      "loss": 1.9723,
      "step": 1960
    },
    {
      "epoch": 0.37169811320754714,
      "grad_norm": 0.6515602469444275,
      "learning_rate": 0.0004628490566037736,
      "loss": 2.0278,
      "step": 1970
    },
    {
      "epoch": 0.37358490566037733,
      "grad_norm": 0.7542306184768677,
      "learning_rate": 0.0004626603773584906,
      "loss": 2.0371,
      "step": 1980
    },
    {
      "epoch": 0.3754716981132076,
      "grad_norm": 0.674730122089386,
      "learning_rate": 0.00046247169811320754,
      "loss": 1.8925,
      "step": 1990
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 0.6074073314666748,
      "learning_rate": 0.00046228301886792455,
      "loss": 2.0176,
      "step": 2000
    },
    {
      "epoch": 0.37924528301886795,
      "grad_norm": 0.7341790199279785,
      "learning_rate": 0.0004620943396226415,
      "loss": 2.0605,
      "step": 2010
    },
    {
      "epoch": 0.38113207547169814,
      "grad_norm": 0.7026288509368896,
      "learning_rate": 0.00046190566037735846,
      "loss": 1.9466,
      "step": 2020
    },
    {
      "epoch": 0.38301886792452833,
      "grad_norm": 0.7883652448654175,
      "learning_rate": 0.0004617169811320755,
      "loss": 1.97,
      "step": 2030
    },
    {
      "epoch": 0.3849056603773585,
      "grad_norm": 0.7481223940849304,
      "learning_rate": 0.0004615283018867925,
      "loss": 2.0772,
      "step": 2040
    },
    {
      "epoch": 0.3867924528301887,
      "grad_norm": 0.658379852771759,
      "learning_rate": 0.0004613396226415095,
      "loss": 2.04,
      "step": 2050
    },
    {
      "epoch": 0.3886792452830189,
      "grad_norm": 0.761544406414032,
      "learning_rate": 0.00046115094339622644,
      "loss": 1.9609,
      "step": 2060
    },
    {
      "epoch": 0.3905660377358491,
      "grad_norm": 0.6498782634735107,
      "learning_rate": 0.0004609622641509434,
      "loss": 1.9063,
      "step": 2070
    },
    {
      "epoch": 0.39245283018867927,
      "grad_norm": 0.608025312423706,
      "learning_rate": 0.0004607735849056604,
      "loss": 2.0057,
      "step": 2080
    },
    {
      "epoch": 0.39433962264150946,
      "grad_norm": 0.6266374588012695,
      "learning_rate": 0.00046058490566037736,
      "loss": 1.9585,
      "step": 2090
    },
    {
      "epoch": 0.39622641509433965,
      "grad_norm": 0.6231977939605713,
      "learning_rate": 0.00046039622641509437,
      "loss": 2.0637,
      "step": 2100
    },
    {
      "epoch": 0.39811320754716983,
      "grad_norm": 0.8261261582374573,
      "learning_rate": 0.0004602075471698113,
      "loss": 2.0757,
      "step": 2110
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.7068877816200256,
      "learning_rate": 0.0004600188679245283,
      "loss": 1.9604,
      "step": 2120
    },
    {
      "epoch": 0.4018867924528302,
      "grad_norm": 0.6626759767532349,
      "learning_rate": 0.0004598301886792453,
      "loss": 2.0079,
      "step": 2130
    },
    {
      "epoch": 0.4037735849056604,
      "grad_norm": 0.7596105337142944,
      "learning_rate": 0.00045964150943396224,
      "loss": 1.9471,
      "step": 2140
    },
    {
      "epoch": 0.4056603773584906,
      "grad_norm": 0.6980749368667603,
      "learning_rate": 0.00045945283018867925,
      "loss": 2.1337,
      "step": 2150
    },
    {
      "epoch": 0.4075471698113208,
      "grad_norm": 0.6534703373908997,
      "learning_rate": 0.00045926415094339626,
      "loss": 1.9234,
      "step": 2160
    },
    {
      "epoch": 0.40943396226415096,
      "grad_norm": 0.7036799192428589,
      "learning_rate": 0.0004590754716981132,
      "loss": 1.9677,
      "step": 2170
    },
    {
      "epoch": 0.41132075471698115,
      "grad_norm": 0.735885500907898,
      "learning_rate": 0.00045888679245283023,
      "loss": 1.9424,
      "step": 2180
    },
    {
      "epoch": 0.41320754716981134,
      "grad_norm": 0.6811690330505371,
      "learning_rate": 0.0004586981132075472,
      "loss": 2.0381,
      "step": 2190
    },
    {
      "epoch": 0.41509433962264153,
      "grad_norm": 0.6665072441101074,
      "learning_rate": 0.00045850943396226414,
      "loss": 1.9503,
      "step": 2200
    },
    {
      "epoch": 0.4169811320754717,
      "grad_norm": 0.6381679177284241,
      "learning_rate": 0.00045832075471698115,
      "loss": 1.9601,
      "step": 2210
    },
    {
      "epoch": 0.4188679245283019,
      "grad_norm": 0.6944700479507446,
      "learning_rate": 0.0004581320754716981,
      "loss": 2.0154,
      "step": 2220
    },
    {
      "epoch": 0.4207547169811321,
      "grad_norm": 0.7448076009750366,
      "learning_rate": 0.0004579433962264151,
      "loss": 2.0428,
      "step": 2230
    },
    {
      "epoch": 0.4226415094339623,
      "grad_norm": 0.7377538681030273,
      "learning_rate": 0.00045775471698113207,
      "loss": 2.0282,
      "step": 2240
    },
    {
      "epoch": 0.42452830188679247,
      "grad_norm": 0.7113017439842224,
      "learning_rate": 0.000457566037735849,
      "loss": 2.0018,
      "step": 2250
    },
    {
      "epoch": 0.42641509433962266,
      "grad_norm": 0.7509166598320007,
      "learning_rate": 0.00045737735849056603,
      "loss": 1.9737,
      "step": 2260
    },
    {
      "epoch": 0.42830188679245285,
      "grad_norm": 0.6163308024406433,
      "learning_rate": 0.000457188679245283,
      "loss": 1.9972,
      "step": 2270
    },
    {
      "epoch": 0.43018867924528303,
      "grad_norm": 0.8948588371276855,
      "learning_rate": 0.00045700000000000005,
      "loss": 1.9946,
      "step": 2280
    },
    {
      "epoch": 0.4320754716981132,
      "grad_norm": 0.7252004146575928,
      "learning_rate": 0.000456811320754717,
      "loss": 1.9732,
      "step": 2290
    },
    {
      "epoch": 0.4339622641509434,
      "grad_norm": 0.7708209156990051,
      "learning_rate": 0.00045662264150943396,
      "loss": 2.006,
      "step": 2300
    },
    {
      "epoch": 0.4358490566037736,
      "grad_norm": 0.6608397364616394,
      "learning_rate": 0.00045643396226415097,
      "loss": 2.0558,
      "step": 2310
    },
    {
      "epoch": 0.4377358490566038,
      "grad_norm": 0.6390059590339661,
      "learning_rate": 0.0004562452830188679,
      "loss": 2.0781,
      "step": 2320
    },
    {
      "epoch": 0.439622641509434,
      "grad_norm": 0.761458158493042,
      "learning_rate": 0.00045605660377358493,
      "loss": 1.9128,
      "step": 2330
    },
    {
      "epoch": 0.44150943396226416,
      "grad_norm": 0.5841718912124634,
      "learning_rate": 0.0004558679245283019,
      "loss": 1.9938,
      "step": 2340
    },
    {
      "epoch": 0.44339622641509435,
      "grad_norm": 0.6831333637237549,
      "learning_rate": 0.00045567924528301884,
      "loss": 2.0484,
      "step": 2350
    },
    {
      "epoch": 0.44528301886792454,
      "grad_norm": 0.7074206471443176,
      "learning_rate": 0.00045549056603773585,
      "loss": 1.9413,
      "step": 2360
    },
    {
      "epoch": 0.44716981132075473,
      "grad_norm": 0.6785129904747009,
      "learning_rate": 0.0004553018867924528,
      "loss": 1.9606,
      "step": 2370
    },
    {
      "epoch": 0.4490566037735849,
      "grad_norm": 0.6917687654495239,
      "learning_rate": 0.0004551132075471698,
      "loss": 2.043,
      "step": 2380
    },
    {
      "epoch": 0.4509433962264151,
      "grad_norm": 0.8019599914550781,
      "learning_rate": 0.00045492452830188677,
      "loss": 2.0011,
      "step": 2390
    },
    {
      "epoch": 0.4528301886792453,
      "grad_norm": 0.6953371167182922,
      "learning_rate": 0.0004547358490566038,
      "loss": 1.9937,
      "step": 2400
    },
    {
      "epoch": 0.4547169811320755,
      "grad_norm": 0.7292705774307251,
      "learning_rate": 0.0004545471698113208,
      "loss": 1.9503,
      "step": 2410
    },
    {
      "epoch": 0.45660377358490567,
      "grad_norm": 0.7122235894203186,
      "learning_rate": 0.00045435849056603775,
      "loss": 2.0065,
      "step": 2420
    },
    {
      "epoch": 0.45849056603773586,
      "grad_norm": 0.6628443598747253,
      "learning_rate": 0.00045416981132075475,
      "loss": 1.9869,
      "step": 2430
    },
    {
      "epoch": 0.46037735849056605,
      "grad_norm": 0.8591076731681824,
      "learning_rate": 0.0004539811320754717,
      "loss": 1.9084,
      "step": 2440
    },
    {
      "epoch": 0.46226415094339623,
      "grad_norm": 0.7521952986717224,
      "learning_rate": 0.00045379245283018867,
      "loss": 1.9912,
      "step": 2450
    },
    {
      "epoch": 0.4641509433962264,
      "grad_norm": 0.692969799041748,
      "learning_rate": 0.0004536037735849057,
      "loss": 1.9954,
      "step": 2460
    },
    {
      "epoch": 0.4660377358490566,
      "grad_norm": 0.6840605735778809,
      "learning_rate": 0.00045341509433962263,
      "loss": 1.9889,
      "step": 2470
    },
    {
      "epoch": 0.4679245283018868,
      "grad_norm": 0.7089123725891113,
      "learning_rate": 0.00045322641509433964,
      "loss": 1.959,
      "step": 2480
    },
    {
      "epoch": 0.469811320754717,
      "grad_norm": 0.6656343340873718,
      "learning_rate": 0.0004530377358490566,
      "loss": 1.8472,
      "step": 2490
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 0.7761107683181763,
      "learning_rate": 0.00045284905660377355,
      "loss": 2.0215,
      "step": 2500
    },
    {
      "epoch": 0.47358490566037736,
      "grad_norm": 0.7117366790771484,
      "learning_rate": 0.00045266037735849056,
      "loss": 2.0067,
      "step": 2510
    },
    {
      "epoch": 0.47547169811320755,
      "grad_norm": 0.6731598377227783,
      "learning_rate": 0.00045247169811320757,
      "loss": 1.9798,
      "step": 2520
    },
    {
      "epoch": 0.47735849056603774,
      "grad_norm": 0.7711283564567566,
      "learning_rate": 0.0004522830188679246,
      "loss": 2.0216,
      "step": 2530
    },
    {
      "epoch": 0.47924528301886793,
      "grad_norm": 0.7091416716575623,
      "learning_rate": 0.00045209433962264153,
      "loss": 2.0281,
      "step": 2540
    },
    {
      "epoch": 0.4811320754716981,
      "grad_norm": 0.7250251770019531,
      "learning_rate": 0.0004519056603773585,
      "loss": 1.9383,
      "step": 2550
    },
    {
      "epoch": 0.4830188679245283,
      "grad_norm": 0.6896761059761047,
      "learning_rate": 0.0004517169811320755,
      "loss": 2.0863,
      "step": 2560
    },
    {
      "epoch": 0.4849056603773585,
      "grad_norm": 0.6573197841644287,
      "learning_rate": 0.00045152830188679245,
      "loss": 1.9517,
      "step": 2570
    },
    {
      "epoch": 0.4867924528301887,
      "grad_norm": 0.7708040475845337,
      "learning_rate": 0.00045133962264150946,
      "loss": 1.938,
      "step": 2580
    },
    {
      "epoch": 0.48867924528301887,
      "grad_norm": 0.7375487089157104,
      "learning_rate": 0.0004511509433962264,
      "loss": 1.9815,
      "step": 2590
    },
    {
      "epoch": 0.49056603773584906,
      "grad_norm": 0.7062280774116516,
      "learning_rate": 0.00045096226415094337,
      "loss": 2.0316,
      "step": 2600
    },
    {
      "epoch": 0.49245283018867925,
      "grad_norm": 0.6708307266235352,
      "learning_rate": 0.0004507735849056604,
      "loss": 1.9739,
      "step": 2610
    },
    {
      "epoch": 0.49433962264150944,
      "grad_norm": 0.6809203624725342,
      "learning_rate": 0.00045058490566037733,
      "loss": 2.033,
      "step": 2620
    },
    {
      "epoch": 0.4962264150943396,
      "grad_norm": 0.6516367793083191,
      "learning_rate": 0.00045039622641509434,
      "loss": 2.0355,
      "step": 2630
    },
    {
      "epoch": 0.4981132075471698,
      "grad_norm": 0.793682873249054,
      "learning_rate": 0.00045020754716981135,
      "loss": 1.9823,
      "step": 2640
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.6694377660751343,
      "learning_rate": 0.0004500188679245283,
      "loss": 1.9703,
      "step": 2650
    },
    {
      "epoch": 0.5018867924528302,
      "grad_norm": 0.853547215461731,
      "learning_rate": 0.0004498301886792453,
      "loss": 1.9866,
      "step": 2660
    },
    {
      "epoch": 0.5037735849056604,
      "grad_norm": 0.8363869190216064,
      "learning_rate": 0.00044964150943396227,
      "loss": 2.0799,
      "step": 2670
    },
    {
      "epoch": 0.5056603773584906,
      "grad_norm": 0.6709806323051453,
      "learning_rate": 0.0004494528301886793,
      "loss": 1.9527,
      "step": 2680
    },
    {
      "epoch": 0.5075471698113208,
      "grad_norm": 0.6639360785484314,
      "learning_rate": 0.00044926415094339624,
      "loss": 2.009,
      "step": 2690
    },
    {
      "epoch": 0.5094339622641509,
      "grad_norm": 0.7593610882759094,
      "learning_rate": 0.0004490754716981132,
      "loss": 1.9238,
      "step": 2700
    },
    {
      "epoch": 0.5113207547169811,
      "grad_norm": 0.6835317611694336,
      "learning_rate": 0.0004488867924528302,
      "loss": 1.8996,
      "step": 2710
    },
    {
      "epoch": 0.5132075471698113,
      "grad_norm": 0.6940683126449585,
      "learning_rate": 0.00044869811320754716,
      "loss": 2.0071,
      "step": 2720
    },
    {
      "epoch": 0.5150943396226415,
      "grad_norm": 0.6532647013664246,
      "learning_rate": 0.00044850943396226417,
      "loss": 2.0142,
      "step": 2730
    },
    {
      "epoch": 0.5169811320754717,
      "grad_norm": 0.6857240796089172,
      "learning_rate": 0.0004483207547169811,
      "loss": 1.9657,
      "step": 2740
    },
    {
      "epoch": 0.5188679245283019,
      "grad_norm": 0.6279357671737671,
      "learning_rate": 0.0004481320754716981,
      "loss": 2.0918,
      "step": 2750
    },
    {
      "epoch": 0.5207547169811321,
      "grad_norm": 0.6839677095413208,
      "learning_rate": 0.0004479433962264151,
      "loss": 2.0187,
      "step": 2760
    },
    {
      "epoch": 0.5226415094339623,
      "grad_norm": 0.737762451171875,
      "learning_rate": 0.0004477547169811321,
      "loss": 2.0198,
      "step": 2770
    },
    {
      "epoch": 0.5245283018867924,
      "grad_norm": 0.6470081806182861,
      "learning_rate": 0.0004475660377358491,
      "loss": 1.8956,
      "step": 2780
    },
    {
      "epoch": 0.5264150943396226,
      "grad_norm": 0.6364725828170776,
      "learning_rate": 0.00044737735849056606,
      "loss": 1.9964,
      "step": 2790
    },
    {
      "epoch": 0.5283018867924528,
      "grad_norm": 0.7050157785415649,
      "learning_rate": 0.000447188679245283,
      "loss": 2.0608,
      "step": 2800
    },
    {
      "epoch": 0.530188679245283,
      "grad_norm": 0.7009301781654358,
      "learning_rate": 0.000447,
      "loss": 2.0204,
      "step": 2810
    },
    {
      "epoch": 0.5320754716981132,
      "grad_norm": 0.6703934669494629,
      "learning_rate": 0.000446811320754717,
      "loss": 1.9796,
      "step": 2820
    },
    {
      "epoch": 0.5339622641509434,
      "grad_norm": 0.7008041143417358,
      "learning_rate": 0.000446622641509434,
      "loss": 2.004,
      "step": 2830
    },
    {
      "epoch": 0.5358490566037736,
      "grad_norm": 0.7281481623649597,
      "learning_rate": 0.00044643396226415094,
      "loss": 1.9966,
      "step": 2840
    },
    {
      "epoch": 0.5377358490566038,
      "grad_norm": 0.6754550933837891,
      "learning_rate": 0.0004462452830188679,
      "loss": 1.9893,
      "step": 2850
    },
    {
      "epoch": 0.539622641509434,
      "grad_norm": 0.8231561779975891,
      "learning_rate": 0.0004460566037735849,
      "loss": 1.9109,
      "step": 2860
    },
    {
      "epoch": 0.5415094339622641,
      "grad_norm": 0.6573865413665771,
      "learning_rate": 0.00044586792452830186,
      "loss": 1.9686,
      "step": 2870
    },
    {
      "epoch": 0.5433962264150943,
      "grad_norm": 0.7629002928733826,
      "learning_rate": 0.00044567924528301887,
      "loss": 2.0386,
      "step": 2880
    },
    {
      "epoch": 0.5452830188679245,
      "grad_norm": 0.6579307317733765,
      "learning_rate": 0.0004454905660377359,
      "loss": 1.9741,
      "step": 2890
    },
    {
      "epoch": 0.5471698113207547,
      "grad_norm": 0.751401960849762,
      "learning_rate": 0.00044530188679245284,
      "loss": 1.9683,
      "step": 2900
    },
    {
      "epoch": 0.5490566037735849,
      "grad_norm": 0.6098260879516602,
      "learning_rate": 0.00044511320754716984,
      "loss": 1.9944,
      "step": 2910
    },
    {
      "epoch": 0.5509433962264151,
      "grad_norm": 0.6618639826774597,
      "learning_rate": 0.0004449245283018868,
      "loss": 1.9477,
      "step": 2920
    },
    {
      "epoch": 0.5528301886792453,
      "grad_norm": 0.7312523126602173,
      "learning_rate": 0.0004447358490566038,
      "loss": 1.8958,
      "step": 2930
    },
    {
      "epoch": 0.5547169811320755,
      "grad_norm": 0.7931923270225525,
      "learning_rate": 0.00044454716981132076,
      "loss": 2.0068,
      "step": 2940
    },
    {
      "epoch": 0.5566037735849056,
      "grad_norm": 0.7006123065948486,
      "learning_rate": 0.0004443584905660377,
      "loss": 1.8936,
      "step": 2950
    },
    {
      "epoch": 0.5584905660377358,
      "grad_norm": 0.7061339616775513,
      "learning_rate": 0.00044416981132075473,
      "loss": 2.004,
      "step": 2960
    },
    {
      "epoch": 0.560377358490566,
      "grad_norm": 0.7090078592300415,
      "learning_rate": 0.0004439811320754717,
      "loss": 1.9137,
      "step": 2970
    },
    {
      "epoch": 0.5622641509433962,
      "grad_norm": 0.7446510195732117,
      "learning_rate": 0.0004437924528301887,
      "loss": 2.0411,
      "step": 2980
    },
    {
      "epoch": 0.5641509433962264,
      "grad_norm": 0.8170174956321716,
      "learning_rate": 0.00044360377358490565,
      "loss": 1.9799,
      "step": 2990
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 0.7303928732872009,
      "learning_rate": 0.0004434150943396226,
      "loss": 1.9506,
      "step": 3000
    },
    {
      "epoch": 0.5679245283018868,
      "grad_norm": 0.7274763584136963,
      "learning_rate": 0.00044322641509433967,
      "loss": 1.9518,
      "step": 3010
    },
    {
      "epoch": 0.569811320754717,
      "grad_norm": 0.7312541007995605,
      "learning_rate": 0.0004430377358490566,
      "loss": 1.9611,
      "step": 3020
    },
    {
      "epoch": 0.5716981132075472,
      "grad_norm": 0.769476056098938,
      "learning_rate": 0.00044284905660377363,
      "loss": 2.0004,
      "step": 3030
    },
    {
      "epoch": 0.5735849056603773,
      "grad_norm": 0.7936493754386902,
      "learning_rate": 0.0004426603773584906,
      "loss": 1.9243,
      "step": 3040
    },
    {
      "epoch": 0.5754716981132075,
      "grad_norm": 0.7359932661056519,
      "learning_rate": 0.00044247169811320754,
      "loss": 1.958,
      "step": 3050
    },
    {
      "epoch": 0.5773584905660377,
      "grad_norm": 0.6691768169403076,
      "learning_rate": 0.00044228301886792455,
      "loss": 1.9373,
      "step": 3060
    },
    {
      "epoch": 0.5792452830188679,
      "grad_norm": 0.6484683156013489,
      "learning_rate": 0.0004420943396226415,
      "loss": 2.0278,
      "step": 3070
    },
    {
      "epoch": 0.5811320754716981,
      "grad_norm": 0.6989763975143433,
      "learning_rate": 0.0004419056603773585,
      "loss": 1.9694,
      "step": 3080
    },
    {
      "epoch": 0.5830188679245283,
      "grad_norm": 0.652847945690155,
      "learning_rate": 0.00044171698113207547,
      "loss": 1.9976,
      "step": 3090
    },
    {
      "epoch": 0.5849056603773585,
      "grad_norm": 0.6955966353416443,
      "learning_rate": 0.0004415283018867924,
      "loss": 1.9717,
      "step": 3100
    },
    {
      "epoch": 0.5867924528301887,
      "grad_norm": 0.8123462200164795,
      "learning_rate": 0.00044133962264150943,
      "loss": 1.8865,
      "step": 3110
    },
    {
      "epoch": 0.5886792452830188,
      "grad_norm": 0.7373541593551636,
      "learning_rate": 0.0004411509433962264,
      "loss": 2.0521,
      "step": 3120
    },
    {
      "epoch": 0.590566037735849,
      "grad_norm": 0.7244933247566223,
      "learning_rate": 0.00044096226415094345,
      "loss": 2.0185,
      "step": 3130
    },
    {
      "epoch": 0.5924528301886792,
      "grad_norm": 0.6834579110145569,
      "learning_rate": 0.0004407735849056604,
      "loss": 1.9656,
      "step": 3140
    },
    {
      "epoch": 0.5943396226415094,
      "grad_norm": 0.9380537867546082,
      "learning_rate": 0.00044058490566037736,
      "loss": 1.9681,
      "step": 3150
    },
    {
      "epoch": 0.5962264150943396,
      "grad_norm": 0.7665178775787354,
      "learning_rate": 0.00044039622641509437,
      "loss": 1.8762,
      "step": 3160
    },
    {
      "epoch": 0.5981132075471698,
      "grad_norm": 0.7156065106391907,
      "learning_rate": 0.00044020754716981133,
      "loss": 1.9955,
      "step": 3170
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.6569814682006836,
      "learning_rate": 0.00044001886792452834,
      "loss": 1.9454,
      "step": 3180
    },
    {
      "epoch": 0.6018867924528302,
      "grad_norm": 0.5972268581390381,
      "learning_rate": 0.0004398301886792453,
      "loss": 1.946,
      "step": 3190
    },
    {
      "epoch": 0.6037735849056604,
      "grad_norm": 0.6173723936080933,
      "learning_rate": 0.00043964150943396225,
      "loss": 1.9344,
      "step": 3200
    },
    {
      "epoch": 0.6056603773584905,
      "grad_norm": 0.7367063164710999,
      "learning_rate": 0.00043945283018867926,
      "loss": 2.0113,
      "step": 3210
    },
    {
      "epoch": 0.6075471698113207,
      "grad_norm": 0.7240420579910278,
      "learning_rate": 0.0004392641509433962,
      "loss": 1.9515,
      "step": 3220
    },
    {
      "epoch": 0.6094339622641509,
      "grad_norm": 0.7358578443527222,
      "learning_rate": 0.0004390754716981132,
      "loss": 2.0019,
      "step": 3230
    },
    {
      "epoch": 0.6113207547169811,
      "grad_norm": 0.6628909707069397,
      "learning_rate": 0.0004388867924528302,
      "loss": 2.0259,
      "step": 3240
    },
    {
      "epoch": 0.6132075471698113,
      "grad_norm": 0.7694278955459595,
      "learning_rate": 0.00043869811320754713,
      "loss": 1.9059,
      "step": 3250
    },
    {
      "epoch": 0.6150943396226415,
      "grad_norm": 0.6811085939407349,
      "learning_rate": 0.0004385094339622642,
      "loss": 1.9778,
      "step": 3260
    },
    {
      "epoch": 0.6169811320754717,
      "grad_norm": 0.8047981262207031,
      "learning_rate": 0.00043832075471698115,
      "loss": 1.9947,
      "step": 3270
    },
    {
      "epoch": 0.6188679245283019,
      "grad_norm": 0.6533488631248474,
      "learning_rate": 0.00043813207547169816,
      "loss": 2.0501,
      "step": 3280
    },
    {
      "epoch": 0.620754716981132,
      "grad_norm": 0.7265681624412537,
      "learning_rate": 0.0004379433962264151,
      "loss": 2.0354,
      "step": 3290
    },
    {
      "epoch": 0.6226415094339622,
      "grad_norm": 0.7334573864936829,
      "learning_rate": 0.00043775471698113207,
      "loss": 1.9698,
      "step": 3300
    },
    {
      "epoch": 0.6245283018867924,
      "grad_norm": 0.8053271770477295,
      "learning_rate": 0.0004375660377358491,
      "loss": 1.9052,
      "step": 3310
    },
    {
      "epoch": 0.6264150943396226,
      "grad_norm": 0.7234533429145813,
      "learning_rate": 0.00043737735849056603,
      "loss": 1.9407,
      "step": 3320
    },
    {
      "epoch": 0.6283018867924528,
      "grad_norm": 0.8308709263801575,
      "learning_rate": 0.00043718867924528304,
      "loss": 1.9234,
      "step": 3330
    },
    {
      "epoch": 0.630188679245283,
      "grad_norm": 0.6914460062980652,
      "learning_rate": 0.000437,
      "loss": 2.0057,
      "step": 3340
    },
    {
      "epoch": 0.6320754716981132,
      "grad_norm": 0.6150305867195129,
      "learning_rate": 0.00043681132075471695,
      "loss": 1.9851,
      "step": 3350
    },
    {
      "epoch": 0.6339622641509434,
      "grad_norm": 0.7012033462524414,
      "learning_rate": 0.00043662264150943396,
      "loss": 1.9603,
      "step": 3360
    },
    {
      "epoch": 0.6358490566037736,
      "grad_norm": 0.75176602602005,
      "learning_rate": 0.0004364339622641509,
      "loss": 2.0562,
      "step": 3370
    },
    {
      "epoch": 0.6377358490566037,
      "grad_norm": 0.7403188943862915,
      "learning_rate": 0.000436245283018868,
      "loss": 1.9045,
      "step": 3380
    },
    {
      "epoch": 0.6396226415094339,
      "grad_norm": 0.7229239344596863,
      "learning_rate": 0.00043605660377358493,
      "loss": 1.9724,
      "step": 3390
    },
    {
      "epoch": 0.6415094339622641,
      "grad_norm": 0.6588183045387268,
      "learning_rate": 0.0004358679245283019,
      "loss": 1.834,
      "step": 3400
    },
    {
      "epoch": 0.6433962264150943,
      "grad_norm": 0.6663601994514465,
      "learning_rate": 0.0004356792452830189,
      "loss": 1.9775,
      "step": 3410
    },
    {
      "epoch": 0.6452830188679245,
      "grad_norm": 0.7025060057640076,
      "learning_rate": 0.00043549056603773585,
      "loss": 2.05,
      "step": 3420
    },
    {
      "epoch": 0.6471698113207547,
      "grad_norm": 0.8087669610977173,
      "learning_rate": 0.00043530188679245286,
      "loss": 1.9661,
      "step": 3430
    },
    {
      "epoch": 0.6490566037735849,
      "grad_norm": 0.67482590675354,
      "learning_rate": 0.0004351132075471698,
      "loss": 1.7971,
      "step": 3440
    },
    {
      "epoch": 0.6509433962264151,
      "grad_norm": 0.7290143370628357,
      "learning_rate": 0.0004349245283018868,
      "loss": 1.9636,
      "step": 3450
    },
    {
      "epoch": 0.6528301886792452,
      "grad_norm": 0.6928234696388245,
      "learning_rate": 0.0004347358490566038,
      "loss": 1.9558,
      "step": 3460
    },
    {
      "epoch": 0.6547169811320754,
      "grad_norm": 0.6944839358329773,
      "learning_rate": 0.00043454716981132074,
      "loss": 1.9861,
      "step": 3470
    },
    {
      "epoch": 0.6566037735849056,
      "grad_norm": 0.6103209853172302,
      "learning_rate": 0.00043435849056603775,
      "loss": 2.0209,
      "step": 3480
    },
    {
      "epoch": 0.6584905660377358,
      "grad_norm": 0.7429606914520264,
      "learning_rate": 0.0004341698113207547,
      "loss": 2.0354,
      "step": 3490
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 0.7363060712814331,
      "learning_rate": 0.0004339811320754717,
      "loss": 1.9675,
      "step": 3500
    },
    {
      "epoch": 0.6622641509433962,
      "grad_norm": 0.8616445064544678,
      "learning_rate": 0.0004337924528301887,
      "loss": 1.9243,
      "step": 3510
    },
    {
      "epoch": 0.6641509433962264,
      "grad_norm": 0.697409987449646,
      "learning_rate": 0.0004336037735849057,
      "loss": 2.0076,
      "step": 3520
    },
    {
      "epoch": 0.6660377358490566,
      "grad_norm": 0.7614265084266663,
      "learning_rate": 0.0004334150943396227,
      "loss": 2.1035,
      "step": 3530
    },
    {
      "epoch": 0.6679245283018868,
      "grad_norm": 0.7008963823318481,
      "learning_rate": 0.00043322641509433964,
      "loss": 1.9876,
      "step": 3540
    },
    {
      "epoch": 0.6698113207547169,
      "grad_norm": 0.7016564011573792,
      "learning_rate": 0.0004330377358490566,
      "loss": 1.8951,
      "step": 3550
    },
    {
      "epoch": 0.6716981132075471,
      "grad_norm": 0.6971780061721802,
      "learning_rate": 0.0004328490566037736,
      "loss": 2.0113,
      "step": 3560
    },
    {
      "epoch": 0.6735849056603773,
      "grad_norm": 0.7084751725196838,
      "learning_rate": 0.00043266037735849056,
      "loss": 1.9359,
      "step": 3570
    },
    {
      "epoch": 0.6754716981132075,
      "grad_norm": 0.7438477873802185,
      "learning_rate": 0.00043247169811320757,
      "loss": 1.9608,
      "step": 3580
    },
    {
      "epoch": 0.6773584905660377,
      "grad_norm": 0.7781243324279785,
      "learning_rate": 0.0004322830188679245,
      "loss": 1.8969,
      "step": 3590
    },
    {
      "epoch": 0.6792452830188679,
      "grad_norm": 0.6566885113716125,
      "learning_rate": 0.0004320943396226415,
      "loss": 1.9828,
      "step": 3600
    },
    {
      "epoch": 0.6811320754716981,
      "grad_norm": 0.7357646226882935,
      "learning_rate": 0.0004319056603773585,
      "loss": 2.0254,
      "step": 3610
    },
    {
      "epoch": 0.6830188679245283,
      "grad_norm": 0.7071355581283569,
      "learning_rate": 0.00043171698113207544,
      "loss": 1.8532,
      "step": 3620
    },
    {
      "epoch": 0.6849056603773584,
      "grad_norm": 0.671409010887146,
      "learning_rate": 0.0004315283018867925,
      "loss": 1.8674,
      "step": 3630
    },
    {
      "epoch": 0.6867924528301886,
      "grad_norm": 0.6813784837722778,
      "learning_rate": 0.00043133962264150946,
      "loss": 1.9662,
      "step": 3640
    },
    {
      "epoch": 0.6886792452830188,
      "grad_norm": 0.6700597405433655,
      "learning_rate": 0.0004311509433962264,
      "loss": 1.9839,
      "step": 3650
    },
    {
      "epoch": 0.690566037735849,
      "grad_norm": 0.8320394158363342,
      "learning_rate": 0.0004309622641509434,
      "loss": 1.9096,
      "step": 3660
    },
    {
      "epoch": 0.6924528301886792,
      "grad_norm": 0.6695120930671692,
      "learning_rate": 0.0004307735849056604,
      "loss": 2.008,
      "step": 3670
    },
    {
      "epoch": 0.6943396226415094,
      "grad_norm": 0.7172922492027283,
      "learning_rate": 0.0004305849056603774,
      "loss": 1.9218,
      "step": 3680
    },
    {
      "epoch": 0.6962264150943396,
      "grad_norm": 0.6479635238647461,
      "learning_rate": 0.00043039622641509435,
      "loss": 2.0039,
      "step": 3690
    },
    {
      "epoch": 0.6981132075471698,
      "grad_norm": 0.686342179775238,
      "learning_rate": 0.0004302075471698113,
      "loss": 1.9809,
      "step": 3700
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.684633731842041,
      "learning_rate": 0.0004300188679245283,
      "loss": 1.9934,
      "step": 3710
    },
    {
      "epoch": 0.7018867924528301,
      "grad_norm": 0.6546320915222168,
      "learning_rate": 0.00042983018867924527,
      "loss": 1.9171,
      "step": 3720
    },
    {
      "epoch": 0.7037735849056603,
      "grad_norm": 0.6697250604629517,
      "learning_rate": 0.0004296415094339623,
      "loss": 2.0398,
      "step": 3730
    },
    {
      "epoch": 0.7056603773584905,
      "grad_norm": 0.7147389054298401,
      "learning_rate": 0.00042945283018867923,
      "loss": 1.9036,
      "step": 3740
    },
    {
      "epoch": 0.7075471698113207,
      "grad_norm": 0.7261271476745605,
      "learning_rate": 0.00042926415094339624,
      "loss": 2.0398,
      "step": 3750
    },
    {
      "epoch": 0.7094339622641509,
      "grad_norm": 0.7078847885131836,
      "learning_rate": 0.00042907547169811325,
      "loss": 1.9363,
      "step": 3760
    },
    {
      "epoch": 0.7113207547169811,
      "grad_norm": 0.6252160668373108,
      "learning_rate": 0.0004288867924528302,
      "loss": 1.9334,
      "step": 3770
    },
    {
      "epoch": 0.7132075471698113,
      "grad_norm": 0.8014699816703796,
      "learning_rate": 0.0004286981132075472,
      "loss": 1.9924,
      "step": 3780
    },
    {
      "epoch": 0.7150943396226415,
      "grad_norm": 0.660459578037262,
      "learning_rate": 0.00042850943396226417,
      "loss": 2.0158,
      "step": 3790
    },
    {
      "epoch": 0.7169811320754716,
      "grad_norm": 0.7151951193809509,
      "learning_rate": 0.0004283207547169811,
      "loss": 2.054,
      "step": 3800
    },
    {
      "epoch": 0.7188679245283018,
      "grad_norm": 0.6896937489509583,
      "learning_rate": 0.00042813207547169813,
      "loss": 2.0197,
      "step": 3810
    },
    {
      "epoch": 0.720754716981132,
      "grad_norm": 0.6866812705993652,
      "learning_rate": 0.0004279433962264151,
      "loss": 1.9439,
      "step": 3820
    },
    {
      "epoch": 0.7226415094339622,
      "grad_norm": 0.7007819414138794,
      "learning_rate": 0.0004277547169811321,
      "loss": 1.9695,
      "step": 3830
    },
    {
      "epoch": 0.7245283018867924,
      "grad_norm": 0.72086501121521,
      "learning_rate": 0.00042756603773584905,
      "loss": 1.9415,
      "step": 3840
    },
    {
      "epoch": 0.7264150943396226,
      "grad_norm": 0.7059128880500793,
      "learning_rate": 0.000427377358490566,
      "loss": 2.0319,
      "step": 3850
    },
    {
      "epoch": 0.7283018867924528,
      "grad_norm": 0.6890622973442078,
      "learning_rate": 0.000427188679245283,
      "loss": 1.954,
      "step": 3860
    },
    {
      "epoch": 0.730188679245283,
      "grad_norm": 0.7101737856864929,
      "learning_rate": 0.000427,
      "loss": 1.9503,
      "step": 3870
    },
    {
      "epoch": 0.7320754716981132,
      "grad_norm": 0.7120329141616821,
      "learning_rate": 0.00042681132075471703,
      "loss": 1.9306,
      "step": 3880
    },
    {
      "epoch": 0.7339622641509433,
      "grad_norm": 0.7229883074760437,
      "learning_rate": 0.000426622641509434,
      "loss": 1.9453,
      "step": 3890
    },
    {
      "epoch": 0.7358490566037735,
      "grad_norm": 0.6764413118362427,
      "learning_rate": 0.00042643396226415094,
      "loss": 1.9614,
      "step": 3900
    },
    {
      "epoch": 0.7377358490566037,
      "grad_norm": 0.7343155741691589,
      "learning_rate": 0.00042624528301886795,
      "loss": 1.9477,
      "step": 3910
    },
    {
      "epoch": 0.7396226415094339,
      "grad_norm": 0.7513315677642822,
      "learning_rate": 0.0004260566037735849,
      "loss": 2.008,
      "step": 3920
    },
    {
      "epoch": 0.7415094339622641,
      "grad_norm": 0.7632419466972351,
      "learning_rate": 0.0004258679245283019,
      "loss": 2.0284,
      "step": 3930
    },
    {
      "epoch": 0.7433962264150943,
      "grad_norm": 0.6371757984161377,
      "learning_rate": 0.0004256792452830189,
      "loss": 1.9353,
      "step": 3940
    },
    {
      "epoch": 0.7452830188679245,
      "grad_norm": 0.631135106086731,
      "learning_rate": 0.00042549056603773583,
      "loss": 1.884,
      "step": 3950
    },
    {
      "epoch": 0.7471698113207547,
      "grad_norm": 0.7579075694084167,
      "learning_rate": 0.00042530188679245284,
      "loss": 1.9879,
      "step": 3960
    },
    {
      "epoch": 0.7490566037735849,
      "grad_norm": 0.7612749338150024,
      "learning_rate": 0.0004251132075471698,
      "loss": 1.8585,
      "step": 3970
    },
    {
      "epoch": 0.7509433962264151,
      "grad_norm": 0.6769037842750549,
      "learning_rate": 0.0004249245283018868,
      "loss": 1.9759,
      "step": 3980
    },
    {
      "epoch": 0.7528301886792453,
      "grad_norm": 0.7067826390266418,
      "learning_rate": 0.0004247358490566038,
      "loss": 1.9844,
      "step": 3990
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 0.7843778133392334,
      "learning_rate": 0.00042454716981132077,
      "loss": 1.9381,
      "step": 4000
    },
    {
      "epoch": 0.7566037735849057,
      "grad_norm": 0.7179023623466492,
      "learning_rate": 0.0004243584905660378,
      "loss": 1.9796,
      "step": 4010
    },
    {
      "epoch": 0.7584905660377359,
      "grad_norm": 0.8241468667984009,
      "learning_rate": 0.00042416981132075473,
      "loss": 1.9723,
      "step": 4020
    },
    {
      "epoch": 0.7603773584905661,
      "grad_norm": 0.7953981757164001,
      "learning_rate": 0.00042398113207547174,
      "loss": 2.0269,
      "step": 4030
    },
    {
      "epoch": 0.7622641509433963,
      "grad_norm": 0.7887871265411377,
      "learning_rate": 0.0004237924528301887,
      "loss": 1.9386,
      "step": 4040
    },
    {
      "epoch": 0.7641509433962265,
      "grad_norm": 0.7098668813705444,
      "learning_rate": 0.00042360377358490565,
      "loss": 1.9405,
      "step": 4050
    },
    {
      "epoch": 0.7660377358490567,
      "grad_norm": 0.7938623428344727,
      "learning_rate": 0.00042341509433962266,
      "loss": 1.9249,
      "step": 4060
    },
    {
      "epoch": 0.7679245283018868,
      "grad_norm": 0.6543278098106384,
      "learning_rate": 0.0004232264150943396,
      "loss": 1.9763,
      "step": 4070
    },
    {
      "epoch": 0.769811320754717,
      "grad_norm": 0.6958367228507996,
      "learning_rate": 0.0004230377358490566,
      "loss": 1.9618,
      "step": 4080
    },
    {
      "epoch": 0.7716981132075472,
      "grad_norm": 0.6845560073852539,
      "learning_rate": 0.0004228490566037736,
      "loss": 1.917,
      "step": 4090
    },
    {
      "epoch": 0.7735849056603774,
      "grad_norm": 0.7128505110740662,
      "learning_rate": 0.00042266037735849053,
      "loss": 1.9551,
      "step": 4100
    },
    {
      "epoch": 0.7754716981132076,
      "grad_norm": 0.7117420434951782,
      "learning_rate": 0.00042247169811320754,
      "loss": 1.9305,
      "step": 4110
    },
    {
      "epoch": 0.7773584905660378,
      "grad_norm": 0.7301112413406372,
      "learning_rate": 0.00042228301886792455,
      "loss": 1.9693,
      "step": 4120
    },
    {
      "epoch": 0.779245283018868,
      "grad_norm": 0.6921272277832031,
      "learning_rate": 0.00042209433962264156,
      "loss": 2.0439,
      "step": 4130
    },
    {
      "epoch": 0.7811320754716982,
      "grad_norm": 0.6901494264602661,
      "learning_rate": 0.0004219056603773585,
      "loss": 1.9925,
      "step": 4140
    },
    {
      "epoch": 0.7830188679245284,
      "grad_norm": 0.7228140830993652,
      "learning_rate": 0.00042171698113207547,
      "loss": 1.9341,
      "step": 4150
    },
    {
      "epoch": 0.7849056603773585,
      "grad_norm": 0.7099171876907349,
      "learning_rate": 0.0004215283018867925,
      "loss": 1.9113,
      "step": 4160
    },
    {
      "epoch": 0.7867924528301887,
      "grad_norm": 0.7458534240722656,
      "learning_rate": 0.00042133962264150944,
      "loss": 2.1068,
      "step": 4170
    },
    {
      "epoch": 0.7886792452830189,
      "grad_norm": 0.7800428867340088,
      "learning_rate": 0.00042115094339622645,
      "loss": 2.0427,
      "step": 4180
    },
    {
      "epoch": 0.7905660377358491,
      "grad_norm": 0.7870848178863525,
      "learning_rate": 0.0004209622641509434,
      "loss": 1.9204,
      "step": 4190
    },
    {
      "epoch": 0.7924528301886793,
      "grad_norm": 0.7368739247322083,
      "learning_rate": 0.00042077358490566036,
      "loss": 1.9505,
      "step": 4200
    },
    {
      "epoch": 0.7943396226415095,
      "grad_norm": 0.8036274313926697,
      "learning_rate": 0.00042058490566037736,
      "loss": 1.9128,
      "step": 4210
    },
    {
      "epoch": 0.7962264150943397,
      "grad_norm": 0.7573596835136414,
      "learning_rate": 0.0004203962264150943,
      "loss": 1.9585,
      "step": 4220
    },
    {
      "epoch": 0.7981132075471699,
      "grad_norm": 0.708652913570404,
      "learning_rate": 0.00042020754716981133,
      "loss": 1.9251,
      "step": 4230
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.7776587605476379,
      "learning_rate": 0.00042001886792452834,
      "loss": 2.0173,
      "step": 4240
    },
    {
      "epoch": 0.8018867924528302,
      "grad_norm": 0.7630447745323181,
      "learning_rate": 0.0004198301886792453,
      "loss": 1.9473,
      "step": 4250
    },
    {
      "epoch": 0.8037735849056604,
      "grad_norm": 0.7837167382240295,
      "learning_rate": 0.0004196415094339623,
      "loss": 2.0561,
      "step": 4260
    },
    {
      "epoch": 0.8056603773584906,
      "grad_norm": 0.8042008280754089,
      "learning_rate": 0.00041945283018867926,
      "loss": 1.9841,
      "step": 4270
    },
    {
      "epoch": 0.8075471698113208,
      "grad_norm": 0.7333820462226868,
      "learning_rate": 0.00041926415094339627,
      "loss": 1.9962,
      "step": 4280
    },
    {
      "epoch": 0.809433962264151,
      "grad_norm": 0.71897292137146,
      "learning_rate": 0.0004190754716981132,
      "loss": 2.0713,
      "step": 4290
    },
    {
      "epoch": 0.8113207547169812,
      "grad_norm": 0.6713532209396362,
      "learning_rate": 0.0004188867924528302,
      "loss": 1.9443,
      "step": 4300
    },
    {
      "epoch": 0.8132075471698114,
      "grad_norm": 0.7169061899185181,
      "learning_rate": 0.0004186981132075472,
      "loss": 1.9485,
      "step": 4310
    },
    {
      "epoch": 0.8150943396226416,
      "grad_norm": 0.7270424365997314,
      "learning_rate": 0.00041850943396226414,
      "loss": 1.9052,
      "step": 4320
    },
    {
      "epoch": 0.8169811320754717,
      "grad_norm": 0.7328714728355408,
      "learning_rate": 0.00041832075471698115,
      "loss": 1.946,
      "step": 4330
    },
    {
      "epoch": 0.8188679245283019,
      "grad_norm": 0.733067512512207,
      "learning_rate": 0.0004181320754716981,
      "loss": 1.9788,
      "step": 4340
    },
    {
      "epoch": 0.8207547169811321,
      "grad_norm": 0.6732206344604492,
      "learning_rate": 0.00041794339622641506,
      "loss": 1.9555,
      "step": 4350
    },
    {
      "epoch": 0.8226415094339623,
      "grad_norm": 0.7241216897964478,
      "learning_rate": 0.0004177547169811321,
      "loss": 1.9413,
      "step": 4360
    },
    {
      "epoch": 0.8245283018867925,
      "grad_norm": 0.7235967516899109,
      "learning_rate": 0.0004175660377358491,
      "loss": 1.9043,
      "step": 4370
    },
    {
      "epoch": 0.8264150943396227,
      "grad_norm": 0.8428477048873901,
      "learning_rate": 0.0004173773584905661,
      "loss": 1.9458,
      "step": 4380
    },
    {
      "epoch": 0.8283018867924529,
      "grad_norm": 0.7356573939323425,
      "learning_rate": 0.00041718867924528304,
      "loss": 2.0064,
      "step": 4390
    },
    {
      "epoch": 0.8301886792452831,
      "grad_norm": 0.7367264628410339,
      "learning_rate": 0.000417,
      "loss": 1.9588,
      "step": 4400
    },
    {
      "epoch": 0.8320754716981132,
      "grad_norm": 0.7612881660461426,
      "learning_rate": 0.000416811320754717,
      "loss": 2.0361,
      "step": 4410
    },
    {
      "epoch": 0.8339622641509434,
      "grad_norm": 0.7175534963607788,
      "learning_rate": 0.00041662264150943396,
      "loss": 2.0033,
      "step": 4420
    },
    {
      "epoch": 0.8358490566037736,
      "grad_norm": 0.6608340740203857,
      "learning_rate": 0.00041643396226415097,
      "loss": 1.9309,
      "step": 4430
    },
    {
      "epoch": 0.8377358490566038,
      "grad_norm": 0.6812207698822021,
      "learning_rate": 0.00041624528301886793,
      "loss": 2.028,
      "step": 4440
    },
    {
      "epoch": 0.839622641509434,
      "grad_norm": 0.6522758603096008,
      "learning_rate": 0.0004160566037735849,
      "loss": 1.9522,
      "step": 4450
    },
    {
      "epoch": 0.8415094339622642,
      "grad_norm": 0.6897231936454773,
      "learning_rate": 0.0004158679245283019,
      "loss": 1.9094,
      "step": 4460
    },
    {
      "epoch": 0.8433962264150944,
      "grad_norm": 0.7315573692321777,
      "learning_rate": 0.00041567924528301885,
      "loss": 1.9689,
      "step": 4470
    },
    {
      "epoch": 0.8452830188679246,
      "grad_norm": 0.7697047591209412,
      "learning_rate": 0.0004154905660377359,
      "loss": 1.9909,
      "step": 4480
    },
    {
      "epoch": 0.8471698113207548,
      "grad_norm": 0.8055801391601562,
      "learning_rate": 0.00041530188679245287,
      "loss": 1.9832,
      "step": 4490
    },
    {
      "epoch": 0.8490566037735849,
      "grad_norm": 0.6905852556228638,
      "learning_rate": 0.0004151132075471698,
      "loss": 2.0863,
      "step": 4500
    },
    {
      "epoch": 0.8509433962264151,
      "grad_norm": 0.6693705320358276,
      "learning_rate": 0.00041492452830188683,
      "loss": 1.9259,
      "step": 4510
    },
    {
      "epoch": 0.8528301886792453,
      "grad_norm": 0.7206969261169434,
      "learning_rate": 0.0004147358490566038,
      "loss": 1.9289,
      "step": 4520
    },
    {
      "epoch": 0.8547169811320755,
      "grad_norm": 0.7810751795768738,
      "learning_rate": 0.0004145471698113208,
      "loss": 1.9528,
      "step": 4530
    },
    {
      "epoch": 0.8566037735849057,
      "grad_norm": 0.8073238134384155,
      "learning_rate": 0.00041435849056603775,
      "loss": 2.0001,
      "step": 4540
    },
    {
      "epoch": 0.8584905660377359,
      "grad_norm": 0.691174328327179,
      "learning_rate": 0.0004141698113207547,
      "loss": 1.9473,
      "step": 4550
    },
    {
      "epoch": 0.8603773584905661,
      "grad_norm": 0.7142412066459656,
      "learning_rate": 0.0004139811320754717,
      "loss": 2.0427,
      "step": 4560
    },
    {
      "epoch": 0.8622641509433963,
      "grad_norm": 0.7116072177886963,
      "learning_rate": 0.00041379245283018867,
      "loss": 1.9906,
      "step": 4570
    },
    {
      "epoch": 0.8641509433962264,
      "grad_norm": 0.7424048781394958,
      "learning_rate": 0.0004136037735849057,
      "loss": 1.904,
      "step": 4580
    },
    {
      "epoch": 0.8660377358490566,
      "grad_norm": 0.663666307926178,
      "learning_rate": 0.00041341509433962263,
      "loss": 1.8884,
      "step": 4590
    },
    {
      "epoch": 0.8679245283018868,
      "grad_norm": 0.6778043508529663,
      "learning_rate": 0.0004132264150943396,
      "loss": 1.965,
      "step": 4600
    },
    {
      "epoch": 0.869811320754717,
      "grad_norm": 0.7157074809074402,
      "learning_rate": 0.00041303773584905665,
      "loss": 2.0162,
      "step": 4610
    },
    {
      "epoch": 0.8716981132075472,
      "grad_norm": 0.6956394910812378,
      "learning_rate": 0.0004128490566037736,
      "loss": 1.914,
      "step": 4620
    },
    {
      "epoch": 0.8735849056603774,
      "grad_norm": 0.724845290184021,
      "learning_rate": 0.0004126603773584906,
      "loss": 1.9888,
      "step": 4630
    },
    {
      "epoch": 0.8754716981132076,
      "grad_norm": 0.6570285558700562,
      "learning_rate": 0.00041247169811320757,
      "loss": 2.0272,
      "step": 4640
    },
    {
      "epoch": 0.8773584905660378,
      "grad_norm": 0.7592867612838745,
      "learning_rate": 0.0004122830188679245,
      "loss": 1.9983,
      "step": 4650
    },
    {
      "epoch": 0.879245283018868,
      "grad_norm": 0.8581104874610901,
      "learning_rate": 0.00041209433962264154,
      "loss": 1.9196,
      "step": 4660
    },
    {
      "epoch": 0.8811320754716981,
      "grad_norm": 0.7383921146392822,
      "learning_rate": 0.0004119056603773585,
      "loss": 2.0073,
      "step": 4670
    },
    {
      "epoch": 0.8830188679245283,
      "grad_norm": 0.6621490120887756,
      "learning_rate": 0.0004117169811320755,
      "loss": 1.928,
      "step": 4680
    },
    {
      "epoch": 0.8849056603773585,
      "grad_norm": 0.9171500205993652,
      "learning_rate": 0.00041152830188679245,
      "loss": 1.9113,
      "step": 4690
    },
    {
      "epoch": 0.8867924528301887,
      "grad_norm": 0.7089508175849915,
      "learning_rate": 0.0004113396226415094,
      "loss": 1.9273,
      "step": 4700
    },
    {
      "epoch": 0.8886792452830189,
      "grad_norm": 0.7590965032577515,
      "learning_rate": 0.0004111509433962264,
      "loss": 1.8762,
      "step": 4710
    },
    {
      "epoch": 0.8905660377358491,
      "grad_norm": 0.738829493522644,
      "learning_rate": 0.0004109622641509434,
      "loss": 1.9496,
      "step": 4720
    },
    {
      "epoch": 0.8924528301886793,
      "grad_norm": 0.8041648864746094,
      "learning_rate": 0.00041077358490566044,
      "loss": 2.022,
      "step": 4730
    },
    {
      "epoch": 0.8943396226415095,
      "grad_norm": 0.6592148542404175,
      "learning_rate": 0.0004105849056603774,
      "loss": 1.9179,
      "step": 4740
    },
    {
      "epoch": 0.8962264150943396,
      "grad_norm": 0.687282145023346,
      "learning_rate": 0.00041039622641509435,
      "loss": 1.9699,
      "step": 4750
    },
    {
      "epoch": 0.8981132075471698,
      "grad_norm": 0.7613898515701294,
      "learning_rate": 0.00041020754716981136,
      "loss": 1.9196,
      "step": 4760
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.7236369848251343,
      "learning_rate": 0.0004100188679245283,
      "loss": 1.9154,
      "step": 4770
    },
    {
      "epoch": 0.9018867924528302,
      "grad_norm": 0.7589379549026489,
      "learning_rate": 0.00040983018867924527,
      "loss": 1.8808,
      "step": 4780
    },
    {
      "epoch": 0.9037735849056604,
      "grad_norm": 0.6665162444114685,
      "learning_rate": 0.0004096415094339623,
      "loss": 1.9386,
      "step": 4790
    },
    {
      "epoch": 0.9056603773584906,
      "grad_norm": 0.9066277146339417,
      "learning_rate": 0.00040945283018867923,
      "loss": 1.9028,
      "step": 4800
    },
    {
      "epoch": 0.9075471698113208,
      "grad_norm": 0.7429379224777222,
      "learning_rate": 0.00040926415094339624,
      "loss": 1.9751,
      "step": 4810
    },
    {
      "epoch": 0.909433962264151,
      "grad_norm": 0.6559168696403503,
      "learning_rate": 0.0004090754716981132,
      "loss": 1.9272,
      "step": 4820
    },
    {
      "epoch": 0.9113207547169812,
      "grad_norm": 0.7064353823661804,
      "learning_rate": 0.00040888679245283015,
      "loss": 1.868,
      "step": 4830
    },
    {
      "epoch": 0.9132075471698113,
      "grad_norm": 0.7182860970497131,
      "learning_rate": 0.00040869811320754716,
      "loss": 1.9353,
      "step": 4840
    },
    {
      "epoch": 0.9150943396226415,
      "grad_norm": 0.6871615648269653,
      "learning_rate": 0.00040850943396226417,
      "loss": 1.9382,
      "step": 4850
    },
    {
      "epoch": 0.9169811320754717,
      "grad_norm": 0.8102856874465942,
      "learning_rate": 0.0004083207547169812,
      "loss": 1.918,
      "step": 4860
    },
    {
      "epoch": 0.9188679245283019,
      "grad_norm": 0.6830564737319946,
      "learning_rate": 0.00040813207547169813,
      "loss": 1.9009,
      "step": 4870
    },
    {
      "epoch": 0.9207547169811321,
      "grad_norm": 0.6758884787559509,
      "learning_rate": 0.0004079433962264151,
      "loss": 1.9106,
      "step": 4880
    },
    {
      "epoch": 0.9226415094339623,
      "grad_norm": 0.7008591294288635,
      "learning_rate": 0.0004077547169811321,
      "loss": 1.8636,
      "step": 4890
    },
    {
      "epoch": 0.9245283018867925,
      "grad_norm": 0.7099432349205017,
      "learning_rate": 0.00040756603773584905,
      "loss": 1.9488,
      "step": 4900
    },
    {
      "epoch": 0.9264150943396227,
      "grad_norm": 0.6600385308265686,
      "learning_rate": 0.00040737735849056606,
      "loss": 1.9616,
      "step": 4910
    },
    {
      "epoch": 0.9283018867924528,
      "grad_norm": 0.688214898109436,
      "learning_rate": 0.000407188679245283,
      "loss": 1.9193,
      "step": 4920
    },
    {
      "epoch": 0.930188679245283,
      "grad_norm": 0.7249725461006165,
      "learning_rate": 0.00040699999999999997,
      "loss": 1.9606,
      "step": 4930
    },
    {
      "epoch": 0.9320754716981132,
      "grad_norm": 0.7393922209739685,
      "learning_rate": 0.000406811320754717,
      "loss": 1.9362,
      "step": 4940
    },
    {
      "epoch": 0.9339622641509434,
      "grad_norm": 0.8093891143798828,
      "learning_rate": 0.00040662264150943394,
      "loss": 1.9124,
      "step": 4950
    },
    {
      "epoch": 0.9358490566037736,
      "grad_norm": 0.7507292032241821,
      "learning_rate": 0.00040643396226415095,
      "loss": 1.891,
      "step": 4960
    },
    {
      "epoch": 0.9377358490566038,
      "grad_norm": 0.6663928627967834,
      "learning_rate": 0.0004062452830188679,
      "loss": 1.968,
      "step": 4970
    },
    {
      "epoch": 0.939622641509434,
      "grad_norm": 0.7546339631080627,
      "learning_rate": 0.0004060566037735849,
      "loss": 1.9601,
      "step": 4980
    },
    {
      "epoch": 0.9415094339622642,
      "grad_norm": 0.6518308520317078,
      "learning_rate": 0.0004058679245283019,
      "loss": 1.9785,
      "step": 4990
    },
    {
      "epoch": 0.9433962264150944,
      "grad_norm": 0.7344942688941956,
      "learning_rate": 0.0004056792452830189,
      "loss": 1.8996,
      "step": 5000
    },
    {
      "epoch": 0.9452830188679245,
      "grad_norm": 0.8647462129592896,
      "learning_rate": 0.0004054905660377359,
      "loss": 1.9399,
      "step": 5010
    },
    {
      "epoch": 0.9471698113207547,
      "grad_norm": 0.7497246265411377,
      "learning_rate": 0.00040530188679245284,
      "loss": 1.9456,
      "step": 5020
    },
    {
      "epoch": 0.9490566037735849,
      "grad_norm": 0.7954546809196472,
      "learning_rate": 0.0004051132075471698,
      "loss": 1.8563,
      "step": 5030
    },
    {
      "epoch": 0.9509433962264151,
      "grad_norm": 0.7526507377624512,
      "learning_rate": 0.0004049245283018868,
      "loss": 1.9885,
      "step": 5040
    },
    {
      "epoch": 0.9528301886792453,
      "grad_norm": 0.7627226114273071,
      "learning_rate": 0.00040473584905660376,
      "loss": 1.8812,
      "step": 5050
    },
    {
      "epoch": 0.9547169811320755,
      "grad_norm": 0.7460110783576965,
      "learning_rate": 0.00040454716981132077,
      "loss": 1.8914,
      "step": 5060
    },
    {
      "epoch": 0.9566037735849057,
      "grad_norm": 0.7143032550811768,
      "learning_rate": 0.0004043584905660377,
      "loss": 1.9341,
      "step": 5070
    },
    {
      "epoch": 0.9584905660377359,
      "grad_norm": 0.6965464353561401,
      "learning_rate": 0.0004041698113207547,
      "loss": 1.9056,
      "step": 5080
    },
    {
      "epoch": 0.960377358490566,
      "grad_norm": 0.7005907297134399,
      "learning_rate": 0.0004039811320754717,
      "loss": 1.9614,
      "step": 5090
    },
    {
      "epoch": 0.9622641509433962,
      "grad_norm": 0.7636845707893372,
      "learning_rate": 0.0004037924528301887,
      "loss": 2.0264,
      "step": 5100
    },
    {
      "epoch": 0.9641509433962264,
      "grad_norm": 0.8119274377822876,
      "learning_rate": 0.0004036037735849057,
      "loss": 2.0019,
      "step": 5110
    },
    {
      "epoch": 0.9660377358490566,
      "grad_norm": 0.7661553025245667,
      "learning_rate": 0.00040341509433962266,
      "loss": 1.8956,
      "step": 5120
    },
    {
      "epoch": 0.9679245283018868,
      "grad_norm": 0.6974188089370728,
      "learning_rate": 0.0004032264150943396,
      "loss": 1.989,
      "step": 5130
    },
    {
      "epoch": 0.969811320754717,
      "grad_norm": 0.728980541229248,
      "learning_rate": 0.0004030377358490566,
      "loss": 1.9551,
      "step": 5140
    },
    {
      "epoch": 0.9716981132075472,
      "grad_norm": 0.8160247206687927,
      "learning_rate": 0.0004028490566037736,
      "loss": 1.9433,
      "step": 5150
    },
    {
      "epoch": 0.9735849056603774,
      "grad_norm": 0.6887897253036499,
      "learning_rate": 0.0004026603773584906,
      "loss": 1.9268,
      "step": 5160
    },
    {
      "epoch": 0.9754716981132076,
      "grad_norm": 0.7144139409065247,
      "learning_rate": 0.00040247169811320754,
      "loss": 1.9916,
      "step": 5170
    },
    {
      "epoch": 0.9773584905660377,
      "grad_norm": 0.7138350605964661,
      "learning_rate": 0.0004022830188679245,
      "loss": 2.0107,
      "step": 5180
    },
    {
      "epoch": 0.9792452830188679,
      "grad_norm": 0.6303955316543579,
      "learning_rate": 0.0004020943396226415,
      "loss": 1.977,
      "step": 5190
    },
    {
      "epoch": 0.9811320754716981,
      "grad_norm": 0.6271638870239258,
      "learning_rate": 0.00040190566037735846,
      "loss": 1.9595,
      "step": 5200
    },
    {
      "epoch": 0.9830188679245283,
      "grad_norm": 0.818135142326355,
      "learning_rate": 0.0004017169811320755,
      "loss": 1.9188,
      "step": 5210
    },
    {
      "epoch": 0.9849056603773585,
      "grad_norm": 0.7344212532043457,
      "learning_rate": 0.0004015283018867925,
      "loss": 1.9212,
      "step": 5220
    },
    {
      "epoch": 0.9867924528301887,
      "grad_norm": 0.7253156304359436,
      "learning_rate": 0.00040133962264150944,
      "loss": 1.9458,
      "step": 5230
    },
    {
      "epoch": 0.9886792452830189,
      "grad_norm": 0.6981648802757263,
      "learning_rate": 0.00040115094339622645,
      "loss": 1.8898,
      "step": 5240
    },
    {
      "epoch": 0.9905660377358491,
      "grad_norm": 0.7204205393791199,
      "learning_rate": 0.0004009622641509434,
      "loss": 1.9319,
      "step": 5250
    },
    {
      "epoch": 0.9924528301886792,
      "grad_norm": 0.6767703294754028,
      "learning_rate": 0.0004007735849056604,
      "loss": 1.9118,
      "step": 5260
    },
    {
      "epoch": 0.9943396226415094,
      "grad_norm": 0.9097689390182495,
      "learning_rate": 0.00040058490566037737,
      "loss": 1.908,
      "step": 5270
    },
    {
      "epoch": 0.9962264150943396,
      "grad_norm": 0.7150821089744568,
      "learning_rate": 0.0004003962264150943,
      "loss": 1.9424,
      "step": 5280
    },
    {
      "epoch": 0.9981132075471698,
      "grad_norm": 0.7475194931030273,
      "learning_rate": 0.00040020754716981133,
      "loss": 1.932,
      "step": 5290
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.501847743988037,
      "learning_rate": 0.0004000188679245283,
      "loss": 1.8953,
      "step": 5300
    },
    {
      "epoch": 1.0018867924528303,
      "grad_norm": 0.7319660782814026,
      "learning_rate": 0.0003998301886792453,
      "loss": 1.8448,
      "step": 5310
    },
    {
      "epoch": 1.0037735849056604,
      "grad_norm": 0.7994239330291748,
      "learning_rate": 0.00039964150943396225,
      "loss": 1.9627,
      "step": 5320
    },
    {
      "epoch": 1.0056603773584907,
      "grad_norm": 0.801516592502594,
      "learning_rate": 0.0003994528301886792,
      "loss": 1.9175,
      "step": 5330
    },
    {
      "epoch": 1.0075471698113208,
      "grad_norm": 0.7753776907920837,
      "learning_rate": 0.00039926415094339627,
      "loss": 1.8879,
      "step": 5340
    },
    {
      "epoch": 1.009433962264151,
      "grad_norm": 0.696719765663147,
      "learning_rate": 0.0003990754716981132,
      "loss": 1.8897,
      "step": 5350
    },
    {
      "epoch": 1.0113207547169811,
      "grad_norm": 0.6777734160423279,
      "learning_rate": 0.00039888679245283023,
      "loss": 1.989,
      "step": 5360
    },
    {
      "epoch": 1.0132075471698114,
      "grad_norm": 0.7308950424194336,
      "learning_rate": 0.0003986981132075472,
      "loss": 1.9361,
      "step": 5370
    },
    {
      "epoch": 1.0150943396226415,
      "grad_norm": 0.6990038156509399,
      "learning_rate": 0.00039850943396226414,
      "loss": 1.8845,
      "step": 5380
    },
    {
      "epoch": 1.0169811320754718,
      "grad_norm": 0.6399896144866943,
      "learning_rate": 0.00039832075471698115,
      "loss": 2.0204,
      "step": 5390
    },
    {
      "epoch": 1.0188679245283019,
      "grad_norm": 0.675154983997345,
      "learning_rate": 0.0003981320754716981,
      "loss": 2.0427,
      "step": 5400
    },
    {
      "epoch": 1.0207547169811322,
      "grad_norm": 0.7232956290245056,
      "learning_rate": 0.0003979433962264151,
      "loss": 1.8866,
      "step": 5410
    },
    {
      "epoch": 1.0226415094339623,
      "grad_norm": 0.7668505907058716,
      "learning_rate": 0.00039775471698113207,
      "loss": 1.9342,
      "step": 5420
    },
    {
      "epoch": 1.0245283018867926,
      "grad_norm": 0.7536171674728394,
      "learning_rate": 0.000397566037735849,
      "loss": 1.9988,
      "step": 5430
    },
    {
      "epoch": 1.0264150943396226,
      "grad_norm": 0.6534638404846191,
      "learning_rate": 0.00039737735849056604,
      "loss": 1.8755,
      "step": 5440
    },
    {
      "epoch": 1.028301886792453,
      "grad_norm": 0.7340596318244934,
      "learning_rate": 0.000397188679245283,
      "loss": 1.8718,
      "step": 5450
    },
    {
      "epoch": 1.030188679245283,
      "grad_norm": 0.7029483914375305,
      "learning_rate": 0.00039700000000000005,
      "loss": 1.9361,
      "step": 5460
    },
    {
      "epoch": 1.0320754716981133,
      "grad_norm": 0.68878573179245,
      "learning_rate": 0.000396811320754717,
      "loss": 1.8798,
      "step": 5470
    },
    {
      "epoch": 1.0339622641509434,
      "grad_norm": 0.6959247589111328,
      "learning_rate": 0.00039662264150943396,
      "loss": 1.9633,
      "step": 5480
    },
    {
      "epoch": 1.0358490566037737,
      "grad_norm": 0.7910130023956299,
      "learning_rate": 0.000396433962264151,
      "loss": 1.8723,
      "step": 5490
    },
    {
      "epoch": 1.0377358490566038,
      "grad_norm": 0.7210732698440552,
      "learning_rate": 0.00039624528301886793,
      "loss": 1.8841,
      "step": 5500
    },
    {
      "epoch": 1.039622641509434,
      "grad_norm": 0.7492297291755676,
      "learning_rate": 0.00039605660377358494,
      "loss": 1.9729,
      "step": 5510
    },
    {
      "epoch": 1.0415094339622641,
      "grad_norm": 0.7576823830604553,
      "learning_rate": 0.0003958679245283019,
      "loss": 1.9852,
      "step": 5520
    },
    {
      "epoch": 1.0433962264150944,
      "grad_norm": 0.7269137501716614,
      "learning_rate": 0.00039567924528301885,
      "loss": 1.9362,
      "step": 5530
    },
    {
      "epoch": 1.0452830188679245,
      "grad_norm": 0.6562398672103882,
      "learning_rate": 0.00039549056603773586,
      "loss": 1.8989,
      "step": 5540
    },
    {
      "epoch": 1.0471698113207548,
      "grad_norm": 0.8767887949943542,
      "learning_rate": 0.0003953018867924528,
      "loss": 1.9407,
      "step": 5550
    },
    {
      "epoch": 1.049056603773585,
      "grad_norm": 0.6913219094276428,
      "learning_rate": 0.0003951132075471698,
      "loss": 1.913,
      "step": 5560
    },
    {
      "epoch": 1.0509433962264152,
      "grad_norm": 0.7310629487037659,
      "learning_rate": 0.0003949245283018868,
      "loss": 1.8704,
      "step": 5570
    },
    {
      "epoch": 1.0528301886792453,
      "grad_norm": 0.7344304323196411,
      "learning_rate": 0.00039473584905660373,
      "loss": 1.9484,
      "step": 5580
    },
    {
      "epoch": 1.0547169811320756,
      "grad_norm": 0.7444815039634705,
      "learning_rate": 0.0003945471698113208,
      "loss": 1.9418,
      "step": 5590
    },
    {
      "epoch": 1.0566037735849056,
      "grad_norm": 0.6358193159103394,
      "learning_rate": 0.00039435849056603775,
      "loss": 2.0638,
      "step": 5600
    },
    {
      "epoch": 1.058490566037736,
      "grad_norm": 0.7303314805030823,
      "learning_rate": 0.00039416981132075476,
      "loss": 1.9362,
      "step": 5610
    },
    {
      "epoch": 1.060377358490566,
      "grad_norm": 0.6817697286605835,
      "learning_rate": 0.0003939811320754717,
      "loss": 1.9428,
      "step": 5620
    },
    {
      "epoch": 1.0622641509433963,
      "grad_norm": 0.6934324502944946,
      "learning_rate": 0.00039379245283018867,
      "loss": 1.9402,
      "step": 5630
    },
    {
      "epoch": 1.0641509433962264,
      "grad_norm": 0.7875274419784546,
      "learning_rate": 0.0003936037735849057,
      "loss": 1.9455,
      "step": 5640
    },
    {
      "epoch": 1.0660377358490567,
      "grad_norm": 0.6551657915115356,
      "learning_rate": 0.00039341509433962263,
      "loss": 1.8872,
      "step": 5650
    },
    {
      "epoch": 1.0679245283018868,
      "grad_norm": 0.7309020757675171,
      "learning_rate": 0.00039322641509433964,
      "loss": 1.9007,
      "step": 5660
    },
    {
      "epoch": 1.069811320754717,
      "grad_norm": 0.7430050373077393,
      "learning_rate": 0.0003930377358490566,
      "loss": 1.9517,
      "step": 5670
    },
    {
      "epoch": 1.0716981132075472,
      "grad_norm": 0.7152501344680786,
      "learning_rate": 0.00039284905660377355,
      "loss": 1.969,
      "step": 5680
    },
    {
      "epoch": 1.0735849056603775,
      "grad_norm": 0.7329331040382385,
      "learning_rate": 0.00039266037735849056,
      "loss": 1.8525,
      "step": 5690
    },
    {
      "epoch": 1.0754716981132075,
      "grad_norm": 0.7076339721679688,
      "learning_rate": 0.0003924716981132075,
      "loss": 1.9066,
      "step": 5700
    },
    {
      "epoch": 1.0773584905660378,
      "grad_norm": 0.8660421371459961,
      "learning_rate": 0.0003922830188679246,
      "loss": 1.835,
      "step": 5710
    },
    {
      "epoch": 1.079245283018868,
      "grad_norm": 0.725330650806427,
      "learning_rate": 0.00039209433962264154,
      "loss": 1.9125,
      "step": 5720
    },
    {
      "epoch": 1.0811320754716982,
      "grad_norm": 0.8152672052383423,
      "learning_rate": 0.0003919056603773585,
      "loss": 1.8541,
      "step": 5730
    },
    {
      "epoch": 1.0830188679245283,
      "grad_norm": 0.874040424823761,
      "learning_rate": 0.0003917169811320755,
      "loss": 1.9509,
      "step": 5740
    },
    {
      "epoch": 1.0849056603773586,
      "grad_norm": 0.8408787846565247,
      "learning_rate": 0.00039152830188679246,
      "loss": 1.8815,
      "step": 5750
    },
    {
      "epoch": 1.0867924528301887,
      "grad_norm": 0.6835271120071411,
      "learning_rate": 0.00039133962264150947,
      "loss": 1.9539,
      "step": 5760
    },
    {
      "epoch": 1.088679245283019,
      "grad_norm": 0.7690618634223938,
      "learning_rate": 0.0003911509433962264,
      "loss": 1.9638,
      "step": 5770
    },
    {
      "epoch": 1.090566037735849,
      "grad_norm": 0.6756152510643005,
      "learning_rate": 0.0003909622641509434,
      "loss": 1.9297,
      "step": 5780
    },
    {
      "epoch": 1.0924528301886793,
      "grad_norm": 0.8026017546653748,
      "learning_rate": 0.0003907735849056604,
      "loss": 1.9206,
      "step": 5790
    },
    {
      "epoch": 1.0943396226415094,
      "grad_norm": 0.7344474196434021,
      "learning_rate": 0.00039058490566037734,
      "loss": 1.9056,
      "step": 5800
    },
    {
      "epoch": 1.0962264150943397,
      "grad_norm": 0.7926591038703918,
      "learning_rate": 0.00039039622641509435,
      "loss": 1.9127,
      "step": 5810
    },
    {
      "epoch": 1.0981132075471698,
      "grad_norm": 0.816719114780426,
      "learning_rate": 0.0003902075471698113,
      "loss": 1.9595,
      "step": 5820
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.7022739052772522,
      "learning_rate": 0.0003900188679245283,
      "loss": 1.9526,
      "step": 5830
    },
    {
      "epoch": 1.1018867924528302,
      "grad_norm": 0.7136926054954529,
      "learning_rate": 0.0003898301886792453,
      "loss": 1.9251,
      "step": 5840
    },
    {
      "epoch": 1.1037735849056605,
      "grad_norm": 0.7446228861808777,
      "learning_rate": 0.0003896415094339623,
      "loss": 1.8913,
      "step": 5850
    },
    {
      "epoch": 1.1056603773584905,
      "grad_norm": 0.6614329814910889,
      "learning_rate": 0.0003894528301886793,
      "loss": 2.0303,
      "step": 5860
    },
    {
      "epoch": 1.1075471698113208,
      "grad_norm": 0.7163876295089722,
      "learning_rate": 0.00038926415094339624,
      "loss": 1.9017,
      "step": 5870
    },
    {
      "epoch": 1.109433962264151,
      "grad_norm": 0.7898224592208862,
      "learning_rate": 0.0003890754716981132,
      "loss": 1.8934,
      "step": 5880
    },
    {
      "epoch": 1.1113207547169812,
      "grad_norm": 0.689737856388092,
      "learning_rate": 0.0003888867924528302,
      "loss": 1.9075,
      "step": 5890
    },
    {
      "epoch": 1.1132075471698113,
      "grad_norm": 0.6890670657157898,
      "learning_rate": 0.00038869811320754716,
      "loss": 1.932,
      "step": 5900
    },
    {
      "epoch": 1.1150943396226416,
      "grad_norm": 0.7347964644432068,
      "learning_rate": 0.00038850943396226417,
      "loss": 1.9528,
      "step": 5910
    },
    {
      "epoch": 1.1169811320754717,
      "grad_norm": 0.7833731770515442,
      "learning_rate": 0.0003883207547169811,
      "loss": 1.8341,
      "step": 5920
    },
    {
      "epoch": 1.118867924528302,
      "grad_norm": 0.7472914457321167,
      "learning_rate": 0.0003881320754716981,
      "loss": 1.9985,
      "step": 5930
    },
    {
      "epoch": 1.120754716981132,
      "grad_norm": 0.7095980048179626,
      "learning_rate": 0.0003879433962264151,
      "loss": 1.9389,
      "step": 5940
    },
    {
      "epoch": 1.1226415094339623,
      "grad_norm": 0.7457901239395142,
      "learning_rate": 0.00038775471698113205,
      "loss": 1.9717,
      "step": 5950
    },
    {
      "epoch": 1.1245283018867924,
      "grad_norm": 0.7899101376533508,
      "learning_rate": 0.0003875660377358491,
      "loss": 1.8795,
      "step": 5960
    },
    {
      "epoch": 1.1264150943396227,
      "grad_norm": 0.7401826977729797,
      "learning_rate": 0.00038737735849056606,
      "loss": 1.9031,
      "step": 5970
    },
    {
      "epoch": 1.1283018867924528,
      "grad_norm": 0.7188355326652527,
      "learning_rate": 0.000387188679245283,
      "loss": 1.8874,
      "step": 5980
    },
    {
      "epoch": 1.130188679245283,
      "grad_norm": 0.797664225101471,
      "learning_rate": 0.00038700000000000003,
      "loss": 1.9792,
      "step": 5990
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 0.6959546208381653,
      "learning_rate": 0.000386811320754717,
      "loss": 1.9481,
      "step": 6000
    },
    {
      "epoch": 1.1339622641509435,
      "grad_norm": 0.7173688411712646,
      "learning_rate": 0.000386622641509434,
      "loss": 1.8819,
      "step": 6010
    },
    {
      "epoch": 1.1358490566037736,
      "grad_norm": 0.8083027601242065,
      "learning_rate": 0.00038643396226415095,
      "loss": 1.905,
      "step": 6020
    },
    {
      "epoch": 1.1377358490566039,
      "grad_norm": 0.7674555778503418,
      "learning_rate": 0.0003862452830188679,
      "loss": 1.8196,
      "step": 6030
    },
    {
      "epoch": 1.139622641509434,
      "grad_norm": 0.7493259906768799,
      "learning_rate": 0.0003860566037735849,
      "loss": 1.9441,
      "step": 6040
    },
    {
      "epoch": 1.1415094339622642,
      "grad_norm": 0.8003576397895813,
      "learning_rate": 0.00038586792452830187,
      "loss": 1.9229,
      "step": 6050
    },
    {
      "epoch": 1.1433962264150943,
      "grad_norm": 0.7517796158790588,
      "learning_rate": 0.0003856792452830189,
      "loss": 1.8196,
      "step": 6060
    },
    {
      "epoch": 1.1452830188679246,
      "grad_norm": 0.7154359817504883,
      "learning_rate": 0.00038549056603773583,
      "loss": 1.9052,
      "step": 6070
    },
    {
      "epoch": 1.1471698113207547,
      "grad_norm": 0.778743326663971,
      "learning_rate": 0.00038530188679245284,
      "loss": 2.0003,
      "step": 6080
    },
    {
      "epoch": 1.149056603773585,
      "grad_norm": 0.8167411684989929,
      "learning_rate": 0.00038511320754716985,
      "loss": 1.8886,
      "step": 6090
    },
    {
      "epoch": 1.150943396226415,
      "grad_norm": 0.8972591757774353,
      "learning_rate": 0.0003849245283018868,
      "loss": 1.8988,
      "step": 6100
    },
    {
      "epoch": 1.1528301886792454,
      "grad_norm": 0.734851598739624,
      "learning_rate": 0.0003847358490566038,
      "loss": 1.8991,
      "step": 6110
    },
    {
      "epoch": 1.1547169811320754,
      "grad_norm": 0.7141783237457275,
      "learning_rate": 0.00038454716981132077,
      "loss": 2.0099,
      "step": 6120
    },
    {
      "epoch": 1.1566037735849057,
      "grad_norm": 0.7052371501922607,
      "learning_rate": 0.0003843584905660377,
      "loss": 1.8752,
      "step": 6130
    },
    {
      "epoch": 1.1584905660377358,
      "grad_norm": 0.7722830772399902,
      "learning_rate": 0.00038416981132075473,
      "loss": 1.8971,
      "step": 6140
    },
    {
      "epoch": 1.1603773584905661,
      "grad_norm": 0.8068470358848572,
      "learning_rate": 0.0003839811320754717,
      "loss": 1.8815,
      "step": 6150
    },
    {
      "epoch": 1.1622641509433962,
      "grad_norm": 0.8185533285140991,
      "learning_rate": 0.0003837924528301887,
      "loss": 1.8894,
      "step": 6160
    },
    {
      "epoch": 1.1641509433962265,
      "grad_norm": 0.694071888923645,
      "learning_rate": 0.00038360377358490565,
      "loss": 1.9194,
      "step": 6170
    },
    {
      "epoch": 1.1660377358490566,
      "grad_norm": 0.8729011416435242,
      "learning_rate": 0.0003834150943396226,
      "loss": 1.7882,
      "step": 6180
    },
    {
      "epoch": 1.1679245283018869,
      "grad_norm": 0.7797282934188843,
      "learning_rate": 0.0003832264150943396,
      "loss": 1.8393,
      "step": 6190
    },
    {
      "epoch": 1.169811320754717,
      "grad_norm": 0.7502763271331787,
      "learning_rate": 0.0003830377358490566,
      "loss": 1.9335,
      "step": 6200
    },
    {
      "epoch": 1.1716981132075472,
      "grad_norm": 0.7214317321777344,
      "learning_rate": 0.00038284905660377364,
      "loss": 2.0027,
      "step": 6210
    },
    {
      "epoch": 1.1735849056603773,
      "grad_norm": 0.7822681069374084,
      "learning_rate": 0.0003826603773584906,
      "loss": 1.9482,
      "step": 6220
    },
    {
      "epoch": 1.1754716981132076,
      "grad_norm": 0.7078913450241089,
      "learning_rate": 0.00038247169811320755,
      "loss": 1.9256,
      "step": 6230
    },
    {
      "epoch": 1.1773584905660377,
      "grad_norm": 0.7050564289093018,
      "learning_rate": 0.00038228301886792456,
      "loss": 1.9189,
      "step": 6240
    },
    {
      "epoch": 1.179245283018868,
      "grad_norm": 0.8640615940093994,
      "learning_rate": 0.0003820943396226415,
      "loss": 1.9495,
      "step": 6250
    },
    {
      "epoch": 1.181132075471698,
      "grad_norm": 0.8444417715072632,
      "learning_rate": 0.0003819056603773585,
      "loss": 1.9206,
      "step": 6260
    },
    {
      "epoch": 1.1830188679245284,
      "grad_norm": 0.68413907289505,
      "learning_rate": 0.0003817169811320755,
      "loss": 1.9506,
      "step": 6270
    },
    {
      "epoch": 1.1849056603773584,
      "grad_norm": 0.7122321128845215,
      "learning_rate": 0.00038152830188679243,
      "loss": 1.9405,
      "step": 6280
    },
    {
      "epoch": 1.1867924528301887,
      "grad_norm": 0.7475281953811646,
      "learning_rate": 0.00038133962264150944,
      "loss": 1.8972,
      "step": 6290
    },
    {
      "epoch": 1.1886792452830188,
      "grad_norm": 0.693213164806366,
      "learning_rate": 0.0003811509433962264,
      "loss": 1.9224,
      "step": 6300
    },
    {
      "epoch": 1.1905660377358491,
      "grad_norm": 0.6648290157318115,
      "learning_rate": 0.0003809622641509434,
      "loss": 1.9582,
      "step": 6310
    },
    {
      "epoch": 1.1924528301886792,
      "grad_norm": 0.6946446299552917,
      "learning_rate": 0.0003807735849056604,
      "loss": 1.8198,
      "step": 6320
    },
    {
      "epoch": 1.1943396226415095,
      "grad_norm": 0.8071405291557312,
      "learning_rate": 0.00038058490566037737,
      "loss": 1.9603,
      "step": 6330
    },
    {
      "epoch": 1.1962264150943396,
      "grad_norm": 0.7046886086463928,
      "learning_rate": 0.0003803962264150944,
      "loss": 1.9752,
      "step": 6340
    },
    {
      "epoch": 1.1981132075471699,
      "grad_norm": 0.6850599646568298,
      "learning_rate": 0.00038020754716981133,
      "loss": 1.9073,
      "step": 6350
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.7252531051635742,
      "learning_rate": 0.00038001886792452834,
      "loss": 1.9233,
      "step": 6360
    },
    {
      "epoch": 1.2018867924528303,
      "grad_norm": 0.7434660196304321,
      "learning_rate": 0.0003798301886792453,
      "loss": 1.9043,
      "step": 6370
    },
    {
      "epoch": 1.2037735849056603,
      "grad_norm": 0.7907554507255554,
      "learning_rate": 0.00037964150943396225,
      "loss": 1.9007,
      "step": 6380
    },
    {
      "epoch": 1.2056603773584906,
      "grad_norm": 0.7922033071517944,
      "learning_rate": 0.00037945283018867926,
      "loss": 1.8474,
      "step": 6390
    },
    {
      "epoch": 1.2075471698113207,
      "grad_norm": 0.6215372681617737,
      "learning_rate": 0.0003792641509433962,
      "loss": 1.9095,
      "step": 6400
    },
    {
      "epoch": 1.209433962264151,
      "grad_norm": 0.7094129920005798,
      "learning_rate": 0.0003790754716981132,
      "loss": 1.9814,
      "step": 6410
    },
    {
      "epoch": 1.211320754716981,
      "grad_norm": 0.7439950704574585,
      "learning_rate": 0.0003788867924528302,
      "loss": 1.9334,
      "step": 6420
    },
    {
      "epoch": 1.2132075471698114,
      "grad_norm": 0.7669808864593506,
      "learning_rate": 0.00037869811320754714,
      "loss": 1.9265,
      "step": 6430
    },
    {
      "epoch": 1.2150943396226415,
      "grad_norm": 0.7951804995536804,
      "learning_rate": 0.00037850943396226414,
      "loss": 1.9333,
      "step": 6440
    },
    {
      "epoch": 1.2169811320754718,
      "grad_norm": 0.7778465747833252,
      "learning_rate": 0.00037832075471698115,
      "loss": 1.9685,
      "step": 6450
    },
    {
      "epoch": 1.2188679245283018,
      "grad_norm": 0.8052546381950378,
      "learning_rate": 0.00037813207547169816,
      "loss": 2.0145,
      "step": 6460
    },
    {
      "epoch": 1.2207547169811321,
      "grad_norm": 0.7491132616996765,
      "learning_rate": 0.0003779433962264151,
      "loss": 1.9463,
      "step": 6470
    },
    {
      "epoch": 1.2226415094339622,
      "grad_norm": 0.7805154919624329,
      "learning_rate": 0.0003777547169811321,
      "loss": 1.9478,
      "step": 6480
    },
    {
      "epoch": 1.2245283018867925,
      "grad_norm": 0.7238505482673645,
      "learning_rate": 0.0003775660377358491,
      "loss": 1.871,
      "step": 6490
    },
    {
      "epoch": 1.2264150943396226,
      "grad_norm": 0.7269676923751831,
      "learning_rate": 0.00037737735849056604,
      "loss": 1.9666,
      "step": 6500
    },
    {
      "epoch": 1.228301886792453,
      "grad_norm": 0.668121874332428,
      "learning_rate": 0.00037718867924528305,
      "loss": 1.9074,
      "step": 6510
    },
    {
      "epoch": 1.230188679245283,
      "grad_norm": 0.798902153968811,
      "learning_rate": 0.000377,
      "loss": 1.956,
      "step": 6520
    },
    {
      "epoch": 1.2320754716981133,
      "grad_norm": 0.7711786031723022,
      "learning_rate": 0.00037681132075471696,
      "loss": 1.8967,
      "step": 6530
    },
    {
      "epoch": 1.2339622641509433,
      "grad_norm": 0.705365002155304,
      "learning_rate": 0.00037662264150943397,
      "loss": 1.9267,
      "step": 6540
    },
    {
      "epoch": 1.2358490566037736,
      "grad_norm": 0.7164071798324585,
      "learning_rate": 0.0003764339622641509,
      "loss": 1.889,
      "step": 6550
    },
    {
      "epoch": 1.2377358490566037,
      "grad_norm": 0.751915693283081,
      "learning_rate": 0.00037624528301886793,
      "loss": 1.9103,
      "step": 6560
    },
    {
      "epoch": 1.239622641509434,
      "grad_norm": 0.7717148661613464,
      "learning_rate": 0.00037605660377358494,
      "loss": 1.9316,
      "step": 6570
    },
    {
      "epoch": 1.241509433962264,
      "grad_norm": 0.7904693484306335,
      "learning_rate": 0.0003758679245283019,
      "loss": 1.8731,
      "step": 6580
    },
    {
      "epoch": 1.2433962264150944,
      "grad_norm": 0.8320675492286682,
      "learning_rate": 0.0003756792452830189,
      "loss": 1.9023,
      "step": 6590
    },
    {
      "epoch": 1.2452830188679245,
      "grad_norm": 0.73769611120224,
      "learning_rate": 0.00037549056603773586,
      "loss": 1.97,
      "step": 6600
    },
    {
      "epoch": 1.2471698113207548,
      "grad_norm": 0.721815824508667,
      "learning_rate": 0.00037530188679245287,
      "loss": 1.9772,
      "step": 6610
    },
    {
      "epoch": 1.2490566037735849,
      "grad_norm": 0.7464032769203186,
      "learning_rate": 0.0003751132075471698,
      "loss": 1.8991,
      "step": 6620
    },
    {
      "epoch": 1.2509433962264151,
      "grad_norm": 0.8250722289085388,
      "learning_rate": 0.0003749245283018868,
      "loss": 1.8425,
      "step": 6630
    },
    {
      "epoch": 1.2528301886792452,
      "grad_norm": 0.683664858341217,
      "learning_rate": 0.0003747358490566038,
      "loss": 1.9666,
      "step": 6640
    },
    {
      "epoch": 1.2547169811320755,
      "grad_norm": 0.7275657057762146,
      "learning_rate": 0.00037454716981132074,
      "loss": 1.942,
      "step": 6650
    },
    {
      "epoch": 1.2566037735849056,
      "grad_norm": 0.7908602356910706,
      "learning_rate": 0.00037435849056603775,
      "loss": 1.9216,
      "step": 6660
    },
    {
      "epoch": 1.258490566037736,
      "grad_norm": 0.901859700679779,
      "learning_rate": 0.0003741698113207547,
      "loss": 1.9238,
      "step": 6670
    },
    {
      "epoch": 1.260377358490566,
      "grad_norm": 0.7135478258132935,
      "learning_rate": 0.00037398113207547166,
      "loss": 1.9616,
      "step": 6680
    },
    {
      "epoch": 1.2622641509433963,
      "grad_norm": 0.6613050103187561,
      "learning_rate": 0.0003737924528301887,
      "loss": 1.9423,
      "step": 6690
    },
    {
      "epoch": 1.2641509433962264,
      "grad_norm": 0.7688335180282593,
      "learning_rate": 0.0003736037735849057,
      "loss": 1.9969,
      "step": 6700
    },
    {
      "epoch": 1.2660377358490567,
      "grad_norm": 0.7540463805198669,
      "learning_rate": 0.0003734150943396227,
      "loss": 1.9373,
      "step": 6710
    },
    {
      "epoch": 1.2679245283018867,
      "grad_norm": 0.7695552110671997,
      "learning_rate": 0.00037322641509433965,
      "loss": 1.9397,
      "step": 6720
    },
    {
      "epoch": 1.269811320754717,
      "grad_norm": 0.6647151708602905,
      "learning_rate": 0.0003730377358490566,
      "loss": 1.8485,
      "step": 6730
    },
    {
      "epoch": 1.271698113207547,
      "grad_norm": 0.8332630395889282,
      "learning_rate": 0.0003728490566037736,
      "loss": 1.9339,
      "step": 6740
    },
    {
      "epoch": 1.2735849056603774,
      "grad_norm": 0.8725355863571167,
      "learning_rate": 0.00037266037735849057,
      "loss": 1.9037,
      "step": 6750
    },
    {
      "epoch": 1.2754716981132075,
      "grad_norm": 0.6989105939865112,
      "learning_rate": 0.0003724716981132076,
      "loss": 1.8431,
      "step": 6760
    },
    {
      "epoch": 1.2773584905660378,
      "grad_norm": 0.6861145496368408,
      "learning_rate": 0.00037228301886792453,
      "loss": 1.9573,
      "step": 6770
    },
    {
      "epoch": 1.2792452830188679,
      "grad_norm": 0.8101359009742737,
      "learning_rate": 0.0003720943396226415,
      "loss": 1.93,
      "step": 6780
    },
    {
      "epoch": 1.2811320754716982,
      "grad_norm": 0.6919932961463928,
      "learning_rate": 0.0003719056603773585,
      "loss": 1.974,
      "step": 6790
    },
    {
      "epoch": 1.2830188679245282,
      "grad_norm": 0.7507563829421997,
      "learning_rate": 0.00037171698113207545,
      "loss": 1.8433,
      "step": 6800
    },
    {
      "epoch": 1.2849056603773585,
      "grad_norm": 0.7290555834770203,
      "learning_rate": 0.0003715283018867925,
      "loss": 1.9211,
      "step": 6810
    },
    {
      "epoch": 1.2867924528301886,
      "grad_norm": 0.7156664729118347,
      "learning_rate": 0.00037133962264150947,
      "loss": 1.876,
      "step": 6820
    },
    {
      "epoch": 1.288679245283019,
      "grad_norm": 0.8509446382522583,
      "learning_rate": 0.0003711509433962264,
      "loss": 1.9255,
      "step": 6830
    },
    {
      "epoch": 1.290566037735849,
      "grad_norm": 0.7234275341033936,
      "learning_rate": 0.00037096226415094343,
      "loss": 1.9395,
      "step": 6840
    },
    {
      "epoch": 1.2924528301886793,
      "grad_norm": 0.6674511432647705,
      "learning_rate": 0.0003707735849056604,
      "loss": 1.9746,
      "step": 6850
    },
    {
      "epoch": 1.2943396226415094,
      "grad_norm": 0.7299849390983582,
      "learning_rate": 0.0003705849056603774,
      "loss": 1.9599,
      "step": 6860
    },
    {
      "epoch": 1.2962264150943397,
      "grad_norm": 0.7764354348182678,
      "learning_rate": 0.00037039622641509435,
      "loss": 1.8462,
      "step": 6870
    },
    {
      "epoch": 1.2981132075471697,
      "grad_norm": 0.7807000875473022,
      "learning_rate": 0.0003702075471698113,
      "loss": 1.8856,
      "step": 6880
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.7636476755142212,
      "learning_rate": 0.0003700188679245283,
      "loss": 1.9322,
      "step": 6890
    },
    {
      "epoch": 1.3018867924528301,
      "grad_norm": 0.7395392060279846,
      "learning_rate": 0.00036983018867924527,
      "loss": 1.9638,
      "step": 6900
    },
    {
      "epoch": 1.3037735849056604,
      "grad_norm": 0.8032942414283752,
      "learning_rate": 0.0003696415094339623,
      "loss": 1.9513,
      "step": 6910
    },
    {
      "epoch": 1.3056603773584905,
      "grad_norm": 0.801757276058197,
      "learning_rate": 0.00036945283018867923,
      "loss": 1.9714,
      "step": 6920
    },
    {
      "epoch": 1.3075471698113208,
      "grad_norm": 0.8829458951950073,
      "learning_rate": 0.0003692641509433962,
      "loss": 1.9168,
      "step": 6930
    },
    {
      "epoch": 1.3094339622641509,
      "grad_norm": 0.7991651892662048,
      "learning_rate": 0.00036907547169811325,
      "loss": 2.0149,
      "step": 6940
    },
    {
      "epoch": 1.3113207547169812,
      "grad_norm": 0.6632213592529297,
      "learning_rate": 0.0003688867924528302,
      "loss": 1.9868,
      "step": 6950
    },
    {
      "epoch": 1.3132075471698113,
      "grad_norm": 0.7318844795227051,
      "learning_rate": 0.0003686981132075472,
      "loss": 1.9898,
      "step": 6960
    },
    {
      "epoch": 1.3150943396226416,
      "grad_norm": 0.7637684345245361,
      "learning_rate": 0.00036850943396226417,
      "loss": 1.9485,
      "step": 6970
    },
    {
      "epoch": 1.3169811320754716,
      "grad_norm": 0.8247295022010803,
      "learning_rate": 0.00036832075471698113,
      "loss": 1.8696,
      "step": 6980
    },
    {
      "epoch": 1.318867924528302,
      "grad_norm": 0.6826764941215515,
      "learning_rate": 0.00036813207547169814,
      "loss": 1.9234,
      "step": 6990
    },
    {
      "epoch": 1.320754716981132,
      "grad_norm": 0.7298246622085571,
      "learning_rate": 0.0003679433962264151,
      "loss": 1.959,
      "step": 7000
    },
    {
      "epoch": 1.3226415094339623,
      "grad_norm": 0.818901538848877,
      "learning_rate": 0.0003677547169811321,
      "loss": 1.9419,
      "step": 7010
    },
    {
      "epoch": 1.3245283018867924,
      "grad_norm": 0.8940302133560181,
      "learning_rate": 0.00036756603773584906,
      "loss": 1.9668,
      "step": 7020
    },
    {
      "epoch": 1.3264150943396227,
      "grad_norm": 0.7103164196014404,
      "learning_rate": 0.000367377358490566,
      "loss": 1.9533,
      "step": 7030
    },
    {
      "epoch": 1.3283018867924528,
      "grad_norm": 0.7460477948188782,
      "learning_rate": 0.000367188679245283,
      "loss": 1.9469,
      "step": 7040
    },
    {
      "epoch": 1.330188679245283,
      "grad_norm": 0.7867244482040405,
      "learning_rate": 0.000367,
      "loss": 1.8761,
      "step": 7050
    },
    {
      "epoch": 1.3320754716981131,
      "grad_norm": 0.7082478404045105,
      "learning_rate": 0.00036681132075471704,
      "loss": 1.9871,
      "step": 7060
    },
    {
      "epoch": 1.3339622641509434,
      "grad_norm": 0.6688833236694336,
      "learning_rate": 0.000366622641509434,
      "loss": 1.9452,
      "step": 7070
    },
    {
      "epoch": 1.3358490566037735,
      "grad_norm": 0.7144750952720642,
      "learning_rate": 0.00036643396226415095,
      "loss": 1.8664,
      "step": 7080
    },
    {
      "epoch": 1.3377358490566038,
      "grad_norm": 0.7520727515220642,
      "learning_rate": 0.00036624528301886796,
      "loss": 1.8394,
      "step": 7090
    },
    {
      "epoch": 1.3396226415094339,
      "grad_norm": 0.7182208299636841,
      "learning_rate": 0.0003660566037735849,
      "loss": 1.9496,
      "step": 7100
    },
    {
      "epoch": 1.3415094339622642,
      "grad_norm": 0.7214614748954773,
      "learning_rate": 0.0003658679245283019,
      "loss": 1.9829,
      "step": 7110
    },
    {
      "epoch": 1.3433962264150943,
      "grad_norm": 0.7191383242607117,
      "learning_rate": 0.0003656792452830189,
      "loss": 1.8472,
      "step": 7120
    },
    {
      "epoch": 1.3452830188679246,
      "grad_norm": 0.7262901663780212,
      "learning_rate": 0.00036549056603773583,
      "loss": 1.9772,
      "step": 7130
    },
    {
      "epoch": 1.3471698113207546,
      "grad_norm": 0.7358649373054504,
      "learning_rate": 0.00036530188679245284,
      "loss": 1.9353,
      "step": 7140
    },
    {
      "epoch": 1.349056603773585,
      "grad_norm": 0.9505597352981567,
      "learning_rate": 0.0003651132075471698,
      "loss": 1.8428,
      "step": 7150
    },
    {
      "epoch": 1.350943396226415,
      "grad_norm": 0.7823827862739563,
      "learning_rate": 0.0003649245283018868,
      "loss": 1.9825,
      "step": 7160
    },
    {
      "epoch": 1.3528301886792453,
      "grad_norm": 0.693640410900116,
      "learning_rate": 0.00036473584905660376,
      "loss": 1.9046,
      "step": 7170
    },
    {
      "epoch": 1.3547169811320754,
      "grad_norm": 0.7012167572975159,
      "learning_rate": 0.00036454716981132077,
      "loss": 2.0071,
      "step": 7180
    },
    {
      "epoch": 1.3566037735849057,
      "grad_norm": 0.681671142578125,
      "learning_rate": 0.0003643584905660378,
      "loss": 1.8219,
      "step": 7190
    },
    {
      "epoch": 1.3584905660377358,
      "grad_norm": 0.7835192680358887,
      "learning_rate": 0.00036416981132075474,
      "loss": 1.8866,
      "step": 7200
    },
    {
      "epoch": 1.360377358490566,
      "grad_norm": 0.7951058745384216,
      "learning_rate": 0.00036398113207547174,
      "loss": 1.8933,
      "step": 7210
    },
    {
      "epoch": 1.3622641509433961,
      "grad_norm": 0.7220962643623352,
      "learning_rate": 0.0003637924528301887,
      "loss": 1.8637,
      "step": 7220
    },
    {
      "epoch": 1.3641509433962264,
      "grad_norm": 0.881455659866333,
      "learning_rate": 0.00036360377358490566,
      "loss": 1.9206,
      "step": 7230
    },
    {
      "epoch": 1.3660377358490565,
      "grad_norm": 0.8146494030952454,
      "learning_rate": 0.00036341509433962266,
      "loss": 1.9547,
      "step": 7240
    },
    {
      "epoch": 1.3679245283018868,
      "grad_norm": 0.6948233246803284,
      "learning_rate": 0.0003632264150943396,
      "loss": 1.8787,
      "step": 7250
    },
    {
      "epoch": 1.369811320754717,
      "grad_norm": 0.7646652460098267,
      "learning_rate": 0.00036303773584905663,
      "loss": 1.8998,
      "step": 7260
    },
    {
      "epoch": 1.3716981132075472,
      "grad_norm": 0.6704815626144409,
      "learning_rate": 0.0003628490566037736,
      "loss": 1.8915,
      "step": 7270
    },
    {
      "epoch": 1.3735849056603773,
      "grad_norm": 0.7548381090164185,
      "learning_rate": 0.00036266037735849054,
      "loss": 1.9294,
      "step": 7280
    },
    {
      "epoch": 1.3754716981132076,
      "grad_norm": 0.7217006683349609,
      "learning_rate": 0.00036247169811320755,
      "loss": 1.9306,
      "step": 7290
    },
    {
      "epoch": 1.3773584905660377,
      "grad_norm": 0.8130690455436707,
      "learning_rate": 0.0003622830188679245,
      "loss": 1.876,
      "step": 7300
    },
    {
      "epoch": 1.379245283018868,
      "grad_norm": 0.7859285473823547,
      "learning_rate": 0.00036209433962264157,
      "loss": 1.9318,
      "step": 7310
    },
    {
      "epoch": 1.3811320754716983,
      "grad_norm": 0.7468209862709045,
      "learning_rate": 0.0003619056603773585,
      "loss": 1.9272,
      "step": 7320
    },
    {
      "epoch": 1.3830188679245283,
      "grad_norm": 0.6849137544631958,
      "learning_rate": 0.0003617169811320755,
      "loss": 1.8035,
      "step": 7330
    },
    {
      "epoch": 1.3849056603773584,
      "grad_norm": 0.8393987417221069,
      "learning_rate": 0.0003615283018867925,
      "loss": 1.9061,
      "step": 7340
    },
    {
      "epoch": 1.3867924528301887,
      "grad_norm": 0.7411864399909973,
      "learning_rate": 0.00036133962264150944,
      "loss": 1.9417,
      "step": 7350
    },
    {
      "epoch": 1.388679245283019,
      "grad_norm": 0.7907050251960754,
      "learning_rate": 0.00036115094339622645,
      "loss": 1.9878,
      "step": 7360
    },
    {
      "epoch": 1.390566037735849,
      "grad_norm": 0.7245079278945923,
      "learning_rate": 0.0003609622641509434,
      "loss": 1.905,
      "step": 7370
    },
    {
      "epoch": 1.3924528301886792,
      "grad_norm": 0.7028325200080872,
      "learning_rate": 0.00036077358490566036,
      "loss": 1.9716,
      "step": 7380
    },
    {
      "epoch": 1.3943396226415095,
      "grad_norm": 0.6983543634414673,
      "learning_rate": 0.00036058490566037737,
      "loss": 1.9642,
      "step": 7390
    },
    {
      "epoch": 1.3962264150943398,
      "grad_norm": 0.8533795475959778,
      "learning_rate": 0.0003603962264150943,
      "loss": 1.8545,
      "step": 7400
    },
    {
      "epoch": 1.3981132075471698,
      "grad_norm": 0.7826106548309326,
      "learning_rate": 0.00036020754716981133,
      "loss": 1.9299,
      "step": 7410
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.7951751947402954,
      "learning_rate": 0.0003600188679245283,
      "loss": 1.889,
      "step": 7420
    },
    {
      "epoch": 1.4018867924528302,
      "grad_norm": 0.665624737739563,
      "learning_rate": 0.0003598301886792453,
      "loss": 1.8585,
      "step": 7430
    },
    {
      "epoch": 1.4037735849056605,
      "grad_norm": 0.6945906281471252,
      "learning_rate": 0.0003596415094339623,
      "loss": 1.8802,
      "step": 7440
    },
    {
      "epoch": 1.4056603773584906,
      "grad_norm": 0.8585901260375977,
      "learning_rate": 0.00035945283018867926,
      "loss": 2.0075,
      "step": 7450
    },
    {
      "epoch": 1.4075471698113207,
      "grad_norm": 0.7707494497299194,
      "learning_rate": 0.0003592641509433962,
      "loss": 1.9168,
      "step": 7460
    },
    {
      "epoch": 1.409433962264151,
      "grad_norm": 0.7231889367103577,
      "learning_rate": 0.00035907547169811323,
      "loss": 1.9457,
      "step": 7470
    },
    {
      "epoch": 1.4113207547169813,
      "grad_norm": 0.8188368678092957,
      "learning_rate": 0.0003588867924528302,
      "loss": 1.8765,
      "step": 7480
    },
    {
      "epoch": 1.4132075471698113,
      "grad_norm": 0.7859367728233337,
      "learning_rate": 0.0003586981132075472,
      "loss": 1.9257,
      "step": 7490
    },
    {
      "epoch": 1.4150943396226414,
      "grad_norm": 0.7305241823196411,
      "learning_rate": 0.00035850943396226415,
      "loss": 1.9514,
      "step": 7500
    },
    {
      "epoch": 1.4169811320754717,
      "grad_norm": 0.839581310749054,
      "learning_rate": 0.0003583207547169811,
      "loss": 1.9257,
      "step": 7510
    },
    {
      "epoch": 1.418867924528302,
      "grad_norm": 0.6982666850090027,
      "learning_rate": 0.0003581320754716981,
      "loss": 1.9571,
      "step": 7520
    },
    {
      "epoch": 1.420754716981132,
      "grad_norm": 0.8724794387817383,
      "learning_rate": 0.00035794339622641507,
      "loss": 1.8861,
      "step": 7530
    },
    {
      "epoch": 1.4226415094339622,
      "grad_norm": 0.7681178450584412,
      "learning_rate": 0.0003577547169811321,
      "loss": 1.9422,
      "step": 7540
    },
    {
      "epoch": 1.4245283018867925,
      "grad_norm": 0.6865693926811218,
      "learning_rate": 0.0003575660377358491,
      "loss": 1.9024,
      "step": 7550
    },
    {
      "epoch": 1.4264150943396228,
      "grad_norm": 0.8104607462882996,
      "learning_rate": 0.00035737735849056604,
      "loss": 1.9271,
      "step": 7560
    },
    {
      "epoch": 1.4283018867924528,
      "grad_norm": 0.7979603409767151,
      "learning_rate": 0.00035718867924528305,
      "loss": 1.8824,
      "step": 7570
    },
    {
      "epoch": 1.430188679245283,
      "grad_norm": 0.7554429173469543,
      "learning_rate": 0.000357,
      "loss": 1.9561,
      "step": 7580
    },
    {
      "epoch": 1.4320754716981132,
      "grad_norm": 0.7823196053504944,
      "learning_rate": 0.000356811320754717,
      "loss": 1.8384,
      "step": 7590
    },
    {
      "epoch": 1.4339622641509435,
      "grad_norm": 0.8189241886138916,
      "learning_rate": 0.00035662264150943397,
      "loss": 1.9371,
      "step": 7600
    },
    {
      "epoch": 1.4358490566037736,
      "grad_norm": 0.7748880982398987,
      "learning_rate": 0.0003564339622641509,
      "loss": 1.858,
      "step": 7610
    },
    {
      "epoch": 1.4377358490566037,
      "grad_norm": 0.74085533618927,
      "learning_rate": 0.00035624528301886793,
      "loss": 1.974,
      "step": 7620
    },
    {
      "epoch": 1.439622641509434,
      "grad_norm": 0.7709797620773315,
      "learning_rate": 0.0003560566037735849,
      "loss": 1.9589,
      "step": 7630
    },
    {
      "epoch": 1.4415094339622643,
      "grad_norm": 0.7954145669937134,
      "learning_rate": 0.0003558679245283019,
      "loss": 1.9736,
      "step": 7640
    },
    {
      "epoch": 1.4433962264150944,
      "grad_norm": 0.7411458492279053,
      "learning_rate": 0.00035567924528301885,
      "loss": 1.8823,
      "step": 7650
    },
    {
      "epoch": 1.4452830188679244,
      "grad_norm": 0.8416046500205994,
      "learning_rate": 0.0003554905660377358,
      "loss": 1.8945,
      "step": 7660
    },
    {
      "epoch": 1.4471698113207547,
      "grad_norm": 0.7368668913841248,
      "learning_rate": 0.00035530188679245287,
      "loss": 1.9845,
      "step": 7670
    },
    {
      "epoch": 1.449056603773585,
      "grad_norm": 0.8095366954803467,
      "learning_rate": 0.0003551132075471698,
      "loss": 1.9523,
      "step": 7680
    },
    {
      "epoch": 1.450943396226415,
      "grad_norm": 0.7414485216140747,
      "learning_rate": 0.00035492452830188683,
      "loss": 1.8956,
      "step": 7690
    },
    {
      "epoch": 1.4528301886792452,
      "grad_norm": 0.7437664866447449,
      "learning_rate": 0.0003547358490566038,
      "loss": 1.9319,
      "step": 7700
    },
    {
      "epoch": 1.4547169811320755,
      "grad_norm": 0.7765570878982544,
      "learning_rate": 0.00035454716981132075,
      "loss": 1.851,
      "step": 7710
    },
    {
      "epoch": 1.4566037735849058,
      "grad_norm": 0.7078450918197632,
      "learning_rate": 0.00035435849056603775,
      "loss": 1.8738,
      "step": 7720
    },
    {
      "epoch": 1.4584905660377359,
      "grad_norm": 0.7737011909484863,
      "learning_rate": 0.0003541698113207547,
      "loss": 1.8847,
      "step": 7730
    },
    {
      "epoch": 1.460377358490566,
      "grad_norm": 0.8838794827461243,
      "learning_rate": 0.0003539811320754717,
      "loss": 1.9131,
      "step": 7740
    },
    {
      "epoch": 1.4622641509433962,
      "grad_norm": 0.7281463742256165,
      "learning_rate": 0.0003537924528301887,
      "loss": 1.9151,
      "step": 7750
    },
    {
      "epoch": 1.4641509433962265,
      "grad_norm": 0.7690781354904175,
      "learning_rate": 0.00035360377358490563,
      "loss": 1.8866,
      "step": 7760
    },
    {
      "epoch": 1.4660377358490566,
      "grad_norm": 0.7569634914398193,
      "learning_rate": 0.00035341509433962264,
      "loss": 1.8756,
      "step": 7770
    },
    {
      "epoch": 1.4679245283018867,
      "grad_norm": 0.878938615322113,
      "learning_rate": 0.0003532264150943396,
      "loss": 1.864,
      "step": 7780
    },
    {
      "epoch": 1.469811320754717,
      "grad_norm": 0.7182915806770325,
      "learning_rate": 0.0003530377358490566,
      "loss": 1.9694,
      "step": 7790
    },
    {
      "epoch": 1.4716981132075473,
      "grad_norm": 0.7143381237983704,
      "learning_rate": 0.0003528490566037736,
      "loss": 1.901,
      "step": 7800
    },
    {
      "epoch": 1.4735849056603774,
      "grad_norm": 0.7530377507209778,
      "learning_rate": 0.00035266037735849057,
      "loss": 1.8747,
      "step": 7810
    },
    {
      "epoch": 1.4754716981132074,
      "grad_norm": 0.6722423434257507,
      "learning_rate": 0.0003524716981132076,
      "loss": 1.7936,
      "step": 7820
    },
    {
      "epoch": 1.4773584905660377,
      "grad_norm": 0.7483815550804138,
      "learning_rate": 0.00035228301886792453,
      "loss": 1.9734,
      "step": 7830
    },
    {
      "epoch": 1.479245283018868,
      "grad_norm": 0.6943289041519165,
      "learning_rate": 0.00035209433962264154,
      "loss": 1.8549,
      "step": 7840
    },
    {
      "epoch": 1.4811320754716981,
      "grad_norm": 0.7986469864845276,
      "learning_rate": 0.0003519056603773585,
      "loss": 2.0118,
      "step": 7850
    },
    {
      "epoch": 1.4830188679245282,
      "grad_norm": 0.750528872013092,
      "learning_rate": 0.00035171698113207545,
      "loss": 1.9106,
      "step": 7860
    },
    {
      "epoch": 1.4849056603773585,
      "grad_norm": 0.71445232629776,
      "learning_rate": 0.00035152830188679246,
      "loss": 1.896,
      "step": 7870
    },
    {
      "epoch": 1.4867924528301888,
      "grad_norm": 0.8036262392997742,
      "learning_rate": 0.0003513396226415094,
      "loss": 1.9058,
      "step": 7880
    },
    {
      "epoch": 1.4886792452830189,
      "grad_norm": 0.7057421803474426,
      "learning_rate": 0.0003511509433962264,
      "loss": 1.9027,
      "step": 7890
    },
    {
      "epoch": 1.490566037735849,
      "grad_norm": 0.7257084250450134,
      "learning_rate": 0.0003509622641509434,
      "loss": 1.9153,
      "step": 7900
    },
    {
      "epoch": 1.4924528301886792,
      "grad_norm": 0.8628336787223816,
      "learning_rate": 0.00035077358490566033,
      "loss": 1.9022,
      "step": 7910
    },
    {
      "epoch": 1.4943396226415095,
      "grad_norm": 0.7466588020324707,
      "learning_rate": 0.0003505849056603774,
      "loss": 1.962,
      "step": 7920
    },
    {
      "epoch": 1.4962264150943396,
      "grad_norm": 0.8264734148979187,
      "learning_rate": 0.00035039622641509435,
      "loss": 1.9542,
      "step": 7930
    },
    {
      "epoch": 1.4981132075471697,
      "grad_norm": 0.7049347162246704,
      "learning_rate": 0.00035020754716981136,
      "loss": 1.9015,
      "step": 7940
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.8476514220237732,
      "learning_rate": 0.0003500188679245283,
      "loss": 1.8522,
      "step": 7950
    },
    {
      "epoch": 1.5018867924528303,
      "grad_norm": 0.7819394469261169,
      "learning_rate": 0.00034983018867924527,
      "loss": 1.8819,
      "step": 7960
    },
    {
      "epoch": 1.5037735849056604,
      "grad_norm": 0.8623307943344116,
      "learning_rate": 0.0003496415094339623,
      "loss": 1.9039,
      "step": 7970
    },
    {
      "epoch": 1.5056603773584905,
      "grad_norm": 0.7400078177452087,
      "learning_rate": 0.00034945283018867924,
      "loss": 1.9576,
      "step": 7980
    },
    {
      "epoch": 1.5075471698113208,
      "grad_norm": 0.8100602030754089,
      "learning_rate": 0.00034926415094339625,
      "loss": 1.9058,
      "step": 7990
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 0.7534993290901184,
      "learning_rate": 0.0003490754716981132,
      "loss": 1.906,
      "step": 8000
    },
    {
      "epoch": 1.5113207547169811,
      "grad_norm": 0.7726837396621704,
      "learning_rate": 0.00034888679245283016,
      "loss": 1.8267,
      "step": 8010
    },
    {
      "epoch": 1.5132075471698112,
      "grad_norm": 0.8355078101158142,
      "learning_rate": 0.00034869811320754717,
      "loss": 1.8979,
      "step": 8020
    },
    {
      "epoch": 1.5150943396226415,
      "grad_norm": 0.6815652847290039,
      "learning_rate": 0.0003485094339622641,
      "loss": 1.8999,
      "step": 8030
    },
    {
      "epoch": 1.5169811320754718,
      "grad_norm": 0.7558580636978149,
      "learning_rate": 0.0003483207547169812,
      "loss": 1.9298,
      "step": 8040
    },
    {
      "epoch": 1.5188679245283019,
      "grad_norm": 0.7013068199157715,
      "learning_rate": 0.00034813207547169814,
      "loss": 1.9485,
      "step": 8050
    },
    {
      "epoch": 1.520754716981132,
      "grad_norm": 0.8463267683982849,
      "learning_rate": 0.0003479433962264151,
      "loss": 1.8457,
      "step": 8060
    },
    {
      "epoch": 1.5226415094339623,
      "grad_norm": 0.7830776572227478,
      "learning_rate": 0.0003477547169811321,
      "loss": 1.846,
      "step": 8070
    },
    {
      "epoch": 1.5245283018867926,
      "grad_norm": 0.8574661016464233,
      "learning_rate": 0.00034756603773584906,
      "loss": 1.9809,
      "step": 8080
    },
    {
      "epoch": 1.5264150943396226,
      "grad_norm": 0.6922407150268555,
      "learning_rate": 0.00034737735849056607,
      "loss": 1.8675,
      "step": 8090
    },
    {
      "epoch": 1.5283018867924527,
      "grad_norm": 0.7160203456878662,
      "learning_rate": 0.000347188679245283,
      "loss": 1.9032,
      "step": 8100
    },
    {
      "epoch": 1.530188679245283,
      "grad_norm": 0.726325273513794,
      "learning_rate": 0.000347,
      "loss": 1.8892,
      "step": 8110
    },
    {
      "epoch": 1.5320754716981133,
      "grad_norm": 0.7340494394302368,
      "learning_rate": 0.000346811320754717,
      "loss": 1.8487,
      "step": 8120
    },
    {
      "epoch": 1.5339622641509434,
      "grad_norm": 0.7674826979637146,
      "learning_rate": 0.00034662264150943394,
      "loss": 1.9164,
      "step": 8130
    },
    {
      "epoch": 1.5358490566037735,
      "grad_norm": 0.7615528702735901,
      "learning_rate": 0.00034643396226415095,
      "loss": 2.0196,
      "step": 8140
    },
    {
      "epoch": 1.5377358490566038,
      "grad_norm": 0.7824289798736572,
      "learning_rate": 0.0003462452830188679,
      "loss": 1.9188,
      "step": 8150
    },
    {
      "epoch": 1.539622641509434,
      "grad_norm": 0.7180842757225037,
      "learning_rate": 0.00034605660377358486,
      "loss": 1.8968,
      "step": 8160
    },
    {
      "epoch": 1.5415094339622641,
      "grad_norm": 0.7882664799690247,
      "learning_rate": 0.0003458679245283019,
      "loss": 1.9349,
      "step": 8170
    },
    {
      "epoch": 1.5433962264150942,
      "grad_norm": 0.7540764212608337,
      "learning_rate": 0.0003456792452830189,
      "loss": 1.9788,
      "step": 8180
    },
    {
      "epoch": 1.5452830188679245,
      "grad_norm": 0.7804048657417297,
      "learning_rate": 0.0003454905660377359,
      "loss": 1.9072,
      "step": 8190
    },
    {
      "epoch": 1.5471698113207548,
      "grad_norm": 0.7751489877700806,
      "learning_rate": 0.00034530188679245284,
      "loss": 1.9787,
      "step": 8200
    },
    {
      "epoch": 1.549056603773585,
      "grad_norm": 0.7311378717422485,
      "learning_rate": 0.0003451132075471698,
      "loss": 1.9774,
      "step": 8210
    },
    {
      "epoch": 1.550943396226415,
      "grad_norm": 0.8169467449188232,
      "learning_rate": 0.0003449245283018868,
      "loss": 1.8734,
      "step": 8220
    },
    {
      "epoch": 1.5528301886792453,
      "grad_norm": 0.6913802623748779,
      "learning_rate": 0.00034473584905660376,
      "loss": 1.9218,
      "step": 8230
    },
    {
      "epoch": 1.5547169811320756,
      "grad_norm": 0.8346660137176514,
      "learning_rate": 0.0003445471698113208,
      "loss": 1.9133,
      "step": 8240
    },
    {
      "epoch": 1.5566037735849056,
      "grad_norm": 0.6770761609077454,
      "learning_rate": 0.00034435849056603773,
      "loss": 1.9804,
      "step": 8250
    },
    {
      "epoch": 1.5584905660377357,
      "grad_norm": 0.8139410018920898,
      "learning_rate": 0.0003441698113207547,
      "loss": 1.9171,
      "step": 8260
    },
    {
      "epoch": 1.560377358490566,
      "grad_norm": 0.7156010270118713,
      "learning_rate": 0.0003439811320754717,
      "loss": 1.9106,
      "step": 8270
    },
    {
      "epoch": 1.5622641509433963,
      "grad_norm": 0.8969778418540955,
      "learning_rate": 0.00034379245283018865,
      "loss": 1.9247,
      "step": 8280
    },
    {
      "epoch": 1.5641509433962264,
      "grad_norm": 0.766766369342804,
      "learning_rate": 0.0003436037735849057,
      "loss": 1.9303,
      "step": 8290
    },
    {
      "epoch": 1.5660377358490565,
      "grad_norm": 0.7924900054931641,
      "learning_rate": 0.00034341509433962267,
      "loss": 1.9385,
      "step": 8300
    },
    {
      "epoch": 1.5679245283018868,
      "grad_norm": 0.8429474234580994,
      "learning_rate": 0.0003432264150943396,
      "loss": 1.8689,
      "step": 8310
    },
    {
      "epoch": 1.569811320754717,
      "grad_norm": 0.7287303805351257,
      "learning_rate": 0.00034303773584905663,
      "loss": 1.8975,
      "step": 8320
    },
    {
      "epoch": 1.5716981132075472,
      "grad_norm": 0.9059645533561707,
      "learning_rate": 0.0003428490566037736,
      "loss": 1.8247,
      "step": 8330
    },
    {
      "epoch": 1.5735849056603772,
      "grad_norm": 0.7206006050109863,
      "learning_rate": 0.0003426603773584906,
      "loss": 1.8516,
      "step": 8340
    },
    {
      "epoch": 1.5754716981132075,
      "grad_norm": 0.7016845345497131,
      "learning_rate": 0.00034247169811320755,
      "loss": 1.932,
      "step": 8350
    },
    {
      "epoch": 1.5773584905660378,
      "grad_norm": 0.7520517110824585,
      "learning_rate": 0.0003422830188679245,
      "loss": 1.8872,
      "step": 8360
    },
    {
      "epoch": 1.579245283018868,
      "grad_norm": 0.7226089835166931,
      "learning_rate": 0.0003420943396226415,
      "loss": 1.9257,
      "step": 8370
    },
    {
      "epoch": 1.581132075471698,
      "grad_norm": 0.8428210020065308,
      "learning_rate": 0.00034190566037735847,
      "loss": 1.9221,
      "step": 8380
    },
    {
      "epoch": 1.5830188679245283,
      "grad_norm": 0.7071080803871155,
      "learning_rate": 0.0003417169811320755,
      "loss": 1.8293,
      "step": 8390
    },
    {
      "epoch": 1.5849056603773586,
      "grad_norm": 0.7146615386009216,
      "learning_rate": 0.00034152830188679243,
      "loss": 1.8732,
      "step": 8400
    },
    {
      "epoch": 1.5867924528301887,
      "grad_norm": 0.8557942509651184,
      "learning_rate": 0.00034133962264150944,
      "loss": 1.9574,
      "step": 8410
    },
    {
      "epoch": 1.5886792452830187,
      "grad_norm": 0.7997991442680359,
      "learning_rate": 0.00034115094339622645,
      "loss": 1.9459,
      "step": 8420
    },
    {
      "epoch": 1.590566037735849,
      "grad_norm": 0.8595538139343262,
      "learning_rate": 0.0003409622641509434,
      "loss": 1.9623,
      "step": 8430
    },
    {
      "epoch": 1.5924528301886793,
      "grad_norm": 0.7482202649116516,
      "learning_rate": 0.0003407735849056604,
      "loss": 1.7821,
      "step": 8440
    },
    {
      "epoch": 1.5943396226415094,
      "grad_norm": 0.7289662957191467,
      "learning_rate": 0.00034058490566037737,
      "loss": 1.8998,
      "step": 8450
    },
    {
      "epoch": 1.5962264150943395,
      "grad_norm": 0.7211437821388245,
      "learning_rate": 0.0003403962264150943,
      "loss": 1.9611,
      "step": 8460
    },
    {
      "epoch": 1.5981132075471698,
      "grad_norm": 0.829127848148346,
      "learning_rate": 0.00034020754716981134,
      "loss": 1.8362,
      "step": 8470
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.7062147855758667,
      "learning_rate": 0.0003400188679245283,
      "loss": 1.8744,
      "step": 8480
    },
    {
      "epoch": 1.6018867924528302,
      "grad_norm": 0.760152280330658,
      "learning_rate": 0.0003398301886792453,
      "loss": 1.8211,
      "step": 8490
    },
    {
      "epoch": 1.6037735849056602,
      "grad_norm": 0.7523080110549927,
      "learning_rate": 0.00033964150943396226,
      "loss": 1.9312,
      "step": 8500
    },
    {
      "epoch": 1.6056603773584905,
      "grad_norm": 0.7134537100791931,
      "learning_rate": 0.0003394528301886792,
      "loss": 1.9656,
      "step": 8510
    },
    {
      "epoch": 1.6075471698113208,
      "grad_norm": 0.6957917809486389,
      "learning_rate": 0.0003392641509433962,
      "loss": 1.904,
      "step": 8520
    },
    {
      "epoch": 1.609433962264151,
      "grad_norm": 0.7930260896682739,
      "learning_rate": 0.00033907547169811323,
      "loss": 1.8412,
      "step": 8530
    },
    {
      "epoch": 1.611320754716981,
      "grad_norm": 0.724868893623352,
      "learning_rate": 0.00033888679245283024,
      "loss": 1.9184,
      "step": 8540
    },
    {
      "epoch": 1.6132075471698113,
      "grad_norm": 0.7335346937179565,
      "learning_rate": 0.0003386981132075472,
      "loss": 1.9394,
      "step": 8550
    },
    {
      "epoch": 1.6150943396226416,
      "grad_norm": 1.1740350723266602,
      "learning_rate": 0.00033850943396226415,
      "loss": 1.8566,
      "step": 8560
    },
    {
      "epoch": 1.6169811320754717,
      "grad_norm": 0.814559280872345,
      "learning_rate": 0.00033832075471698116,
      "loss": 1.9074,
      "step": 8570
    },
    {
      "epoch": 1.6188679245283017,
      "grad_norm": 0.7108903527259827,
      "learning_rate": 0.0003381320754716981,
      "loss": 1.8752,
      "step": 8580
    },
    {
      "epoch": 1.620754716981132,
      "grad_norm": 0.7761796712875366,
      "learning_rate": 0.0003379433962264151,
      "loss": 1.8814,
      "step": 8590
    },
    {
      "epoch": 1.6226415094339623,
      "grad_norm": 0.7557936906814575,
      "learning_rate": 0.0003377547169811321,
      "loss": 1.8841,
      "step": 8600
    },
    {
      "epoch": 1.6245283018867924,
      "grad_norm": 0.7416086196899414,
      "learning_rate": 0.00033756603773584903,
      "loss": 1.8609,
      "step": 8610
    },
    {
      "epoch": 1.6264150943396225,
      "grad_norm": 0.8717814683914185,
      "learning_rate": 0.00033737735849056604,
      "loss": 1.8632,
      "step": 8620
    },
    {
      "epoch": 1.6283018867924528,
      "grad_norm": 0.809594988822937,
      "learning_rate": 0.000337188679245283,
      "loss": 1.8758,
      "step": 8630
    },
    {
      "epoch": 1.630188679245283,
      "grad_norm": 0.9109255075454712,
      "learning_rate": 0.000337,
      "loss": 1.8939,
      "step": 8640
    },
    {
      "epoch": 1.6320754716981132,
      "grad_norm": 0.7397728562355042,
      "learning_rate": 0.00033681132075471696,
      "loss": 1.8187,
      "step": 8650
    },
    {
      "epoch": 1.6339622641509433,
      "grad_norm": 0.7757728695869446,
      "learning_rate": 0.00033662264150943397,
      "loss": 1.9484,
      "step": 8660
    },
    {
      "epoch": 1.6358490566037736,
      "grad_norm": 0.7183390855789185,
      "learning_rate": 0.000336433962264151,
      "loss": 1.9344,
      "step": 8670
    },
    {
      "epoch": 1.6377358490566039,
      "grad_norm": 0.6487734317779541,
      "learning_rate": 0.00033624528301886793,
      "loss": 1.8426,
      "step": 8680
    },
    {
      "epoch": 1.639622641509434,
      "grad_norm": 0.7543579339981079,
      "learning_rate": 0.00033605660377358494,
      "loss": 1.8049,
      "step": 8690
    },
    {
      "epoch": 1.641509433962264,
      "grad_norm": 0.827715277671814,
      "learning_rate": 0.0003358679245283019,
      "loss": 1.8489,
      "step": 8700
    },
    {
      "epoch": 1.6433962264150943,
      "grad_norm": 0.7429811954498291,
      "learning_rate": 0.00033567924528301885,
      "loss": 1.9054,
      "step": 8710
    },
    {
      "epoch": 1.6452830188679246,
      "grad_norm": 0.6890600919723511,
      "learning_rate": 0.00033549056603773586,
      "loss": 1.9847,
      "step": 8720
    },
    {
      "epoch": 1.6471698113207547,
      "grad_norm": 0.7991039156913757,
      "learning_rate": 0.0003353018867924528,
      "loss": 1.9082,
      "step": 8730
    },
    {
      "epoch": 1.6490566037735848,
      "grad_norm": 0.8056094646453857,
      "learning_rate": 0.00033511320754716983,
      "loss": 1.9125,
      "step": 8740
    },
    {
      "epoch": 1.650943396226415,
      "grad_norm": 0.6723660826683044,
      "learning_rate": 0.0003349245283018868,
      "loss": 1.8551,
      "step": 8750
    },
    {
      "epoch": 1.6528301886792454,
      "grad_norm": 0.7285041213035583,
      "learning_rate": 0.00033473584905660374,
      "loss": 1.9493,
      "step": 8760
    },
    {
      "epoch": 1.6547169811320754,
      "grad_norm": 0.7203649878501892,
      "learning_rate": 0.00033454716981132075,
      "loss": 1.9038,
      "step": 8770
    },
    {
      "epoch": 1.6566037735849055,
      "grad_norm": 0.7490846514701843,
      "learning_rate": 0.00033435849056603776,
      "loss": 1.9275,
      "step": 8780
    },
    {
      "epoch": 1.6584905660377358,
      "grad_norm": 0.6736448407173157,
      "learning_rate": 0.00033416981132075477,
      "loss": 1.7905,
      "step": 8790
    },
    {
      "epoch": 1.6603773584905661,
      "grad_norm": 0.7134812474250793,
      "learning_rate": 0.0003339811320754717,
      "loss": 1.7992,
      "step": 8800
    },
    {
      "epoch": 1.6622641509433962,
      "grad_norm": 0.7683269381523132,
      "learning_rate": 0.0003337924528301887,
      "loss": 1.8374,
      "step": 8810
    },
    {
      "epoch": 1.6641509433962263,
      "grad_norm": 0.7312276363372803,
      "learning_rate": 0.0003336037735849057,
      "loss": 1.8396,
      "step": 8820
    },
    {
      "epoch": 1.6660377358490566,
      "grad_norm": 0.7549210786819458,
      "learning_rate": 0.00033341509433962264,
      "loss": 1.9265,
      "step": 8830
    },
    {
      "epoch": 1.6679245283018869,
      "grad_norm": 0.7099592685699463,
      "learning_rate": 0.00033322641509433965,
      "loss": 1.8146,
      "step": 8840
    },
    {
      "epoch": 1.669811320754717,
      "grad_norm": 0.7859920859336853,
      "learning_rate": 0.0003330377358490566,
      "loss": 1.9077,
      "step": 8850
    },
    {
      "epoch": 1.671698113207547,
      "grad_norm": 0.9168362617492676,
      "learning_rate": 0.00033284905660377356,
      "loss": 1.9242,
      "step": 8860
    },
    {
      "epoch": 1.6735849056603773,
      "grad_norm": 0.6972419023513794,
      "learning_rate": 0.00033266037735849057,
      "loss": 1.9004,
      "step": 8870
    },
    {
      "epoch": 1.6754716981132076,
      "grad_norm": 0.8190483450889587,
      "learning_rate": 0.0003324716981132075,
      "loss": 1.8485,
      "step": 8880
    },
    {
      "epoch": 1.6773584905660377,
      "grad_norm": 0.7092880010604858,
      "learning_rate": 0.00033228301886792453,
      "loss": 1.8688,
      "step": 8890
    },
    {
      "epoch": 1.6792452830188678,
      "grad_norm": 0.7800996899604797,
      "learning_rate": 0.00033209433962264154,
      "loss": 1.8656,
      "step": 8900
    },
    {
      "epoch": 1.681132075471698,
      "grad_norm": 0.7452324628829956,
      "learning_rate": 0.0003319056603773585,
      "loss": 1.9259,
      "step": 8910
    },
    {
      "epoch": 1.6830188679245284,
      "grad_norm": 0.7839518785476685,
      "learning_rate": 0.0003317169811320755,
      "loss": 1.8727,
      "step": 8920
    },
    {
      "epoch": 1.6849056603773584,
      "grad_norm": 0.7496175765991211,
      "learning_rate": 0.00033152830188679246,
      "loss": 1.9343,
      "step": 8930
    },
    {
      "epoch": 1.6867924528301885,
      "grad_norm": 0.7278044819831848,
      "learning_rate": 0.00033133962264150947,
      "loss": 1.8759,
      "step": 8940
    },
    {
      "epoch": 1.6886792452830188,
      "grad_norm": 0.7035598754882812,
      "learning_rate": 0.0003311509433962264,
      "loss": 2.0041,
      "step": 8950
    },
    {
      "epoch": 1.6905660377358491,
      "grad_norm": 0.7673270106315613,
      "learning_rate": 0.0003309622641509434,
      "loss": 1.9644,
      "step": 8960
    },
    {
      "epoch": 1.6924528301886792,
      "grad_norm": 0.786121666431427,
      "learning_rate": 0.0003307735849056604,
      "loss": 1.8606,
      "step": 8970
    },
    {
      "epoch": 1.6943396226415093,
      "grad_norm": 0.6989141702651978,
      "learning_rate": 0.00033058490566037735,
      "loss": 1.964,
      "step": 8980
    },
    {
      "epoch": 1.6962264150943396,
      "grad_norm": 0.8069393634796143,
      "learning_rate": 0.00033039622641509435,
      "loss": 1.8417,
      "step": 8990
    },
    {
      "epoch": 1.6981132075471699,
      "grad_norm": 0.7245251536369324,
      "learning_rate": 0.0003302075471698113,
      "loss": 1.9585,
      "step": 9000
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.7269404530525208,
      "learning_rate": 0.00033001886792452826,
      "loss": 1.9131,
      "step": 9010
    },
    {
      "epoch": 1.70188679245283,
      "grad_norm": 0.6565699577331543,
      "learning_rate": 0.00032983018867924533,
      "loss": 1.8784,
      "step": 9020
    },
    {
      "epoch": 1.7037735849056603,
      "grad_norm": 0.7873000502586365,
      "learning_rate": 0.0003296415094339623,
      "loss": 1.9584,
      "step": 9030
    },
    {
      "epoch": 1.7056603773584906,
      "grad_norm": 0.7105101943016052,
      "learning_rate": 0.0003294528301886793,
      "loss": 1.9556,
      "step": 9040
    },
    {
      "epoch": 1.7075471698113207,
      "grad_norm": 0.7497922778129578,
      "learning_rate": 0.00032926415094339625,
      "loss": 1.931,
      "step": 9050
    },
    {
      "epoch": 1.7094339622641508,
      "grad_norm": 0.7492458820343018,
      "learning_rate": 0.0003290754716981132,
      "loss": 1.872,
      "step": 9060
    },
    {
      "epoch": 1.711320754716981,
      "grad_norm": 0.7110623121261597,
      "learning_rate": 0.0003288867924528302,
      "loss": 1.9719,
      "step": 9070
    },
    {
      "epoch": 1.7132075471698114,
      "grad_norm": 0.6997737288475037,
      "learning_rate": 0.00032869811320754717,
      "loss": 1.8536,
      "step": 9080
    },
    {
      "epoch": 1.7150943396226415,
      "grad_norm": 0.718949019908905,
      "learning_rate": 0.0003285094339622642,
      "loss": 1.8481,
      "step": 9090
    },
    {
      "epoch": 1.7169811320754715,
      "grad_norm": 0.6976349949836731,
      "learning_rate": 0.00032832075471698113,
      "loss": 1.8834,
      "step": 9100
    },
    {
      "epoch": 1.7188679245283018,
      "grad_norm": 0.6911079287528992,
      "learning_rate": 0.0003281320754716981,
      "loss": 1.9424,
      "step": 9110
    },
    {
      "epoch": 1.7207547169811321,
      "grad_norm": 0.7309988737106323,
      "learning_rate": 0.0003279433962264151,
      "loss": 1.9237,
      "step": 9120
    },
    {
      "epoch": 1.7226415094339622,
      "grad_norm": 0.8022026419639587,
      "learning_rate": 0.00032775471698113205,
      "loss": 1.8346,
      "step": 9130
    },
    {
      "epoch": 1.7245283018867923,
      "grad_norm": 0.7309944033622742,
      "learning_rate": 0.0003275660377358491,
      "loss": 1.9143,
      "step": 9140
    },
    {
      "epoch": 1.7264150943396226,
      "grad_norm": 0.742855429649353,
      "learning_rate": 0.00032737735849056607,
      "loss": 1.8812,
      "step": 9150
    },
    {
      "epoch": 1.728301886792453,
      "grad_norm": 0.7791370153427124,
      "learning_rate": 0.000327188679245283,
      "loss": 1.9876,
      "step": 9160
    },
    {
      "epoch": 1.730188679245283,
      "grad_norm": 0.7064318060874939,
      "learning_rate": 0.00032700000000000003,
      "loss": 1.9736,
      "step": 9170
    },
    {
      "epoch": 1.732075471698113,
      "grad_norm": 0.722334086894989,
      "learning_rate": 0.000326811320754717,
      "loss": 1.7945,
      "step": 9180
    },
    {
      "epoch": 1.7339622641509433,
      "grad_norm": 0.7880585789680481,
      "learning_rate": 0.000326622641509434,
      "loss": 1.9235,
      "step": 9190
    },
    {
      "epoch": 1.7358490566037736,
      "grad_norm": 0.6861926317214966,
      "learning_rate": 0.00032643396226415095,
      "loss": 1.8677,
      "step": 9200
    },
    {
      "epoch": 1.7377358490566037,
      "grad_norm": 0.7230889201164246,
      "learning_rate": 0.0003262452830188679,
      "loss": 1.9266,
      "step": 9210
    },
    {
      "epoch": 1.7396226415094338,
      "grad_norm": 0.8353850841522217,
      "learning_rate": 0.0003260566037735849,
      "loss": 1.9608,
      "step": 9220
    },
    {
      "epoch": 1.741509433962264,
      "grad_norm": 0.6360123157501221,
      "learning_rate": 0.00032586792452830187,
      "loss": 1.8344,
      "step": 9230
    },
    {
      "epoch": 1.7433962264150944,
      "grad_norm": 0.6994743347167969,
      "learning_rate": 0.0003256792452830189,
      "loss": 1.9472,
      "step": 9240
    },
    {
      "epoch": 1.7452830188679245,
      "grad_norm": 0.7571813464164734,
      "learning_rate": 0.00032549056603773584,
      "loss": 1.8999,
      "step": 9250
    },
    {
      "epoch": 1.7471698113207546,
      "grad_norm": 0.724433183670044,
      "learning_rate": 0.0003253018867924528,
      "loss": 1.8285,
      "step": 9260
    },
    {
      "epoch": 1.7490566037735849,
      "grad_norm": 0.8802738189697266,
      "learning_rate": 0.00032511320754716986,
      "loss": 1.929,
      "step": 9270
    },
    {
      "epoch": 1.7509433962264151,
      "grad_norm": 0.7405597567558289,
      "learning_rate": 0.0003249245283018868,
      "loss": 1.8669,
      "step": 9280
    },
    {
      "epoch": 1.7528301886792454,
      "grad_norm": 0.6920768618583679,
      "learning_rate": 0.0003247358490566038,
      "loss": 1.8837,
      "step": 9290
    },
    {
      "epoch": 1.7547169811320755,
      "grad_norm": 0.7488152384757996,
      "learning_rate": 0.0003245471698113208,
      "loss": 1.9473,
      "step": 9300
    },
    {
      "epoch": 1.7566037735849056,
      "grad_norm": 0.706710159778595,
      "learning_rate": 0.00032435849056603773,
      "loss": 1.8942,
      "step": 9310
    },
    {
      "epoch": 1.758490566037736,
      "grad_norm": 0.7204844355583191,
      "learning_rate": 0.00032416981132075474,
      "loss": 1.8849,
      "step": 9320
    },
    {
      "epoch": 1.7603773584905662,
      "grad_norm": 0.7302986979484558,
      "learning_rate": 0.0003239811320754717,
      "loss": 1.9112,
      "step": 9330
    },
    {
      "epoch": 1.7622641509433963,
      "grad_norm": 0.7910389304161072,
      "learning_rate": 0.0003237924528301887,
      "loss": 1.9719,
      "step": 9340
    },
    {
      "epoch": 1.7641509433962264,
      "grad_norm": 0.7124210000038147,
      "learning_rate": 0.00032360377358490566,
      "loss": 1.8141,
      "step": 9350
    },
    {
      "epoch": 1.7660377358490567,
      "grad_norm": 0.849786639213562,
      "learning_rate": 0.0003234150943396226,
      "loss": 1.9395,
      "step": 9360
    },
    {
      "epoch": 1.767924528301887,
      "grad_norm": 0.8047391772270203,
      "learning_rate": 0.0003232264150943396,
      "loss": 1.9173,
      "step": 9370
    },
    {
      "epoch": 1.769811320754717,
      "grad_norm": 0.7172788381576538,
      "learning_rate": 0.0003230377358490566,
      "loss": 1.9474,
      "step": 9380
    },
    {
      "epoch": 1.771698113207547,
      "grad_norm": 0.8270654678344727,
      "learning_rate": 0.00032284905660377364,
      "loss": 1.9501,
      "step": 9390
    },
    {
      "epoch": 1.7735849056603774,
      "grad_norm": 0.7372636795043945,
      "learning_rate": 0.0003226603773584906,
      "loss": 1.9053,
      "step": 9400
    },
    {
      "epoch": 1.7754716981132077,
      "grad_norm": 0.6838557720184326,
      "learning_rate": 0.00032247169811320755,
      "loss": 1.9357,
      "step": 9410
    },
    {
      "epoch": 1.7773584905660378,
      "grad_norm": 0.7963840365409851,
      "learning_rate": 0.00032228301886792456,
      "loss": 1.9199,
      "step": 9420
    },
    {
      "epoch": 1.7792452830188679,
      "grad_norm": 0.7133448123931885,
      "learning_rate": 0.0003220943396226415,
      "loss": 1.9552,
      "step": 9430
    },
    {
      "epoch": 1.7811320754716982,
      "grad_norm": 0.7117114663124084,
      "learning_rate": 0.0003219056603773585,
      "loss": 1.8683,
      "step": 9440
    },
    {
      "epoch": 1.7830188679245285,
      "grad_norm": 0.8506219983100891,
      "learning_rate": 0.0003217169811320755,
      "loss": 1.8753,
      "step": 9450
    },
    {
      "epoch": 1.7849056603773585,
      "grad_norm": 0.8141700029373169,
      "learning_rate": 0.00032152830188679244,
      "loss": 1.8809,
      "step": 9460
    },
    {
      "epoch": 1.7867924528301886,
      "grad_norm": 0.6976975202560425,
      "learning_rate": 0.00032133962264150944,
      "loss": 1.8031,
      "step": 9470
    },
    {
      "epoch": 1.788679245283019,
      "grad_norm": 0.7214177250862122,
      "learning_rate": 0.0003211509433962264,
      "loss": 1.8597,
      "step": 9480
    },
    {
      "epoch": 1.7905660377358492,
      "grad_norm": 0.7027537226676941,
      "learning_rate": 0.0003209622641509434,
      "loss": 1.9955,
      "step": 9490
    },
    {
      "epoch": 1.7924528301886793,
      "grad_norm": 0.670087456703186,
      "learning_rate": 0.00032077358490566036,
      "loss": 1.906,
      "step": 9500
    },
    {
      "epoch": 1.7943396226415094,
      "grad_norm": 0.744573175907135,
      "learning_rate": 0.0003205849056603773,
      "loss": 1.8765,
      "step": 9510
    },
    {
      "epoch": 1.7962264150943397,
      "grad_norm": 0.8646011352539062,
      "learning_rate": 0.0003203962264150944,
      "loss": 1.8654,
      "step": 9520
    },
    {
      "epoch": 1.79811320754717,
      "grad_norm": 0.9318859577178955,
      "learning_rate": 0.00032020754716981134,
      "loss": 1.8675,
      "step": 9530
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.7491164803504944,
      "learning_rate": 0.00032001886792452835,
      "loss": 1.8924,
      "step": 9540
    },
    {
      "epoch": 1.8018867924528301,
      "grad_norm": 0.6888189911842346,
      "learning_rate": 0.0003198301886792453,
      "loss": 1.895,
      "step": 9550
    },
    {
      "epoch": 1.8037735849056604,
      "grad_norm": 0.7132939100265503,
      "learning_rate": 0.00031964150943396226,
      "loss": 1.9014,
      "step": 9560
    },
    {
      "epoch": 1.8056603773584907,
      "grad_norm": 0.6834372878074646,
      "learning_rate": 0.00031945283018867927,
      "loss": 1.8909,
      "step": 9570
    },
    {
      "epoch": 1.8075471698113208,
      "grad_norm": 0.7769665122032166,
      "learning_rate": 0.0003192641509433962,
      "loss": 1.8399,
      "step": 9580
    },
    {
      "epoch": 1.8094339622641509,
      "grad_norm": 0.7877170443534851,
      "learning_rate": 0.00031907547169811323,
      "loss": 1.9578,
      "step": 9590
    },
    {
      "epoch": 1.8113207547169812,
      "grad_norm": 0.724526584148407,
      "learning_rate": 0.0003188867924528302,
      "loss": 1.9501,
      "step": 9600
    },
    {
      "epoch": 1.8132075471698115,
      "grad_norm": 0.7195760011672974,
      "learning_rate": 0.00031869811320754714,
      "loss": 1.8782,
      "step": 9610
    },
    {
      "epoch": 1.8150943396226416,
      "grad_norm": 0.7600510716438293,
      "learning_rate": 0.00031850943396226415,
      "loss": 1.8979,
      "step": 9620
    },
    {
      "epoch": 1.8169811320754716,
      "grad_norm": 0.7355839014053345,
      "learning_rate": 0.0003183207547169811,
      "loss": 1.8081,
      "step": 9630
    },
    {
      "epoch": 1.818867924528302,
      "grad_norm": 0.7666388750076294,
      "learning_rate": 0.00031813207547169817,
      "loss": 1.9293,
      "step": 9640
    },
    {
      "epoch": 1.8207547169811322,
      "grad_norm": 0.7067894339561462,
      "learning_rate": 0.0003179433962264151,
      "loss": 1.9767,
      "step": 9650
    },
    {
      "epoch": 1.8226415094339623,
      "grad_norm": 0.7326235175132751,
      "learning_rate": 0.0003177547169811321,
      "loss": 1.9319,
      "step": 9660
    },
    {
      "epoch": 1.8245283018867924,
      "grad_norm": 0.9011062383651733,
      "learning_rate": 0.0003175660377358491,
      "loss": 1.9597,
      "step": 9670
    },
    {
      "epoch": 1.8264150943396227,
      "grad_norm": 0.7528982162475586,
      "learning_rate": 0.00031737735849056604,
      "loss": 1.8648,
      "step": 9680
    },
    {
      "epoch": 1.828301886792453,
      "grad_norm": 0.7226542234420776,
      "learning_rate": 0.00031718867924528305,
      "loss": 1.9372,
      "step": 9690
    },
    {
      "epoch": 1.830188679245283,
      "grad_norm": 0.6916003823280334,
      "learning_rate": 0.000317,
      "loss": 1.8356,
      "step": 9700
    },
    {
      "epoch": 1.8320754716981131,
      "grad_norm": 0.7673676013946533,
      "learning_rate": 0.00031681132075471696,
      "loss": 1.8983,
      "step": 9710
    },
    {
      "epoch": 1.8339622641509434,
      "grad_norm": 0.7987264394760132,
      "learning_rate": 0.00031662264150943397,
      "loss": 1.8706,
      "step": 9720
    },
    {
      "epoch": 1.8358490566037737,
      "grad_norm": 0.7844293117523193,
      "learning_rate": 0.0003164339622641509,
      "loss": 1.8336,
      "step": 9730
    },
    {
      "epoch": 1.8377358490566038,
      "grad_norm": 0.8419997692108154,
      "learning_rate": 0.00031624528301886794,
      "loss": 1.9002,
      "step": 9740
    },
    {
      "epoch": 1.8396226415094339,
      "grad_norm": 0.8135342001914978,
      "learning_rate": 0.0003160566037735849,
      "loss": 1.9764,
      "step": 9750
    },
    {
      "epoch": 1.8415094339622642,
      "grad_norm": 0.755865752696991,
      "learning_rate": 0.0003158679245283019,
      "loss": 1.951,
      "step": 9760
    },
    {
      "epoch": 1.8433962264150945,
      "grad_norm": 0.7131441831588745,
      "learning_rate": 0.0003156792452830189,
      "loss": 1.9233,
      "step": 9770
    },
    {
      "epoch": 1.8452830188679246,
      "grad_norm": 0.7524367570877075,
      "learning_rate": 0.00031549056603773586,
      "loss": 1.9146,
      "step": 9780
    },
    {
      "epoch": 1.8471698113207546,
      "grad_norm": 0.8212631940841675,
      "learning_rate": 0.0003153018867924529,
      "loss": 1.8379,
      "step": 9790
    },
    {
      "epoch": 1.849056603773585,
      "grad_norm": 0.7427534461021423,
      "learning_rate": 0.00031511320754716983,
      "loss": 1.8879,
      "step": 9800
    },
    {
      "epoch": 1.8509433962264152,
      "grad_norm": 0.8157886862754822,
      "learning_rate": 0.0003149245283018868,
      "loss": 1.9202,
      "step": 9810
    },
    {
      "epoch": 1.8528301886792453,
      "grad_norm": 0.8194231390953064,
      "learning_rate": 0.0003147358490566038,
      "loss": 1.7923,
      "step": 9820
    },
    {
      "epoch": 1.8547169811320754,
      "grad_norm": 0.8560187220573425,
      "learning_rate": 0.00031454716981132075,
      "loss": 1.9993,
      "step": 9830
    },
    {
      "epoch": 1.8566037735849057,
      "grad_norm": 0.7587292194366455,
      "learning_rate": 0.00031435849056603776,
      "loss": 1.8477,
      "step": 9840
    },
    {
      "epoch": 1.858490566037736,
      "grad_norm": 0.8185806274414062,
      "learning_rate": 0.0003141698113207547,
      "loss": 1.7888,
      "step": 9850
    },
    {
      "epoch": 1.860377358490566,
      "grad_norm": 0.8687983751296997,
      "learning_rate": 0.00031398113207547167,
      "loss": 1.8298,
      "step": 9860
    },
    {
      "epoch": 1.8622641509433961,
      "grad_norm": 0.7230930924415588,
      "learning_rate": 0.0003137924528301887,
      "loss": 1.7916,
      "step": 9870
    },
    {
      "epoch": 1.8641509433962264,
      "grad_norm": 0.9880080819129944,
      "learning_rate": 0.0003136037735849057,
      "loss": 1.8517,
      "step": 9880
    },
    {
      "epoch": 1.8660377358490567,
      "grad_norm": 0.7449461221694946,
      "learning_rate": 0.0003134150943396227,
      "loss": 1.9213,
      "step": 9890
    },
    {
      "epoch": 1.8679245283018868,
      "grad_norm": 0.7591525912284851,
      "learning_rate": 0.00031322641509433965,
      "loss": 1.9244,
      "step": 9900
    },
    {
      "epoch": 1.869811320754717,
      "grad_norm": 0.717202365398407,
      "learning_rate": 0.0003130377358490566,
      "loss": 1.9019,
      "step": 9910
    },
    {
      "epoch": 1.8716981132075472,
      "grad_norm": 0.7795852422714233,
      "learning_rate": 0.0003128490566037736,
      "loss": 1.9684,
      "step": 9920
    },
    {
      "epoch": 1.8735849056603775,
      "grad_norm": 0.7216859459877014,
      "learning_rate": 0.00031266037735849057,
      "loss": 1.8446,
      "step": 9930
    },
    {
      "epoch": 1.8754716981132076,
      "grad_norm": 0.6872166395187378,
      "learning_rate": 0.0003124716981132076,
      "loss": 1.8503,
      "step": 9940
    },
    {
      "epoch": 1.8773584905660377,
      "grad_norm": 0.7106099724769592,
      "learning_rate": 0.00031228301886792453,
      "loss": 1.8556,
      "step": 9950
    },
    {
      "epoch": 1.879245283018868,
      "grad_norm": 0.84439617395401,
      "learning_rate": 0.0003120943396226415,
      "loss": 1.9365,
      "step": 9960
    },
    {
      "epoch": 1.8811320754716983,
      "grad_norm": 0.7192960977554321,
      "learning_rate": 0.0003119056603773585,
      "loss": 1.7808,
      "step": 9970
    },
    {
      "epoch": 1.8830188679245283,
      "grad_norm": 0.863939106464386,
      "learning_rate": 0.00031171698113207545,
      "loss": 1.8342,
      "step": 9980
    },
    {
      "epoch": 1.8849056603773584,
      "grad_norm": 0.7213850021362305,
      "learning_rate": 0.00031152830188679246,
      "loss": 1.8686,
      "step": 9990
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 0.7570160031318665,
      "learning_rate": 0.00031133962264150947,
      "loss": 1.8522,
      "step": 10000
    },
    {
      "epoch": 1.888679245283019,
      "grad_norm": 0.7283083200454712,
      "learning_rate": 0.00031115094339622643,
      "loss": 1.9202,
      "step": 10010
    },
    {
      "epoch": 1.890566037735849,
      "grad_norm": 0.698168933391571,
      "learning_rate": 0.00031096226415094344,
      "loss": 1.8462,
      "step": 10020
    },
    {
      "epoch": 1.8924528301886792,
      "grad_norm": 0.7540903091430664,
      "learning_rate": 0.0003107735849056604,
      "loss": 1.8672,
      "step": 10030
    },
    {
      "epoch": 1.8943396226415095,
      "grad_norm": 0.8343262076377869,
      "learning_rate": 0.0003105849056603774,
      "loss": 1.8451,
      "step": 10040
    },
    {
      "epoch": 1.8962264150943398,
      "grad_norm": 0.7459455728530884,
      "learning_rate": 0.00031039622641509436,
      "loss": 1.9062,
      "step": 10050
    },
    {
      "epoch": 1.8981132075471698,
      "grad_norm": 0.7554920315742493,
      "learning_rate": 0.0003102075471698113,
      "loss": 1.9385,
      "step": 10060
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.7707183361053467,
      "learning_rate": 0.0003100188679245283,
      "loss": 1.9414,
      "step": 10070
    },
    {
      "epoch": 1.9018867924528302,
      "grad_norm": 0.7653830051422119,
      "learning_rate": 0.0003098301886792453,
      "loss": 1.8957,
      "step": 10080
    },
    {
      "epoch": 1.9037735849056605,
      "grad_norm": 0.7364192605018616,
      "learning_rate": 0.00030964150943396223,
      "loss": 1.8803,
      "step": 10090
    },
    {
      "epoch": 1.9056603773584906,
      "grad_norm": 0.8321145176887512,
      "learning_rate": 0.00030945283018867924,
      "loss": 1.8727,
      "step": 10100
    },
    {
      "epoch": 1.9075471698113207,
      "grad_norm": 0.7183201313018799,
      "learning_rate": 0.0003092641509433962,
      "loss": 1.9499,
      "step": 10110
    },
    {
      "epoch": 1.909433962264151,
      "grad_norm": 0.7859011292457581,
      "learning_rate": 0.0003090754716981132,
      "loss": 1.8684,
      "step": 10120
    },
    {
      "epoch": 1.9113207547169813,
      "grad_norm": 0.8015928864479065,
      "learning_rate": 0.0003088867924528302,
      "loss": 1.8755,
      "step": 10130
    },
    {
      "epoch": 1.9132075471698113,
      "grad_norm": 0.7373136878013611,
      "learning_rate": 0.00030869811320754717,
      "loss": 1.9147,
      "step": 10140
    },
    {
      "epoch": 1.9150943396226414,
      "grad_norm": 0.7244554758071899,
      "learning_rate": 0.0003085094339622642,
      "loss": 1.9012,
      "step": 10150
    },
    {
      "epoch": 1.9169811320754717,
      "grad_norm": 0.9399049282073975,
      "learning_rate": 0.00030832075471698113,
      "loss": 1.8544,
      "step": 10160
    },
    {
      "epoch": 1.918867924528302,
      "grad_norm": 0.756127119064331,
      "learning_rate": 0.00030813207547169814,
      "loss": 1.9599,
      "step": 10170
    },
    {
      "epoch": 1.920754716981132,
      "grad_norm": 0.8771906495094299,
      "learning_rate": 0.0003079433962264151,
      "loss": 1.9724,
      "step": 10180
    },
    {
      "epoch": 1.9226415094339622,
      "grad_norm": 0.7249427437782288,
      "learning_rate": 0.00030775471698113205,
      "loss": 1.8197,
      "step": 10190
    },
    {
      "epoch": 1.9245283018867925,
      "grad_norm": 0.65516197681427,
      "learning_rate": 0.00030756603773584906,
      "loss": 1.8786,
      "step": 10200
    },
    {
      "epoch": 1.9264150943396228,
      "grad_norm": 0.760061502456665,
      "learning_rate": 0.000307377358490566,
      "loss": 1.9544,
      "step": 10210
    },
    {
      "epoch": 1.9283018867924528,
      "grad_norm": 0.809380829334259,
      "learning_rate": 0.000307188679245283,
      "loss": 1.9541,
      "step": 10220
    },
    {
      "epoch": 1.930188679245283,
      "grad_norm": 0.7626461386680603,
      "learning_rate": 0.000307,
      "loss": 1.9382,
      "step": 10230
    },
    {
      "epoch": 1.9320754716981132,
      "grad_norm": 0.7414308786392212,
      "learning_rate": 0.00030681132075471694,
      "loss": 1.9129,
      "step": 10240
    },
    {
      "epoch": 1.9339622641509435,
      "grad_norm": 0.7475429177284241,
      "learning_rate": 0.000306622641509434,
      "loss": 1.8462,
      "step": 10250
    },
    {
      "epoch": 1.9358490566037736,
      "grad_norm": 0.7907422184944153,
      "learning_rate": 0.00030643396226415095,
      "loss": 1.8876,
      "step": 10260
    },
    {
      "epoch": 1.9377358490566037,
      "grad_norm": 0.8896166682243347,
      "learning_rate": 0.00030624528301886796,
      "loss": 1.9014,
      "step": 10270
    },
    {
      "epoch": 1.939622641509434,
      "grad_norm": 0.7373491525650024,
      "learning_rate": 0.0003060566037735849,
      "loss": 1.8655,
      "step": 10280
    },
    {
      "epoch": 1.9415094339622643,
      "grad_norm": 0.7380940914154053,
      "learning_rate": 0.0003058679245283019,
      "loss": 1.9174,
      "step": 10290
    },
    {
      "epoch": 1.9433962264150944,
      "grad_norm": 0.7364529967308044,
      "learning_rate": 0.0003056792452830189,
      "loss": 1.9501,
      "step": 10300
    },
    {
      "epoch": 1.9452830188679244,
      "grad_norm": 0.7281417846679688,
      "learning_rate": 0.00030549056603773584,
      "loss": 1.7998,
      "step": 10310
    },
    {
      "epoch": 1.9471698113207547,
      "grad_norm": 0.7254284620285034,
      "learning_rate": 0.00030530188679245285,
      "loss": 1.9001,
      "step": 10320
    },
    {
      "epoch": 1.949056603773585,
      "grad_norm": 0.7533168792724609,
      "learning_rate": 0.0003051132075471698,
      "loss": 1.9,
      "step": 10330
    },
    {
      "epoch": 1.950943396226415,
      "grad_norm": 0.7386412620544434,
      "learning_rate": 0.00030492452830188676,
      "loss": 1.8764,
      "step": 10340
    },
    {
      "epoch": 1.9528301886792452,
      "grad_norm": 0.7611686587333679,
      "learning_rate": 0.00030473584905660377,
      "loss": 1.8993,
      "step": 10350
    },
    {
      "epoch": 1.9547169811320755,
      "grad_norm": 0.7094703912734985,
      "learning_rate": 0.0003045471698113207,
      "loss": 1.8705,
      "step": 10360
    },
    {
      "epoch": 1.9566037735849058,
      "grad_norm": 0.7700314521789551,
      "learning_rate": 0.0003043584905660378,
      "loss": 1.9076,
      "step": 10370
    },
    {
      "epoch": 1.9584905660377359,
      "grad_norm": 0.7374099493026733,
      "learning_rate": 0.00030416981132075474,
      "loss": 1.949,
      "step": 10380
    },
    {
      "epoch": 1.960377358490566,
      "grad_norm": 0.7870099544525146,
      "learning_rate": 0.0003039811320754717,
      "loss": 1.8595,
      "step": 10390
    },
    {
      "epoch": 1.9622641509433962,
      "grad_norm": 0.6800148487091064,
      "learning_rate": 0.0003037924528301887,
      "loss": 1.9133,
      "step": 10400
    },
    {
      "epoch": 1.9641509433962265,
      "grad_norm": 0.8029755353927612,
      "learning_rate": 0.00030360377358490566,
      "loss": 1.8331,
      "step": 10410
    },
    {
      "epoch": 1.9660377358490566,
      "grad_norm": 0.6986097097396851,
      "learning_rate": 0.00030341509433962267,
      "loss": 1.9272,
      "step": 10420
    },
    {
      "epoch": 1.9679245283018867,
      "grad_norm": 0.7481592893600464,
      "learning_rate": 0.0003032264150943396,
      "loss": 1.9488,
      "step": 10430
    },
    {
      "epoch": 1.969811320754717,
      "grad_norm": 0.7604794502258301,
      "learning_rate": 0.0003030377358490566,
      "loss": 1.9479,
      "step": 10440
    },
    {
      "epoch": 1.9716981132075473,
      "grad_norm": 0.7532884478569031,
      "learning_rate": 0.0003028490566037736,
      "loss": 1.8333,
      "step": 10450
    },
    {
      "epoch": 1.9735849056603774,
      "grad_norm": 0.8063851594924927,
      "learning_rate": 0.00030266037735849054,
      "loss": 1.952,
      "step": 10460
    },
    {
      "epoch": 1.9754716981132074,
      "grad_norm": 0.7819125652313232,
      "learning_rate": 0.00030247169811320755,
      "loss": 1.8926,
      "step": 10470
    },
    {
      "epoch": 1.9773584905660377,
      "grad_norm": 0.7241382002830505,
      "learning_rate": 0.0003022830188679245,
      "loss": 1.8839,
      "step": 10480
    },
    {
      "epoch": 1.979245283018868,
      "grad_norm": 0.8182200193405151,
      "learning_rate": 0.00030209433962264146,
      "loss": 1.8717,
      "step": 10490
    },
    {
      "epoch": 1.9811320754716981,
      "grad_norm": 0.8343344330787659,
      "learning_rate": 0.0003019056603773585,
      "loss": 1.8817,
      "step": 10500
    },
    {
      "epoch": 1.9830188679245282,
      "grad_norm": 0.8320025205612183,
      "learning_rate": 0.0003017169811320755,
      "loss": 1.8412,
      "step": 10510
    },
    {
      "epoch": 1.9849056603773585,
      "grad_norm": 0.7707900404930115,
      "learning_rate": 0.0003015283018867925,
      "loss": 1.9009,
      "step": 10520
    },
    {
      "epoch": 1.9867924528301888,
      "grad_norm": 0.7166686058044434,
      "learning_rate": 0.00030133962264150945,
      "loss": 1.9536,
      "step": 10530
    },
    {
      "epoch": 1.9886792452830189,
      "grad_norm": 0.754043459892273,
      "learning_rate": 0.0003011509433962264,
      "loss": 1.9643,
      "step": 10540
    },
    {
      "epoch": 1.990566037735849,
      "grad_norm": 0.8045457601547241,
      "learning_rate": 0.0003009622641509434,
      "loss": 1.8227,
      "step": 10550
    },
    {
      "epoch": 1.9924528301886792,
      "grad_norm": 0.8168668150901794,
      "learning_rate": 0.00030077358490566037,
      "loss": 1.8328,
      "step": 10560
    },
    {
      "epoch": 1.9943396226415095,
      "grad_norm": 0.9476487040519714,
      "learning_rate": 0.0003005849056603774,
      "loss": 1.8779,
      "step": 10570
    },
    {
      "epoch": 1.9962264150943396,
      "grad_norm": 0.6902395486831665,
      "learning_rate": 0.00030039622641509433,
      "loss": 1.8347,
      "step": 10580
    },
    {
      "epoch": 1.9981132075471697,
      "grad_norm": 0.9466736912727356,
      "learning_rate": 0.0003002075471698113,
      "loss": 1.9231,
      "step": 10590
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.5040363073349,
      "learning_rate": 0.0003000188679245283,
      "loss": 1.9449,
      "step": 10600
    },
    {
      "epoch": 2.0018867924528303,
      "grad_norm": 0.6850531697273254,
      "learning_rate": 0.00029983018867924525,
      "loss": 1.8347,
      "step": 10610
    },
    {
      "epoch": 2.0037735849056606,
      "grad_norm": 0.7434271574020386,
      "learning_rate": 0.0002996415094339623,
      "loss": 1.8087,
      "step": 10620
    },
    {
      "epoch": 2.0056603773584905,
      "grad_norm": 0.7381595969200134,
      "learning_rate": 0.00029945283018867927,
      "loss": 1.8627,
      "step": 10630
    },
    {
      "epoch": 2.0075471698113208,
      "grad_norm": 0.7694036960601807,
      "learning_rate": 0.0002992641509433962,
      "loss": 1.887,
      "step": 10640
    },
    {
      "epoch": 2.009433962264151,
      "grad_norm": 0.809316873550415,
      "learning_rate": 0.00029907547169811323,
      "loss": 1.8202,
      "step": 10650
    },
    {
      "epoch": 2.0113207547169814,
      "grad_norm": 0.7399115562438965,
      "learning_rate": 0.0002988867924528302,
      "loss": 1.8551,
      "step": 10660
    },
    {
      "epoch": 2.013207547169811,
      "grad_norm": 0.7956845760345459,
      "learning_rate": 0.0002986981132075472,
      "loss": 1.9508,
      "step": 10670
    },
    {
      "epoch": 2.0150943396226415,
      "grad_norm": 0.6648094058036804,
      "learning_rate": 0.00029850943396226415,
      "loss": 1.9392,
      "step": 10680
    },
    {
      "epoch": 2.016981132075472,
      "grad_norm": 0.7877151966094971,
      "learning_rate": 0.0002983207547169811,
      "loss": 1.8248,
      "step": 10690
    },
    {
      "epoch": 2.018867924528302,
      "grad_norm": 0.7362643480300903,
      "learning_rate": 0.0002981320754716981,
      "loss": 1.9244,
      "step": 10700
    },
    {
      "epoch": 2.020754716981132,
      "grad_norm": 0.7044603228569031,
      "learning_rate": 0.00029794339622641507,
      "loss": 1.8213,
      "step": 10710
    },
    {
      "epoch": 2.0226415094339623,
      "grad_norm": 0.7486922740936279,
      "learning_rate": 0.0002977547169811321,
      "loss": 1.873,
      "step": 10720
    },
    {
      "epoch": 2.0245283018867926,
      "grad_norm": 0.7502981424331665,
      "learning_rate": 0.00029756603773584904,
      "loss": 1.8643,
      "step": 10730
    },
    {
      "epoch": 2.026415094339623,
      "grad_norm": 0.8161067366600037,
      "learning_rate": 0.00029737735849056604,
      "loss": 1.7899,
      "step": 10740
    },
    {
      "epoch": 2.0283018867924527,
      "grad_norm": 0.7275348901748657,
      "learning_rate": 0.00029718867924528305,
      "loss": 1.9106,
      "step": 10750
    },
    {
      "epoch": 2.030188679245283,
      "grad_norm": 0.8166969418525696,
      "learning_rate": 0.000297,
      "loss": 1.8688,
      "step": 10760
    },
    {
      "epoch": 2.0320754716981133,
      "grad_norm": 0.7791852355003357,
      "learning_rate": 0.000296811320754717,
      "loss": 1.8901,
      "step": 10770
    },
    {
      "epoch": 2.0339622641509436,
      "grad_norm": 0.8351361155509949,
      "learning_rate": 0.000296622641509434,
      "loss": 1.8838,
      "step": 10780
    },
    {
      "epoch": 2.0358490566037735,
      "grad_norm": 0.7840869426727295,
      "learning_rate": 0.00029643396226415093,
      "loss": 1.9457,
      "step": 10790
    },
    {
      "epoch": 2.0377358490566038,
      "grad_norm": 0.750650942325592,
      "learning_rate": 0.00029624528301886794,
      "loss": 1.8736,
      "step": 10800
    },
    {
      "epoch": 2.039622641509434,
      "grad_norm": 0.7502761483192444,
      "learning_rate": 0.0002960566037735849,
      "loss": 1.8468,
      "step": 10810
    },
    {
      "epoch": 2.0415094339622644,
      "grad_norm": 0.7500250339508057,
      "learning_rate": 0.0002958679245283019,
      "loss": 1.8762,
      "step": 10820
    },
    {
      "epoch": 2.043396226415094,
      "grad_norm": 0.7977621555328369,
      "learning_rate": 0.00029567924528301886,
      "loss": 1.8922,
      "step": 10830
    },
    {
      "epoch": 2.0452830188679245,
      "grad_norm": 0.710109293460846,
      "learning_rate": 0.0002954905660377358,
      "loss": 1.8465,
      "step": 10840
    },
    {
      "epoch": 2.047169811320755,
      "grad_norm": 0.8512477278709412,
      "learning_rate": 0.0002953018867924528,
      "loss": 1.8363,
      "step": 10850
    },
    {
      "epoch": 2.049056603773585,
      "grad_norm": 0.7323750853538513,
      "learning_rate": 0.00029511320754716983,
      "loss": 1.8868,
      "step": 10860
    },
    {
      "epoch": 2.050943396226415,
      "grad_norm": 0.7587020993232727,
      "learning_rate": 0.00029492452830188684,
      "loss": 1.8738,
      "step": 10870
    },
    {
      "epoch": 2.0528301886792453,
      "grad_norm": 0.7462083101272583,
      "learning_rate": 0.0002947358490566038,
      "loss": 1.8387,
      "step": 10880
    },
    {
      "epoch": 2.0547169811320756,
      "grad_norm": 0.8556411862373352,
      "learning_rate": 0.00029454716981132075,
      "loss": 1.938,
      "step": 10890
    },
    {
      "epoch": 2.056603773584906,
      "grad_norm": 0.7758258581161499,
      "learning_rate": 0.00029435849056603776,
      "loss": 1.8877,
      "step": 10900
    },
    {
      "epoch": 2.0584905660377357,
      "grad_norm": 0.8035060167312622,
      "learning_rate": 0.0002941698113207547,
      "loss": 1.8859,
      "step": 10910
    },
    {
      "epoch": 2.060377358490566,
      "grad_norm": 0.7316475510597229,
      "learning_rate": 0.0002939811320754717,
      "loss": 1.9083,
      "step": 10920
    },
    {
      "epoch": 2.0622641509433963,
      "grad_norm": 0.6966090202331543,
      "learning_rate": 0.0002937924528301887,
      "loss": 1.9221,
      "step": 10930
    },
    {
      "epoch": 2.0641509433962266,
      "grad_norm": 0.6864719986915588,
      "learning_rate": 0.00029360377358490563,
      "loss": 1.8924,
      "step": 10940
    },
    {
      "epoch": 2.0660377358490565,
      "grad_norm": 0.711174488067627,
      "learning_rate": 0.00029341509433962264,
      "loss": 1.9104,
      "step": 10950
    },
    {
      "epoch": 2.0679245283018868,
      "grad_norm": 0.776553213596344,
      "learning_rate": 0.0002932264150943396,
      "loss": 1.9188,
      "step": 10960
    },
    {
      "epoch": 2.069811320754717,
      "grad_norm": 0.8168472647666931,
      "learning_rate": 0.0002930377358490566,
      "loss": 1.933,
      "step": 10970
    },
    {
      "epoch": 2.0716981132075474,
      "grad_norm": 0.7703675627708435,
      "learning_rate": 0.00029284905660377356,
      "loss": 1.8867,
      "step": 10980
    },
    {
      "epoch": 2.0735849056603772,
      "grad_norm": 0.7051811814308167,
      "learning_rate": 0.00029266037735849057,
      "loss": 1.8811,
      "step": 10990
    },
    {
      "epoch": 2.0754716981132075,
      "grad_norm": 0.7031225562095642,
      "learning_rate": 0.0002924716981132076,
      "loss": 1.9796,
      "step": 11000
    },
    {
      "epoch": 2.077358490566038,
      "grad_norm": 0.7752034068107605,
      "learning_rate": 0.00029228301886792454,
      "loss": 1.8354,
      "step": 11010
    },
    {
      "epoch": 2.079245283018868,
      "grad_norm": 0.76606684923172,
      "learning_rate": 0.00029209433962264155,
      "loss": 1.9088,
      "step": 11020
    },
    {
      "epoch": 2.081132075471698,
      "grad_norm": 0.7504165768623352,
      "learning_rate": 0.0002919056603773585,
      "loss": 1.8574,
      "step": 11030
    },
    {
      "epoch": 2.0830188679245283,
      "grad_norm": 0.6677709817886353,
      "learning_rate": 0.00029171698113207546,
      "loss": 1.935,
      "step": 11040
    },
    {
      "epoch": 2.0849056603773586,
      "grad_norm": 0.7948572039604187,
      "learning_rate": 0.00029152830188679247,
      "loss": 1.9229,
      "step": 11050
    },
    {
      "epoch": 2.086792452830189,
      "grad_norm": 0.6528365612030029,
      "learning_rate": 0.0002913396226415094,
      "loss": 1.9053,
      "step": 11060
    },
    {
      "epoch": 2.0886792452830187,
      "grad_norm": 0.9241406321525574,
      "learning_rate": 0.00029115094339622643,
      "loss": 1.8179,
      "step": 11070
    },
    {
      "epoch": 2.090566037735849,
      "grad_norm": 0.791943371295929,
      "learning_rate": 0.0002909622641509434,
      "loss": 1.8255,
      "step": 11080
    },
    {
      "epoch": 2.0924528301886793,
      "grad_norm": 0.80291748046875,
      "learning_rate": 0.00029077358490566034,
      "loss": 1.9064,
      "step": 11090
    },
    {
      "epoch": 2.0943396226415096,
      "grad_norm": 0.779280960559845,
      "learning_rate": 0.00029058490566037735,
      "loss": 1.8835,
      "step": 11100
    },
    {
      "epoch": 2.0962264150943395,
      "grad_norm": 0.6913021802902222,
      "learning_rate": 0.00029039622641509436,
      "loss": 1.8944,
      "step": 11110
    },
    {
      "epoch": 2.09811320754717,
      "grad_norm": 0.7940987944602966,
      "learning_rate": 0.00029020754716981137,
      "loss": 1.8424,
      "step": 11120
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.7682672739028931,
      "learning_rate": 0.0002900188679245283,
      "loss": 1.9161,
      "step": 11130
    },
    {
      "epoch": 2.1018867924528304,
      "grad_norm": 0.7978420257568359,
      "learning_rate": 0.0002898301886792453,
      "loss": 1.9239,
      "step": 11140
    },
    {
      "epoch": 2.1037735849056602,
      "grad_norm": 0.8014338612556458,
      "learning_rate": 0.0002896415094339623,
      "loss": 1.8723,
      "step": 11150
    },
    {
      "epoch": 2.1056603773584905,
      "grad_norm": 0.9492523074150085,
      "learning_rate": 0.00028945283018867924,
      "loss": 1.847,
      "step": 11160
    },
    {
      "epoch": 2.107547169811321,
      "grad_norm": 0.694970428943634,
      "learning_rate": 0.00028926415094339625,
      "loss": 1.8353,
      "step": 11170
    },
    {
      "epoch": 2.109433962264151,
      "grad_norm": 0.8932681679725647,
      "learning_rate": 0.0002890754716981132,
      "loss": 1.9278,
      "step": 11180
    },
    {
      "epoch": 2.111320754716981,
      "grad_norm": 0.7530858516693115,
      "learning_rate": 0.00028888679245283016,
      "loss": 1.8427,
      "step": 11190
    },
    {
      "epoch": 2.1132075471698113,
      "grad_norm": 0.7408515214920044,
      "learning_rate": 0.00028869811320754717,
      "loss": 1.9421,
      "step": 11200
    },
    {
      "epoch": 2.1150943396226416,
      "grad_norm": 0.8429136276245117,
      "learning_rate": 0.0002885094339622641,
      "loss": 1.873,
      "step": 11210
    },
    {
      "epoch": 2.116981132075472,
      "grad_norm": 0.8049519062042236,
      "learning_rate": 0.00028832075471698113,
      "loss": 1.9025,
      "step": 11220
    },
    {
      "epoch": 2.1188679245283017,
      "grad_norm": 0.7757302522659302,
      "learning_rate": 0.00028813207547169814,
      "loss": 1.96,
      "step": 11230
    },
    {
      "epoch": 2.120754716981132,
      "grad_norm": 0.906645655632019,
      "learning_rate": 0.0002879433962264151,
      "loss": 1.8632,
      "step": 11240
    },
    {
      "epoch": 2.1226415094339623,
      "grad_norm": 0.7992925643920898,
      "learning_rate": 0.0002877547169811321,
      "loss": 1.8879,
      "step": 11250
    },
    {
      "epoch": 2.1245283018867926,
      "grad_norm": 0.8199301958084106,
      "learning_rate": 0.00028756603773584906,
      "loss": 1.8976,
      "step": 11260
    },
    {
      "epoch": 2.1264150943396225,
      "grad_norm": 0.8391996026039124,
      "learning_rate": 0.00028737735849056607,
      "loss": 1.8981,
      "step": 11270
    },
    {
      "epoch": 2.128301886792453,
      "grad_norm": 0.8028163313865662,
      "learning_rate": 0.00028718867924528303,
      "loss": 1.8079,
      "step": 11280
    },
    {
      "epoch": 2.130188679245283,
      "grad_norm": 0.8211103677749634,
      "learning_rate": 0.000287,
      "loss": 1.8876,
      "step": 11290
    },
    {
      "epoch": 2.1320754716981134,
      "grad_norm": 0.7007796764373779,
      "learning_rate": 0.000286811320754717,
      "loss": 1.8963,
      "step": 11300
    },
    {
      "epoch": 2.1339622641509433,
      "grad_norm": 0.826571524143219,
      "learning_rate": 0.00028662264150943395,
      "loss": 1.8417,
      "step": 11310
    },
    {
      "epoch": 2.1358490566037736,
      "grad_norm": 0.8056465983390808,
      "learning_rate": 0.00028643396226415096,
      "loss": 1.8072,
      "step": 11320
    },
    {
      "epoch": 2.137735849056604,
      "grad_norm": 0.7737205624580383,
      "learning_rate": 0.0002862452830188679,
      "loss": 1.832,
      "step": 11330
    },
    {
      "epoch": 2.139622641509434,
      "grad_norm": 0.8111943602561951,
      "learning_rate": 0.00028605660377358487,
      "loss": 1.8597,
      "step": 11340
    },
    {
      "epoch": 2.141509433962264,
      "grad_norm": 0.7717664837837219,
      "learning_rate": 0.00028586792452830193,
      "loss": 1.9473,
      "step": 11350
    },
    {
      "epoch": 2.1433962264150943,
      "grad_norm": 0.787202000617981,
      "learning_rate": 0.0002856792452830189,
      "loss": 1.8649,
      "step": 11360
    },
    {
      "epoch": 2.1452830188679246,
      "grad_norm": 0.8129977583885193,
      "learning_rate": 0.0002854905660377359,
      "loss": 1.8961,
      "step": 11370
    },
    {
      "epoch": 2.147169811320755,
      "grad_norm": 0.7762576341629028,
      "learning_rate": 0.00028530188679245285,
      "loss": 1.9621,
      "step": 11380
    },
    {
      "epoch": 2.1490566037735848,
      "grad_norm": 0.8172262907028198,
      "learning_rate": 0.0002851132075471698,
      "loss": 1.7841,
      "step": 11390
    },
    {
      "epoch": 2.150943396226415,
      "grad_norm": 0.7173696756362915,
      "learning_rate": 0.0002849245283018868,
      "loss": 1.8478,
      "step": 11400
    },
    {
      "epoch": 2.1528301886792454,
      "grad_norm": 0.7500424981117249,
      "learning_rate": 0.00028473584905660377,
      "loss": 1.8673,
      "step": 11410
    },
    {
      "epoch": 2.1547169811320757,
      "grad_norm": 0.7751813530921936,
      "learning_rate": 0.0002845471698113208,
      "loss": 1.8681,
      "step": 11420
    },
    {
      "epoch": 2.1566037735849055,
      "grad_norm": 0.9747641086578369,
      "learning_rate": 0.00028435849056603773,
      "loss": 1.9209,
      "step": 11430
    },
    {
      "epoch": 2.158490566037736,
      "grad_norm": 0.8313957452774048,
      "learning_rate": 0.0002841698113207547,
      "loss": 1.8658,
      "step": 11440
    },
    {
      "epoch": 2.160377358490566,
      "grad_norm": 0.8337726593017578,
      "learning_rate": 0.0002839811320754717,
      "loss": 1.9203,
      "step": 11450
    },
    {
      "epoch": 2.1622641509433964,
      "grad_norm": 0.7217211127281189,
      "learning_rate": 0.00028379245283018865,
      "loss": 1.8511,
      "step": 11460
    },
    {
      "epoch": 2.1641509433962263,
      "grad_norm": 0.8143007159233093,
      "learning_rate": 0.00028360377358490566,
      "loss": 1.9449,
      "step": 11470
    },
    {
      "epoch": 2.1660377358490566,
      "grad_norm": 0.781199038028717,
      "learning_rate": 0.00028341509433962267,
      "loss": 1.911,
      "step": 11480
    },
    {
      "epoch": 2.167924528301887,
      "grad_norm": 0.715600848197937,
      "learning_rate": 0.0002832264150943396,
      "loss": 1.9038,
      "step": 11490
    },
    {
      "epoch": 2.169811320754717,
      "grad_norm": 0.7850223779678345,
      "learning_rate": 0.00028303773584905664,
      "loss": 1.911,
      "step": 11500
    },
    {
      "epoch": 2.171698113207547,
      "grad_norm": 0.8531944751739502,
      "learning_rate": 0.0002828490566037736,
      "loss": 1.9442,
      "step": 11510
    },
    {
      "epoch": 2.1735849056603773,
      "grad_norm": 0.7563462853431702,
      "learning_rate": 0.0002826603773584906,
      "loss": 1.8119,
      "step": 11520
    },
    {
      "epoch": 2.1754716981132076,
      "grad_norm": 0.9591594934463501,
      "learning_rate": 0.00028247169811320756,
      "loss": 1.9182,
      "step": 11530
    },
    {
      "epoch": 2.177358490566038,
      "grad_norm": 0.8583648800849915,
      "learning_rate": 0.0002822830188679245,
      "loss": 1.8263,
      "step": 11540
    },
    {
      "epoch": 2.1792452830188678,
      "grad_norm": 0.7384650707244873,
      "learning_rate": 0.0002820943396226415,
      "loss": 1.9577,
      "step": 11550
    },
    {
      "epoch": 2.181132075471698,
      "grad_norm": 0.7872804403305054,
      "learning_rate": 0.0002819056603773585,
      "loss": 1.9037,
      "step": 11560
    },
    {
      "epoch": 2.1830188679245284,
      "grad_norm": 0.773271918296814,
      "learning_rate": 0.0002817169811320755,
      "loss": 1.8607,
      "step": 11570
    },
    {
      "epoch": 2.1849056603773587,
      "grad_norm": 0.7978825569152832,
      "learning_rate": 0.00028152830188679244,
      "loss": 1.9322,
      "step": 11580
    },
    {
      "epoch": 2.1867924528301885,
      "grad_norm": 0.8050822019577026,
      "learning_rate": 0.0002813396226415094,
      "loss": 1.8965,
      "step": 11590
    },
    {
      "epoch": 2.188679245283019,
      "grad_norm": 0.7440901398658752,
      "learning_rate": 0.00028115094339622646,
      "loss": 1.9349,
      "step": 11600
    },
    {
      "epoch": 2.190566037735849,
      "grad_norm": 0.7741793394088745,
      "learning_rate": 0.0002809622641509434,
      "loss": 1.8356,
      "step": 11610
    },
    {
      "epoch": 2.1924528301886794,
      "grad_norm": 0.6815228462219238,
      "learning_rate": 0.0002807735849056604,
      "loss": 1.8746,
      "step": 11620
    },
    {
      "epoch": 2.1943396226415093,
      "grad_norm": 0.7894241809844971,
      "learning_rate": 0.0002805849056603774,
      "loss": 1.8859,
      "step": 11630
    },
    {
      "epoch": 2.1962264150943396,
      "grad_norm": 0.7033816576004028,
      "learning_rate": 0.00028039622641509433,
      "loss": 1.8584,
      "step": 11640
    },
    {
      "epoch": 2.19811320754717,
      "grad_norm": 0.7304489016532898,
      "learning_rate": 0.00028020754716981134,
      "loss": 1.8333,
      "step": 11650
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.8004820942878723,
      "learning_rate": 0.0002800188679245283,
      "loss": 1.9909,
      "step": 11660
    },
    {
      "epoch": 2.20188679245283,
      "grad_norm": 0.74045330286026,
      "learning_rate": 0.0002798301886792453,
      "loss": 1.9054,
      "step": 11670
    },
    {
      "epoch": 2.2037735849056603,
      "grad_norm": 0.8282209634780884,
      "learning_rate": 0.00027964150943396226,
      "loss": 1.9104,
      "step": 11680
    },
    {
      "epoch": 2.2056603773584906,
      "grad_norm": 0.8628226518630981,
      "learning_rate": 0.0002794528301886792,
      "loss": 1.8815,
      "step": 11690
    },
    {
      "epoch": 2.207547169811321,
      "grad_norm": 0.7230659127235413,
      "learning_rate": 0.0002792641509433962,
      "loss": 1.7597,
      "step": 11700
    },
    {
      "epoch": 2.209433962264151,
      "grad_norm": 0.7778328061103821,
      "learning_rate": 0.0002790754716981132,
      "loss": 1.8033,
      "step": 11710
    },
    {
      "epoch": 2.211320754716981,
      "grad_norm": 0.8020375967025757,
      "learning_rate": 0.00027888679245283024,
      "loss": 1.6849,
      "step": 11720
    },
    {
      "epoch": 2.2132075471698114,
      "grad_norm": 0.8092707991600037,
      "learning_rate": 0.0002786981132075472,
      "loss": 1.8846,
      "step": 11730
    },
    {
      "epoch": 2.2150943396226417,
      "grad_norm": 0.819585919380188,
      "learning_rate": 0.00027850943396226415,
      "loss": 1.9274,
      "step": 11740
    },
    {
      "epoch": 2.2169811320754715,
      "grad_norm": 0.7181550860404968,
      "learning_rate": 0.00027832075471698116,
      "loss": 1.933,
      "step": 11750
    },
    {
      "epoch": 2.218867924528302,
      "grad_norm": 0.8078043460845947,
      "learning_rate": 0.0002781320754716981,
      "loss": 1.9626,
      "step": 11760
    },
    {
      "epoch": 2.220754716981132,
      "grad_norm": 0.736039400100708,
      "learning_rate": 0.00027794339622641513,
      "loss": 1.9648,
      "step": 11770
    },
    {
      "epoch": 2.2226415094339624,
      "grad_norm": 0.7964351177215576,
      "learning_rate": 0.0002777547169811321,
      "loss": 1.8519,
      "step": 11780
    },
    {
      "epoch": 2.2245283018867923,
      "grad_norm": 0.6812192797660828,
      "learning_rate": 0.00027756603773584904,
      "loss": 1.858,
      "step": 11790
    },
    {
      "epoch": 2.2264150943396226,
      "grad_norm": 0.821424126625061,
      "learning_rate": 0.00027737735849056605,
      "loss": 1.8692,
      "step": 11800
    },
    {
      "epoch": 2.228301886792453,
      "grad_norm": 0.8376649618148804,
      "learning_rate": 0.000277188679245283,
      "loss": 1.976,
      "step": 11810
    },
    {
      "epoch": 2.230188679245283,
      "grad_norm": 0.7699478268623352,
      "learning_rate": 0.000277,
      "loss": 1.8252,
      "step": 11820
    },
    {
      "epoch": 2.232075471698113,
      "grad_norm": 0.7748491168022156,
      "learning_rate": 0.00027681132075471697,
      "loss": 1.7733,
      "step": 11830
    },
    {
      "epoch": 2.2339622641509433,
      "grad_norm": 0.8734055757522583,
      "learning_rate": 0.0002766226415094339,
      "loss": 1.811,
      "step": 11840
    },
    {
      "epoch": 2.2358490566037736,
      "grad_norm": 0.8055007457733154,
      "learning_rate": 0.000276433962264151,
      "loss": 1.8706,
      "step": 11850
    },
    {
      "epoch": 2.237735849056604,
      "grad_norm": 0.8176400065422058,
      "learning_rate": 0.00027624528301886794,
      "loss": 1.8852,
      "step": 11860
    },
    {
      "epoch": 2.239622641509434,
      "grad_norm": 0.7215754985809326,
      "learning_rate": 0.00027605660377358495,
      "loss": 1.9018,
      "step": 11870
    },
    {
      "epoch": 2.241509433962264,
      "grad_norm": 0.9487243294715881,
      "learning_rate": 0.0002758679245283019,
      "loss": 1.9792,
      "step": 11880
    },
    {
      "epoch": 2.2433962264150944,
      "grad_norm": 0.7651824951171875,
      "learning_rate": 0.00027567924528301886,
      "loss": 1.8987,
      "step": 11890
    },
    {
      "epoch": 2.2452830188679247,
      "grad_norm": 0.8216924667358398,
      "learning_rate": 0.00027549056603773587,
      "loss": 1.8191,
      "step": 11900
    },
    {
      "epoch": 2.2471698113207546,
      "grad_norm": 0.7723641991615295,
      "learning_rate": 0.0002753018867924528,
      "loss": 1.8612,
      "step": 11910
    },
    {
      "epoch": 2.249056603773585,
      "grad_norm": 0.7536110281944275,
      "learning_rate": 0.00027511320754716983,
      "loss": 1.8766,
      "step": 11920
    },
    {
      "epoch": 2.250943396226415,
      "grad_norm": 0.7673664093017578,
      "learning_rate": 0.0002749245283018868,
      "loss": 1.775,
      "step": 11930
    },
    {
      "epoch": 2.2528301886792454,
      "grad_norm": 0.8621387481689453,
      "learning_rate": 0.00027473584905660374,
      "loss": 1.855,
      "step": 11940
    },
    {
      "epoch": 2.2547169811320753,
      "grad_norm": 0.8291190266609192,
      "learning_rate": 0.00027454716981132075,
      "loss": 1.8883,
      "step": 11950
    },
    {
      "epoch": 2.2566037735849056,
      "grad_norm": 0.7831971049308777,
      "learning_rate": 0.0002743584905660377,
      "loss": 1.8383,
      "step": 11960
    },
    {
      "epoch": 2.258490566037736,
      "grad_norm": 0.7580150961875916,
      "learning_rate": 0.00027416981132075477,
      "loss": 1.934,
      "step": 11970
    },
    {
      "epoch": 2.260377358490566,
      "grad_norm": 0.8647801280021667,
      "learning_rate": 0.0002739811320754717,
      "loss": 1.8741,
      "step": 11980
    },
    {
      "epoch": 2.262264150943396,
      "grad_norm": 0.7437734603881836,
      "learning_rate": 0.0002737924528301887,
      "loss": 1.858,
      "step": 11990
    },
    {
      "epoch": 2.2641509433962264,
      "grad_norm": 0.795907199382782,
      "learning_rate": 0.0002736037735849057,
      "loss": 1.9103,
      "step": 12000
    },
    {
      "epoch": 2.2660377358490567,
      "grad_norm": 0.7082441449165344,
      "learning_rate": 0.00027341509433962265,
      "loss": 1.7766,
      "step": 12010
    },
    {
      "epoch": 2.267924528301887,
      "grad_norm": 0.7707059979438782,
      "learning_rate": 0.00027322641509433965,
      "loss": 1.8618,
      "step": 12020
    },
    {
      "epoch": 2.269811320754717,
      "grad_norm": 0.8098462224006653,
      "learning_rate": 0.0002730377358490566,
      "loss": 1.8246,
      "step": 12030
    },
    {
      "epoch": 2.271698113207547,
      "grad_norm": 0.8795305490493774,
      "learning_rate": 0.00027284905660377356,
      "loss": 1.9755,
      "step": 12040
    },
    {
      "epoch": 2.2735849056603774,
      "grad_norm": 0.8094635009765625,
      "learning_rate": 0.0002726603773584906,
      "loss": 1.9211,
      "step": 12050
    },
    {
      "epoch": 2.2754716981132077,
      "grad_norm": 0.7859498262405396,
      "learning_rate": 0.00027247169811320753,
      "loss": 1.9053,
      "step": 12060
    },
    {
      "epoch": 2.2773584905660376,
      "grad_norm": 0.8243895173072815,
      "learning_rate": 0.00027228301886792454,
      "loss": 1.9265,
      "step": 12070
    },
    {
      "epoch": 2.279245283018868,
      "grad_norm": 0.7599847316741943,
      "learning_rate": 0.0002720943396226415,
      "loss": 1.8785,
      "step": 12080
    },
    {
      "epoch": 2.281132075471698,
      "grad_norm": 0.793107271194458,
      "learning_rate": 0.0002719056603773585,
      "loss": 1.9379,
      "step": 12090
    },
    {
      "epoch": 2.2830188679245285,
      "grad_norm": 0.8986262679100037,
      "learning_rate": 0.0002717169811320755,
      "loss": 1.9162,
      "step": 12100
    },
    {
      "epoch": 2.2849056603773583,
      "grad_norm": 0.8026441335678101,
      "learning_rate": 0.00027152830188679247,
      "loss": 1.852,
      "step": 12110
    },
    {
      "epoch": 2.2867924528301886,
      "grad_norm": 0.8144442439079285,
      "learning_rate": 0.0002713396226415095,
      "loss": 1.8233,
      "step": 12120
    },
    {
      "epoch": 2.288679245283019,
      "grad_norm": 0.7876712679862976,
      "learning_rate": 0.00027115094339622643,
      "loss": 1.9326,
      "step": 12130
    },
    {
      "epoch": 2.290566037735849,
      "grad_norm": 0.7979635000228882,
      "learning_rate": 0.0002709622641509434,
      "loss": 1.9252,
      "step": 12140
    },
    {
      "epoch": 2.292452830188679,
      "grad_norm": 0.8806134462356567,
      "learning_rate": 0.0002707735849056604,
      "loss": 1.8192,
      "step": 12150
    },
    {
      "epoch": 2.2943396226415094,
      "grad_norm": 0.7938712239265442,
      "learning_rate": 0.00027058490566037735,
      "loss": 1.8784,
      "step": 12160
    },
    {
      "epoch": 2.2962264150943397,
      "grad_norm": 0.7557908296585083,
      "learning_rate": 0.00027039622641509436,
      "loss": 1.8314,
      "step": 12170
    },
    {
      "epoch": 2.29811320754717,
      "grad_norm": 0.7944236397743225,
      "learning_rate": 0.0002702075471698113,
      "loss": 1.8116,
      "step": 12180
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.900937557220459,
      "learning_rate": 0.00027001886792452827,
      "loss": 1.8262,
      "step": 12190
    },
    {
      "epoch": 2.30188679245283,
      "grad_norm": 0.7932180166244507,
      "learning_rate": 0.0002698301886792453,
      "loss": 1.9079,
      "step": 12200
    },
    {
      "epoch": 2.3037735849056604,
      "grad_norm": 0.7562607526779175,
      "learning_rate": 0.0002696415094339623,
      "loss": 1.8084,
      "step": 12210
    },
    {
      "epoch": 2.3056603773584907,
      "grad_norm": 0.8068732023239136,
      "learning_rate": 0.0002694528301886793,
      "loss": 1.8848,
      "step": 12220
    },
    {
      "epoch": 2.3075471698113206,
      "grad_norm": 0.704479992389679,
      "learning_rate": 0.00026926415094339625,
      "loss": 1.9128,
      "step": 12230
    },
    {
      "epoch": 2.309433962264151,
      "grad_norm": 0.8433565497398376,
      "learning_rate": 0.0002690754716981132,
      "loss": 1.8141,
      "step": 12240
    },
    {
      "epoch": 2.311320754716981,
      "grad_norm": 0.7264156937599182,
      "learning_rate": 0.0002688867924528302,
      "loss": 1.7941,
      "step": 12250
    },
    {
      "epoch": 2.3132075471698115,
      "grad_norm": 0.817346453666687,
      "learning_rate": 0.00026869811320754717,
      "loss": 1.9456,
      "step": 12260
    },
    {
      "epoch": 2.3150943396226413,
      "grad_norm": 0.7527860999107361,
      "learning_rate": 0.0002685094339622642,
      "loss": 1.9262,
      "step": 12270
    },
    {
      "epoch": 2.3169811320754716,
      "grad_norm": 0.6982164978981018,
      "learning_rate": 0.00026832075471698114,
      "loss": 1.908,
      "step": 12280
    },
    {
      "epoch": 2.318867924528302,
      "grad_norm": 0.7714651226997375,
      "learning_rate": 0.0002681320754716981,
      "loss": 1.896,
      "step": 12290
    },
    {
      "epoch": 2.3207547169811322,
      "grad_norm": 0.9191035032272339,
      "learning_rate": 0.0002679433962264151,
      "loss": 1.9224,
      "step": 12300
    },
    {
      "epoch": 2.322641509433962,
      "grad_norm": 0.8031037449836731,
      "learning_rate": 0.00026775471698113206,
      "loss": 1.8861,
      "step": 12310
    },
    {
      "epoch": 2.3245283018867924,
      "grad_norm": 0.9053866863250732,
      "learning_rate": 0.00026756603773584907,
      "loss": 1.8587,
      "step": 12320
    },
    {
      "epoch": 2.3264150943396227,
      "grad_norm": 0.7831084728240967,
      "learning_rate": 0.000267377358490566,
      "loss": 1.9133,
      "step": 12330
    },
    {
      "epoch": 2.328301886792453,
      "grad_norm": 0.703385591506958,
      "learning_rate": 0.00026718867924528303,
      "loss": 1.7929,
      "step": 12340
    },
    {
      "epoch": 2.330188679245283,
      "grad_norm": 0.7738015651702881,
      "learning_rate": 0.00026700000000000004,
      "loss": 1.8415,
      "step": 12350
    },
    {
      "epoch": 2.332075471698113,
      "grad_norm": 0.7067212462425232,
      "learning_rate": 0.000266811320754717,
      "loss": 1.8285,
      "step": 12360
    },
    {
      "epoch": 2.3339622641509434,
      "grad_norm": 0.879393994808197,
      "learning_rate": 0.000266622641509434,
      "loss": 1.9014,
      "step": 12370
    },
    {
      "epoch": 2.3358490566037737,
      "grad_norm": 0.7603340148925781,
      "learning_rate": 0.00026643396226415096,
      "loss": 1.929,
      "step": 12380
    },
    {
      "epoch": 2.3377358490566036,
      "grad_norm": 0.7176015973091125,
      "learning_rate": 0.0002662452830188679,
      "loss": 1.9513,
      "step": 12390
    },
    {
      "epoch": 2.339622641509434,
      "grad_norm": 0.7467360496520996,
      "learning_rate": 0.0002660566037735849,
      "loss": 1.9242,
      "step": 12400
    },
    {
      "epoch": 2.341509433962264,
      "grad_norm": 0.7887352705001831,
      "learning_rate": 0.0002658679245283019,
      "loss": 1.8535,
      "step": 12410
    },
    {
      "epoch": 2.3433962264150945,
      "grad_norm": 0.806025505065918,
      "learning_rate": 0.0002656792452830189,
      "loss": 1.7987,
      "step": 12420
    },
    {
      "epoch": 2.3452830188679243,
      "grad_norm": 0.7605155110359192,
      "learning_rate": 0.00026549056603773584,
      "loss": 1.8294,
      "step": 12430
    },
    {
      "epoch": 2.3471698113207546,
      "grad_norm": 0.7467135787010193,
      "learning_rate": 0.0002653018867924528,
      "loss": 1.8801,
      "step": 12440
    },
    {
      "epoch": 2.349056603773585,
      "grad_norm": 0.8795685172080994,
      "learning_rate": 0.0002651132075471698,
      "loss": 1.9303,
      "step": 12450
    },
    {
      "epoch": 2.3509433962264152,
      "grad_norm": 1.1168633699417114,
      "learning_rate": 0.0002649245283018868,
      "loss": 1.8832,
      "step": 12460
    },
    {
      "epoch": 2.352830188679245,
      "grad_norm": 0.7552995085716248,
      "learning_rate": 0.0002647358490566038,
      "loss": 1.8983,
      "step": 12470
    },
    {
      "epoch": 2.3547169811320754,
      "grad_norm": 0.7893361449241638,
      "learning_rate": 0.0002645471698113208,
      "loss": 1.88,
      "step": 12480
    },
    {
      "epoch": 2.3566037735849057,
      "grad_norm": 0.7935708165168762,
      "learning_rate": 0.00026435849056603774,
      "loss": 1.9223,
      "step": 12490
    },
    {
      "epoch": 2.358490566037736,
      "grad_norm": 0.8368620872497559,
      "learning_rate": 0.00026416981132075474,
      "loss": 1.8775,
      "step": 12500
    },
    {
      "epoch": 2.360377358490566,
      "grad_norm": 0.762365460395813,
      "learning_rate": 0.0002639811320754717,
      "loss": 1.8358,
      "step": 12510
    },
    {
      "epoch": 2.362264150943396,
      "grad_norm": 0.8476556539535522,
      "learning_rate": 0.0002637924528301887,
      "loss": 1.9092,
      "step": 12520
    },
    {
      "epoch": 2.3641509433962264,
      "grad_norm": 0.9112697243690491,
      "learning_rate": 0.00026360377358490566,
      "loss": 1.8317,
      "step": 12530
    },
    {
      "epoch": 2.3660377358490567,
      "grad_norm": 0.7658770680427551,
      "learning_rate": 0.0002634150943396226,
      "loss": 1.8476,
      "step": 12540
    },
    {
      "epoch": 2.3679245283018866,
      "grad_norm": 0.784636914730072,
      "learning_rate": 0.00026322641509433963,
      "loss": 1.8207,
      "step": 12550
    },
    {
      "epoch": 2.369811320754717,
      "grad_norm": 0.8392016291618347,
      "learning_rate": 0.0002630377358490566,
      "loss": 1.8709,
      "step": 12560
    },
    {
      "epoch": 2.371698113207547,
      "grad_norm": 0.7476776242256165,
      "learning_rate": 0.0002628490566037736,
      "loss": 1.8751,
      "step": 12570
    },
    {
      "epoch": 2.3735849056603775,
      "grad_norm": 0.7239780426025391,
      "learning_rate": 0.0002626603773584906,
      "loss": 1.8154,
      "step": 12580
    },
    {
      "epoch": 2.3754716981132074,
      "grad_norm": 0.7923220992088318,
      "learning_rate": 0.00026247169811320756,
      "loss": 1.9106,
      "step": 12590
    },
    {
      "epoch": 2.3773584905660377,
      "grad_norm": 0.8570379614830017,
      "learning_rate": 0.00026228301886792457,
      "loss": 1.942,
      "step": 12600
    },
    {
      "epoch": 2.379245283018868,
      "grad_norm": 0.7877200841903687,
      "learning_rate": 0.0002620943396226415,
      "loss": 1.9067,
      "step": 12610
    },
    {
      "epoch": 2.3811320754716983,
      "grad_norm": 0.8990855813026428,
      "learning_rate": 0.00026190566037735853,
      "loss": 1.9517,
      "step": 12620
    },
    {
      "epoch": 2.3830188679245285,
      "grad_norm": 0.8192134499549866,
      "learning_rate": 0.0002617169811320755,
      "loss": 1.9159,
      "step": 12630
    },
    {
      "epoch": 2.3849056603773584,
      "grad_norm": 0.7598971724510193,
      "learning_rate": 0.00026152830188679244,
      "loss": 1.8543,
      "step": 12640
    },
    {
      "epoch": 2.3867924528301887,
      "grad_norm": 1.00441575050354,
      "learning_rate": 0.00026133962264150945,
      "loss": 1.9093,
      "step": 12650
    },
    {
      "epoch": 2.388679245283019,
      "grad_norm": 0.8261236548423767,
      "learning_rate": 0.0002611509433962264,
      "loss": 1.8625,
      "step": 12660
    },
    {
      "epoch": 2.390566037735849,
      "grad_norm": 0.7754991054534912,
      "learning_rate": 0.0002609622641509434,
      "loss": 1.9251,
      "step": 12670
    },
    {
      "epoch": 2.392452830188679,
      "grad_norm": 0.7133496999740601,
      "learning_rate": 0.00026077358490566037,
      "loss": 1.9423,
      "step": 12680
    },
    {
      "epoch": 2.3943396226415095,
      "grad_norm": 0.7650933861732483,
      "learning_rate": 0.0002605849056603773,
      "loss": 1.829,
      "step": 12690
    },
    {
      "epoch": 2.3962264150943398,
      "grad_norm": 0.820488452911377,
      "learning_rate": 0.0002603962264150944,
      "loss": 1.8987,
      "step": 12700
    },
    {
      "epoch": 2.39811320754717,
      "grad_norm": 0.8574065566062927,
      "learning_rate": 0.00026020754716981134,
      "loss": 1.8574,
      "step": 12710
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.7551575303077698,
      "learning_rate": 0.00026001886792452835,
      "loss": 1.8252,
      "step": 12720
    },
    {
      "epoch": 2.40188679245283,
      "grad_norm": 0.7382228374481201,
      "learning_rate": 0.0002598301886792453,
      "loss": 1.8704,
      "step": 12730
    },
    {
      "epoch": 2.4037735849056605,
      "grad_norm": 0.7167612314224243,
      "learning_rate": 0.00025964150943396226,
      "loss": 1.848,
      "step": 12740
    },
    {
      "epoch": 2.4056603773584904,
      "grad_norm": 0.7695122957229614,
      "learning_rate": 0.00025945283018867927,
      "loss": 2.0094,
      "step": 12750
    },
    {
      "epoch": 2.4075471698113207,
      "grad_norm": 0.8108821511268616,
      "learning_rate": 0.0002592641509433962,
      "loss": 1.8865,
      "step": 12760
    },
    {
      "epoch": 2.409433962264151,
      "grad_norm": 0.7630618214607239,
      "learning_rate": 0.0002590754716981132,
      "loss": 1.839,
      "step": 12770
    },
    {
      "epoch": 2.4113207547169813,
      "grad_norm": 0.8032107949256897,
      "learning_rate": 0.0002588867924528302,
      "loss": 1.8825,
      "step": 12780
    },
    {
      "epoch": 2.4132075471698116,
      "grad_norm": 0.8483603000640869,
      "learning_rate": 0.00025869811320754715,
      "loss": 1.8991,
      "step": 12790
    },
    {
      "epoch": 2.4150943396226414,
      "grad_norm": 0.8174042701721191,
      "learning_rate": 0.00025850943396226416,
      "loss": 1.7902,
      "step": 12800
    },
    {
      "epoch": 2.4169811320754717,
      "grad_norm": 0.8031938076019287,
      "learning_rate": 0.0002583207547169811,
      "loss": 1.889,
      "step": 12810
    },
    {
      "epoch": 2.418867924528302,
      "grad_norm": 0.7526648044586182,
      "learning_rate": 0.00025813207547169807,
      "loss": 1.9808,
      "step": 12820
    },
    {
      "epoch": 2.420754716981132,
      "grad_norm": 0.7512980103492737,
      "learning_rate": 0.00025794339622641513,
      "loss": 1.7798,
      "step": 12830
    },
    {
      "epoch": 2.422641509433962,
      "grad_norm": 0.8236560821533203,
      "learning_rate": 0.0002577547169811321,
      "loss": 1.9086,
      "step": 12840
    },
    {
      "epoch": 2.4245283018867925,
      "grad_norm": 0.7252369523048401,
      "learning_rate": 0.0002575660377358491,
      "loss": 1.8342,
      "step": 12850
    },
    {
      "epoch": 2.4264150943396228,
      "grad_norm": 0.6698943376541138,
      "learning_rate": 0.00025737735849056605,
      "loss": 1.863,
      "step": 12860
    },
    {
      "epoch": 2.428301886792453,
      "grad_norm": 0.8238218426704407,
      "learning_rate": 0.000257188679245283,
      "loss": 1.8749,
      "step": 12870
    },
    {
      "epoch": 2.430188679245283,
      "grad_norm": 0.7768046855926514,
      "learning_rate": 0.000257,
      "loss": 1.883,
      "step": 12880
    },
    {
      "epoch": 2.4320754716981132,
      "grad_norm": 0.7099146246910095,
      "learning_rate": 0.00025681132075471697,
      "loss": 1.8417,
      "step": 12890
    },
    {
      "epoch": 2.4339622641509435,
      "grad_norm": 0.7806511521339417,
      "learning_rate": 0.000256622641509434,
      "loss": 1.9439,
      "step": 12900
    },
    {
      "epoch": 2.4358490566037734,
      "grad_norm": 0.7644333243370056,
      "learning_rate": 0.00025643396226415093,
      "loss": 1.815,
      "step": 12910
    },
    {
      "epoch": 2.4377358490566037,
      "grad_norm": 0.7455388307571411,
      "learning_rate": 0.0002562452830188679,
      "loss": 1.8734,
      "step": 12920
    },
    {
      "epoch": 2.439622641509434,
      "grad_norm": 0.7515101432800293,
      "learning_rate": 0.0002560566037735849,
      "loss": 1.9216,
      "step": 12930
    },
    {
      "epoch": 2.4415094339622643,
      "grad_norm": 0.7281655669212341,
      "learning_rate": 0.00025586792452830185,
      "loss": 1.8895,
      "step": 12940
    },
    {
      "epoch": 2.4433962264150946,
      "grad_norm": 0.7602782845497131,
      "learning_rate": 0.0002556792452830189,
      "loss": 1.819,
      "step": 12950
    },
    {
      "epoch": 2.4452830188679244,
      "grad_norm": 0.7236889600753784,
      "learning_rate": 0.00025549056603773587,
      "loss": 1.9078,
      "step": 12960
    },
    {
      "epoch": 2.4471698113207547,
      "grad_norm": 0.7874224185943604,
      "learning_rate": 0.0002553018867924528,
      "loss": 1.9427,
      "step": 12970
    },
    {
      "epoch": 2.449056603773585,
      "grad_norm": 0.8558986783027649,
      "learning_rate": 0.00025511320754716983,
      "loss": 1.9016,
      "step": 12980
    },
    {
      "epoch": 2.450943396226415,
      "grad_norm": 0.7685670852661133,
      "learning_rate": 0.0002549245283018868,
      "loss": 1.8509,
      "step": 12990
    },
    {
      "epoch": 2.452830188679245,
      "grad_norm": 0.7528431415557861,
      "learning_rate": 0.0002547358490566038,
      "loss": 1.9044,
      "step": 13000
    },
    {
      "epoch": 2.4547169811320755,
      "grad_norm": 0.8228341937065125,
      "learning_rate": 0.00025454716981132075,
      "loss": 1.9341,
      "step": 13010
    },
    {
      "epoch": 2.456603773584906,
      "grad_norm": 0.7337040901184082,
      "learning_rate": 0.0002543584905660377,
      "loss": 1.8157,
      "step": 13020
    },
    {
      "epoch": 2.458490566037736,
      "grad_norm": 0.768000602722168,
      "learning_rate": 0.0002541698113207547,
      "loss": 1.8279,
      "step": 13030
    },
    {
      "epoch": 2.460377358490566,
      "grad_norm": 0.8755502700805664,
      "learning_rate": 0.0002539811320754717,
      "loss": 1.8773,
      "step": 13040
    },
    {
      "epoch": 2.4622641509433962,
      "grad_norm": 0.7645186185836792,
      "learning_rate": 0.0002537924528301887,
      "loss": 1.8294,
      "step": 13050
    },
    {
      "epoch": 2.4641509433962265,
      "grad_norm": 0.8643773794174194,
      "learning_rate": 0.00025360377358490564,
      "loss": 1.882,
      "step": 13060
    },
    {
      "epoch": 2.4660377358490564,
      "grad_norm": 0.832785427570343,
      "learning_rate": 0.00025341509433962265,
      "loss": 1.9178,
      "step": 13070
    },
    {
      "epoch": 2.4679245283018867,
      "grad_norm": 0.8057634830474854,
      "learning_rate": 0.00025322641509433966,
      "loss": 1.9197,
      "step": 13080
    },
    {
      "epoch": 2.469811320754717,
      "grad_norm": 0.7159252166748047,
      "learning_rate": 0.0002530377358490566,
      "loss": 1.9027,
      "step": 13090
    },
    {
      "epoch": 2.4716981132075473,
      "grad_norm": 0.7953423857688904,
      "learning_rate": 0.0002528490566037736,
      "loss": 1.8545,
      "step": 13100
    },
    {
      "epoch": 2.4735849056603776,
      "grad_norm": 0.8506676554679871,
      "learning_rate": 0.0002526603773584906,
      "loss": 1.9261,
      "step": 13110
    },
    {
      "epoch": 2.4754716981132074,
      "grad_norm": 0.7806293964385986,
      "learning_rate": 0.00025247169811320753,
      "loss": 1.8402,
      "step": 13120
    },
    {
      "epoch": 2.4773584905660377,
      "grad_norm": 0.7893391847610474,
      "learning_rate": 0.00025228301886792454,
      "loss": 1.7905,
      "step": 13130
    },
    {
      "epoch": 2.479245283018868,
      "grad_norm": 0.8439154028892517,
      "learning_rate": 0.0002520943396226415,
      "loss": 1.905,
      "step": 13140
    },
    {
      "epoch": 2.481132075471698,
      "grad_norm": 0.7710583806037903,
      "learning_rate": 0.0002519056603773585,
      "loss": 1.8062,
      "step": 13150
    },
    {
      "epoch": 2.483018867924528,
      "grad_norm": 0.8095448613166809,
      "learning_rate": 0.00025171698113207546,
      "loss": 1.8898,
      "step": 13160
    },
    {
      "epoch": 2.4849056603773585,
      "grad_norm": 0.8840690851211548,
      "learning_rate": 0.0002515283018867924,
      "loss": 1.8557,
      "step": 13170
    },
    {
      "epoch": 2.486792452830189,
      "grad_norm": 0.7982583045959473,
      "learning_rate": 0.0002513396226415094,
      "loss": 1.8455,
      "step": 13180
    },
    {
      "epoch": 2.488679245283019,
      "grad_norm": 0.7908482551574707,
      "learning_rate": 0.0002511509433962264,
      "loss": 1.9232,
      "step": 13190
    },
    {
      "epoch": 2.490566037735849,
      "grad_norm": 0.7719239592552185,
      "learning_rate": 0.00025096226415094344,
      "loss": 1.9184,
      "step": 13200
    },
    {
      "epoch": 2.4924528301886792,
      "grad_norm": 0.7957084774971008,
      "learning_rate": 0.0002507735849056604,
      "loss": 1.9166,
      "step": 13210
    },
    {
      "epoch": 2.4943396226415095,
      "grad_norm": 0.8172612190246582,
      "learning_rate": 0.00025058490566037735,
      "loss": 1.852,
      "step": 13220
    },
    {
      "epoch": 2.4962264150943394,
      "grad_norm": 0.7806146740913391,
      "learning_rate": 0.00025039622641509436,
      "loss": 1.8348,
      "step": 13230
    },
    {
      "epoch": 2.4981132075471697,
      "grad_norm": 0.9076222777366638,
      "learning_rate": 0.0002502075471698113,
      "loss": 1.8654,
      "step": 13240
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.8570784330368042,
      "learning_rate": 0.0002500188679245283,
      "loss": 1.927,
      "step": 13250
    },
    {
      "epoch": 2.5018867924528303,
      "grad_norm": 0.7340179085731506,
      "learning_rate": 0.0002498301886792453,
      "loss": 1.9182,
      "step": 13260
    },
    {
      "epoch": 2.5037735849056606,
      "grad_norm": 0.7699559330940247,
      "learning_rate": 0.0002496415094339623,
      "loss": 1.7823,
      "step": 13270
    },
    {
      "epoch": 2.5056603773584905,
      "grad_norm": 0.8618305325508118,
      "learning_rate": 0.00024945283018867925,
      "loss": 1.8332,
      "step": 13280
    },
    {
      "epoch": 2.5075471698113208,
      "grad_norm": 0.8554996252059937,
      "learning_rate": 0.00024926415094339625,
      "loss": 1.9158,
      "step": 13290
    },
    {
      "epoch": 2.509433962264151,
      "grad_norm": 0.6639350652694702,
      "learning_rate": 0.0002490754716981132,
      "loss": 1.8557,
      "step": 13300
    },
    {
      "epoch": 2.511320754716981,
      "grad_norm": 0.7348451018333435,
      "learning_rate": 0.00024888679245283016,
      "loss": 1.9645,
      "step": 13310
    },
    {
      "epoch": 2.513207547169811,
      "grad_norm": 0.813552737236023,
      "learning_rate": 0.0002486981132075472,
      "loss": 1.8383,
      "step": 13320
    },
    {
      "epoch": 2.5150943396226415,
      "grad_norm": 0.8383359909057617,
      "learning_rate": 0.0002485094339622642,
      "loss": 1.8712,
      "step": 13330
    },
    {
      "epoch": 2.516981132075472,
      "grad_norm": 0.7323715686798096,
      "learning_rate": 0.00024832075471698114,
      "loss": 1.8744,
      "step": 13340
    },
    {
      "epoch": 2.518867924528302,
      "grad_norm": 0.7416187524795532,
      "learning_rate": 0.0002481320754716981,
      "loss": 1.7941,
      "step": 13350
    },
    {
      "epoch": 2.520754716981132,
      "grad_norm": 0.8280463814735413,
      "learning_rate": 0.0002479433962264151,
      "loss": 1.8758,
      "step": 13360
    },
    {
      "epoch": 2.5226415094339623,
      "grad_norm": 0.8523865938186646,
      "learning_rate": 0.00024775471698113206,
      "loss": 1.8953,
      "step": 13370
    },
    {
      "epoch": 2.5245283018867926,
      "grad_norm": 0.8902333378791809,
      "learning_rate": 0.00024756603773584907,
      "loss": 1.8966,
      "step": 13380
    },
    {
      "epoch": 2.5264150943396224,
      "grad_norm": 0.7771150469779968,
      "learning_rate": 0.000247377358490566,
      "loss": 1.9504,
      "step": 13390
    },
    {
      "epoch": 2.5283018867924527,
      "grad_norm": 0.7740883231163025,
      "learning_rate": 0.00024718867924528303,
      "loss": 1.8704,
      "step": 13400
    },
    {
      "epoch": 2.530188679245283,
      "grad_norm": 0.751152753829956,
      "learning_rate": 0.000247,
      "loss": 1.9281,
      "step": 13410
    },
    {
      "epoch": 2.5320754716981133,
      "grad_norm": 0.7617432475090027,
      "learning_rate": 0.000246811320754717,
      "loss": 1.8236,
      "step": 13420
    },
    {
      "epoch": 2.5339622641509436,
      "grad_norm": 0.8830051422119141,
      "learning_rate": 0.00024662264150943395,
      "loss": 1.8339,
      "step": 13430
    },
    {
      "epoch": 2.5358490566037735,
      "grad_norm": 0.8332361578941345,
      "learning_rate": 0.00024643396226415096,
      "loss": 1.8219,
      "step": 13440
    },
    {
      "epoch": 2.5377358490566038,
      "grad_norm": 0.7706746459007263,
      "learning_rate": 0.0002462452830188679,
      "loss": 1.876,
      "step": 13450
    },
    {
      "epoch": 2.539622641509434,
      "grad_norm": 0.7756526470184326,
      "learning_rate": 0.0002460566037735849,
      "loss": 1.9496,
      "step": 13460
    },
    {
      "epoch": 2.541509433962264,
      "grad_norm": 0.8714817762374878,
      "learning_rate": 0.0002458679245283019,
      "loss": 1.9662,
      "step": 13470
    },
    {
      "epoch": 2.543396226415094,
      "grad_norm": 0.823857843875885,
      "learning_rate": 0.0002456792452830189,
      "loss": 1.796,
      "step": 13480
    },
    {
      "epoch": 2.5452830188679245,
      "grad_norm": 0.750881016254425,
      "learning_rate": 0.00024549056603773584,
      "loss": 1.8516,
      "step": 13490
    },
    {
      "epoch": 2.547169811320755,
      "grad_norm": 0.8101645708084106,
      "learning_rate": 0.00024530188679245285,
      "loss": 1.931,
      "step": 13500
    },
    {
      "epoch": 2.549056603773585,
      "grad_norm": 0.7723513841629028,
      "learning_rate": 0.0002451132075471698,
      "loss": 1.8458,
      "step": 13510
    },
    {
      "epoch": 2.550943396226415,
      "grad_norm": 0.9548459053039551,
      "learning_rate": 0.0002449245283018868,
      "loss": 1.9213,
      "step": 13520
    },
    {
      "epoch": 2.5528301886792453,
      "grad_norm": 0.7372543811798096,
      "learning_rate": 0.00024473584905660377,
      "loss": 1.9068,
      "step": 13530
    },
    {
      "epoch": 2.5547169811320756,
      "grad_norm": 0.7906842827796936,
      "learning_rate": 0.00024454716981132073,
      "loss": 1.8737,
      "step": 13540
    },
    {
      "epoch": 2.5566037735849054,
      "grad_norm": 0.8353627920150757,
      "learning_rate": 0.00024435849056603774,
      "loss": 1.8419,
      "step": 13550
    },
    {
      "epoch": 2.5584905660377357,
      "grad_norm": 0.7681281566619873,
      "learning_rate": 0.00024416981132075475,
      "loss": 1.8535,
      "step": 13560
    },
    {
      "epoch": 2.560377358490566,
      "grad_norm": 0.7044603228569031,
      "learning_rate": 0.0002439811320754717,
      "loss": 1.869,
      "step": 13570
    },
    {
      "epoch": 2.5622641509433963,
      "grad_norm": 0.8352383971214294,
      "learning_rate": 0.00024379245283018868,
      "loss": 1.8968,
      "step": 13580
    },
    {
      "epoch": 2.5641509433962266,
      "grad_norm": 0.7369757890701294,
      "learning_rate": 0.00024360377358490567,
      "loss": 1.8598,
      "step": 13590
    },
    {
      "epoch": 2.5660377358490565,
      "grad_norm": 0.826389491558075,
      "learning_rate": 0.00024341509433962265,
      "loss": 1.8499,
      "step": 13600
    },
    {
      "epoch": 2.5679245283018868,
      "grad_norm": 0.6843906044960022,
      "learning_rate": 0.00024322641509433963,
      "loss": 1.7829,
      "step": 13610
    },
    {
      "epoch": 2.569811320754717,
      "grad_norm": 0.7311634421348572,
      "learning_rate": 0.0002430377358490566,
      "loss": 1.8315,
      "step": 13620
    },
    {
      "epoch": 2.571698113207547,
      "grad_norm": 0.791492760181427,
      "learning_rate": 0.0002428490566037736,
      "loss": 1.7988,
      "step": 13630
    },
    {
      "epoch": 2.5735849056603772,
      "grad_norm": 0.8647737503051758,
      "learning_rate": 0.00024266037735849055,
      "loss": 1.8799,
      "step": 13640
    },
    {
      "epoch": 2.5754716981132075,
      "grad_norm": 0.7880516648292542,
      "learning_rate": 0.00024247169811320753,
      "loss": 1.7827,
      "step": 13650
    },
    {
      "epoch": 2.577358490566038,
      "grad_norm": 0.78509521484375,
      "learning_rate": 0.00024228301886792454,
      "loss": 1.858,
      "step": 13660
    },
    {
      "epoch": 2.579245283018868,
      "grad_norm": 0.7775330543518066,
      "learning_rate": 0.00024209433962264152,
      "loss": 1.8484,
      "step": 13670
    },
    {
      "epoch": 2.581132075471698,
      "grad_norm": 0.8130406141281128,
      "learning_rate": 0.0002419056603773585,
      "loss": 1.8374,
      "step": 13680
    },
    {
      "epoch": 2.5830188679245283,
      "grad_norm": 0.7215401530265808,
      "learning_rate": 0.00024171698113207546,
      "loss": 1.9062,
      "step": 13690
    },
    {
      "epoch": 2.5849056603773586,
      "grad_norm": 0.813625693321228,
      "learning_rate": 0.00024152830188679244,
      "loss": 1.8296,
      "step": 13700
    },
    {
      "epoch": 2.5867924528301884,
      "grad_norm": 0.8521751761436462,
      "learning_rate": 0.00024133962264150942,
      "loss": 1.9639,
      "step": 13710
    },
    {
      "epoch": 2.5886792452830187,
      "grad_norm": 0.7138717770576477,
      "learning_rate": 0.00024115094339622643,
      "loss": 1.8729,
      "step": 13720
    },
    {
      "epoch": 2.590566037735849,
      "grad_norm": 0.7444691061973572,
      "learning_rate": 0.00024096226415094342,
      "loss": 1.8647,
      "step": 13730
    },
    {
      "epoch": 2.5924528301886793,
      "grad_norm": 0.7423484921455383,
      "learning_rate": 0.00024077358490566037,
      "loss": 1.9388,
      "step": 13740
    },
    {
      "epoch": 2.5943396226415096,
      "grad_norm": 0.8375096917152405,
      "learning_rate": 0.00024058490566037735,
      "loss": 1.8666,
      "step": 13750
    },
    {
      "epoch": 2.5962264150943395,
      "grad_norm": 0.833737313747406,
      "learning_rate": 0.00024039622641509434,
      "loss": 1.8109,
      "step": 13760
    },
    {
      "epoch": 2.59811320754717,
      "grad_norm": 0.8063191771507263,
      "learning_rate": 0.00024020754716981132,
      "loss": 1.893,
      "step": 13770
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.7344600558280945,
      "learning_rate": 0.00024001886792452833,
      "loss": 1.9192,
      "step": 13780
    },
    {
      "epoch": 2.60188679245283,
      "grad_norm": 0.6793774366378784,
      "learning_rate": 0.00023983018867924528,
      "loss": 1.8732,
      "step": 13790
    },
    {
      "epoch": 2.6037735849056602,
      "grad_norm": 0.8179276585578918,
      "learning_rate": 0.00023964150943396226,
      "loss": 1.8634,
      "step": 13800
    },
    {
      "epoch": 2.6056603773584905,
      "grad_norm": 0.8677635192871094,
      "learning_rate": 0.00023945283018867925,
      "loss": 1.875,
      "step": 13810
    },
    {
      "epoch": 2.607547169811321,
      "grad_norm": 0.9064232707023621,
      "learning_rate": 0.00023926415094339623,
      "loss": 1.8646,
      "step": 13820
    },
    {
      "epoch": 2.609433962264151,
      "grad_norm": 0.8022164702415466,
      "learning_rate": 0.0002390754716981132,
      "loss": 1.837,
      "step": 13830
    },
    {
      "epoch": 2.611320754716981,
      "grad_norm": 0.7401173710823059,
      "learning_rate": 0.0002388867924528302,
      "loss": 1.7739,
      "step": 13840
    },
    {
      "epoch": 2.6132075471698113,
      "grad_norm": 0.8157917857170105,
      "learning_rate": 0.00023869811320754717,
      "loss": 1.9502,
      "step": 13850
    },
    {
      "epoch": 2.6150943396226416,
      "grad_norm": 0.7306062579154968,
      "learning_rate": 0.00023850943396226416,
      "loss": 1.9053,
      "step": 13860
    },
    {
      "epoch": 2.6169811320754715,
      "grad_norm": 0.8127171993255615,
      "learning_rate": 0.00023832075471698114,
      "loss": 1.8824,
      "step": 13870
    },
    {
      "epoch": 2.6188679245283017,
      "grad_norm": 0.8278481960296631,
      "learning_rate": 0.00023813207547169812,
      "loss": 1.8741,
      "step": 13880
    },
    {
      "epoch": 2.620754716981132,
      "grad_norm": 0.8226322531700134,
      "learning_rate": 0.00023794339622641508,
      "loss": 1.8302,
      "step": 13890
    },
    {
      "epoch": 2.6226415094339623,
      "grad_norm": 0.8277758359909058,
      "learning_rate": 0.00023775471698113209,
      "loss": 1.8762,
      "step": 13900
    },
    {
      "epoch": 2.6245283018867926,
      "grad_norm": 0.8575352430343628,
      "learning_rate": 0.00023756603773584907,
      "loss": 1.9139,
      "step": 13910
    },
    {
      "epoch": 2.6264150943396225,
      "grad_norm": 0.8310786485671997,
      "learning_rate": 0.00023737735849056605,
      "loss": 1.8735,
      "step": 13920
    },
    {
      "epoch": 2.628301886792453,
      "grad_norm": 0.8242114782333374,
      "learning_rate": 0.00023718867924528303,
      "loss": 1.8861,
      "step": 13930
    },
    {
      "epoch": 2.630188679245283,
      "grad_norm": 0.7867717742919922,
      "learning_rate": 0.000237,
      "loss": 1.8552,
      "step": 13940
    },
    {
      "epoch": 2.632075471698113,
      "grad_norm": 0.7779049277305603,
      "learning_rate": 0.00023681132075471697,
      "loss": 1.8366,
      "step": 13950
    },
    {
      "epoch": 2.6339622641509433,
      "grad_norm": 0.7872679829597473,
      "learning_rate": 0.00023662264150943398,
      "loss": 1.8888,
      "step": 13960
    },
    {
      "epoch": 2.6358490566037736,
      "grad_norm": 0.7494384050369263,
      "learning_rate": 0.00023643396226415096,
      "loss": 1.7449,
      "step": 13970
    },
    {
      "epoch": 2.637735849056604,
      "grad_norm": 0.8284075260162354,
      "learning_rate": 0.00023624528301886794,
      "loss": 1.8126,
      "step": 13980
    },
    {
      "epoch": 2.639622641509434,
      "grad_norm": 0.7985209822654724,
      "learning_rate": 0.0002360566037735849,
      "loss": 1.7803,
      "step": 13990
    },
    {
      "epoch": 2.641509433962264,
      "grad_norm": 0.6964560747146606,
      "learning_rate": 0.00023586792452830188,
      "loss": 1.8137,
      "step": 14000
    },
    {
      "epoch": 2.6433962264150943,
      "grad_norm": 0.7902607917785645,
      "learning_rate": 0.00023567924528301886,
      "loss": 1.8327,
      "step": 14010
    },
    {
      "epoch": 2.6452830188679246,
      "grad_norm": 0.8720148205757141,
      "learning_rate": 0.00023549056603773587,
      "loss": 1.8632,
      "step": 14020
    },
    {
      "epoch": 2.6471698113207545,
      "grad_norm": 0.7989489436149597,
      "learning_rate": 0.00023530188679245285,
      "loss": 1.8608,
      "step": 14030
    },
    {
      "epoch": 2.6490566037735848,
      "grad_norm": 0.850562334060669,
      "learning_rate": 0.0002351132075471698,
      "loss": 1.925,
      "step": 14040
    },
    {
      "epoch": 2.650943396226415,
      "grad_norm": 0.701464831829071,
      "learning_rate": 0.0002349245283018868,
      "loss": 1.9121,
      "step": 14050
    },
    {
      "epoch": 2.6528301886792454,
      "grad_norm": 0.7809286713600159,
      "learning_rate": 0.00023473584905660377,
      "loss": 1.8448,
      "step": 14060
    },
    {
      "epoch": 2.6547169811320757,
      "grad_norm": 0.9149122834205627,
      "learning_rate": 0.00023454716981132076,
      "loss": 1.879,
      "step": 14070
    },
    {
      "epoch": 2.6566037735849055,
      "grad_norm": 0.7884365916252136,
      "learning_rate": 0.00023435849056603776,
      "loss": 1.9037,
      "step": 14080
    },
    {
      "epoch": 2.658490566037736,
      "grad_norm": 0.7857202291488647,
      "learning_rate": 0.00023416981132075472,
      "loss": 1.8854,
      "step": 14090
    },
    {
      "epoch": 2.660377358490566,
      "grad_norm": 0.7873293161392212,
      "learning_rate": 0.0002339811320754717,
      "loss": 1.8319,
      "step": 14100
    },
    {
      "epoch": 2.662264150943396,
      "grad_norm": 0.7432778477668762,
      "learning_rate": 0.00023379245283018868,
      "loss": 1.9329,
      "step": 14110
    },
    {
      "epoch": 2.6641509433962263,
      "grad_norm": 0.765734076499939,
      "learning_rate": 0.00023360377358490567,
      "loss": 1.9333,
      "step": 14120
    },
    {
      "epoch": 2.6660377358490566,
      "grad_norm": 0.8493621945381165,
      "learning_rate": 0.00023341509433962265,
      "loss": 1.8194,
      "step": 14130
    },
    {
      "epoch": 2.667924528301887,
      "grad_norm": 0.7944188714027405,
      "learning_rate": 0.0002332264150943396,
      "loss": 1.8073,
      "step": 14140
    },
    {
      "epoch": 2.669811320754717,
      "grad_norm": 0.7745795845985413,
      "learning_rate": 0.0002330377358490566,
      "loss": 1.8454,
      "step": 14150
    },
    {
      "epoch": 2.671698113207547,
      "grad_norm": 0.798709511756897,
      "learning_rate": 0.0002328490566037736,
      "loss": 1.8499,
      "step": 14160
    },
    {
      "epoch": 2.6735849056603773,
      "grad_norm": 0.7526935338973999,
      "learning_rate": 0.00023266037735849058,
      "loss": 1.8602,
      "step": 14170
    },
    {
      "epoch": 2.6754716981132076,
      "grad_norm": 0.7538561820983887,
      "learning_rate": 0.00023247169811320756,
      "loss": 1.8212,
      "step": 14180
    },
    {
      "epoch": 2.6773584905660375,
      "grad_norm": 0.733239471912384,
      "learning_rate": 0.00023228301886792451,
      "loss": 1.8358,
      "step": 14190
    },
    {
      "epoch": 2.6792452830188678,
      "grad_norm": 1.0033104419708252,
      "learning_rate": 0.0002320943396226415,
      "loss": 1.8843,
      "step": 14200
    },
    {
      "epoch": 2.681132075471698,
      "grad_norm": 0.778376579284668,
      "learning_rate": 0.0002319056603773585,
      "loss": 1.9161,
      "step": 14210
    },
    {
      "epoch": 2.6830188679245284,
      "grad_norm": 0.7222176790237427,
      "learning_rate": 0.0002317169811320755,
      "loss": 1.9019,
      "step": 14220
    },
    {
      "epoch": 2.6849056603773587,
      "grad_norm": 0.8303166031837463,
      "learning_rate": 0.00023152830188679247,
      "loss": 1.7975,
      "step": 14230
    },
    {
      "epoch": 2.6867924528301885,
      "grad_norm": 0.7251253724098206,
      "learning_rate": 0.00023133962264150943,
      "loss": 1.9116,
      "step": 14240
    },
    {
      "epoch": 2.688679245283019,
      "grad_norm": 0.7488741278648376,
      "learning_rate": 0.0002311509433962264,
      "loss": 1.9302,
      "step": 14250
    },
    {
      "epoch": 2.690566037735849,
      "grad_norm": 0.7237771153450012,
      "learning_rate": 0.0002309622641509434,
      "loss": 1.961,
      "step": 14260
    },
    {
      "epoch": 2.692452830188679,
      "grad_norm": 0.7757959365844727,
      "learning_rate": 0.0002307735849056604,
      "loss": 1.8568,
      "step": 14270
    },
    {
      "epoch": 2.6943396226415093,
      "grad_norm": 0.7980212569236755,
      "learning_rate": 0.00023058490566037738,
      "loss": 1.7657,
      "step": 14280
    },
    {
      "epoch": 2.6962264150943396,
      "grad_norm": 0.7947661876678467,
      "learning_rate": 0.00023039622641509434,
      "loss": 1.8949,
      "step": 14290
    },
    {
      "epoch": 2.69811320754717,
      "grad_norm": 0.809938907623291,
      "learning_rate": 0.00023020754716981132,
      "loss": 1.8201,
      "step": 14300
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.9277948141098022,
      "learning_rate": 0.0002300188679245283,
      "loss": 1.9227,
      "step": 14310
    },
    {
      "epoch": 2.70188679245283,
      "grad_norm": 0.7155025601387024,
      "learning_rate": 0.00022983018867924528,
      "loss": 1.8183,
      "step": 14320
    },
    {
      "epoch": 2.7037735849056603,
      "grad_norm": 0.800283670425415,
      "learning_rate": 0.0002296415094339623,
      "loss": 1.9088,
      "step": 14330
    },
    {
      "epoch": 2.7056603773584906,
      "grad_norm": 0.9299555420875549,
      "learning_rate": 0.00022945283018867925,
      "loss": 1.8924,
      "step": 14340
    },
    {
      "epoch": 2.7075471698113205,
      "grad_norm": 0.7115142345428467,
      "learning_rate": 0.00022926415094339623,
      "loss": 1.9446,
      "step": 14350
    },
    {
      "epoch": 2.709433962264151,
      "grad_norm": 0.7660768032073975,
      "learning_rate": 0.0002290754716981132,
      "loss": 1.8228,
      "step": 14360
    },
    {
      "epoch": 2.711320754716981,
      "grad_norm": 0.850036084651947,
      "learning_rate": 0.0002288867924528302,
      "loss": 1.8284,
      "step": 14370
    },
    {
      "epoch": 2.7132075471698114,
      "grad_norm": 0.8093549013137817,
      "learning_rate": 0.00022869811320754718,
      "loss": 1.8375,
      "step": 14380
    },
    {
      "epoch": 2.7150943396226417,
      "grad_norm": 0.7833985090255737,
      "learning_rate": 0.00022850943396226416,
      "loss": 1.9374,
      "step": 14390
    },
    {
      "epoch": 2.7169811320754715,
      "grad_norm": 0.8249929547309875,
      "learning_rate": 0.00022832075471698114,
      "loss": 1.8626,
      "step": 14400
    },
    {
      "epoch": 2.718867924528302,
      "grad_norm": 0.8425351977348328,
      "learning_rate": 0.00022813207547169812,
      "loss": 1.7911,
      "step": 14410
    },
    {
      "epoch": 2.720754716981132,
      "grad_norm": 0.7454749345779419,
      "learning_rate": 0.0002279433962264151,
      "loss": 1.9654,
      "step": 14420
    },
    {
      "epoch": 2.722641509433962,
      "grad_norm": 0.8217174410820007,
      "learning_rate": 0.0002277547169811321,
      "loss": 1.8658,
      "step": 14430
    },
    {
      "epoch": 2.7245283018867923,
      "grad_norm": 0.884867250919342,
      "learning_rate": 0.00022756603773584904,
      "loss": 1.9139,
      "step": 14440
    },
    {
      "epoch": 2.7264150943396226,
      "grad_norm": 0.7831339240074158,
      "learning_rate": 0.00022737735849056605,
      "loss": 1.913,
      "step": 14450
    },
    {
      "epoch": 2.728301886792453,
      "grad_norm": 0.7107868790626526,
      "learning_rate": 0.00022718867924528303,
      "loss": 1.7596,
      "step": 14460
    },
    {
      "epoch": 2.730188679245283,
      "grad_norm": 0.8228533864021301,
      "learning_rate": 0.00022700000000000002,
      "loss": 1.7785,
      "step": 14470
    },
    {
      "epoch": 2.732075471698113,
      "grad_norm": 0.8796197772026062,
      "learning_rate": 0.000226811320754717,
      "loss": 1.9661,
      "step": 14480
    },
    {
      "epoch": 2.7339622641509433,
      "grad_norm": 0.9498200416564941,
      "learning_rate": 0.00022662264150943395,
      "loss": 1.9519,
      "step": 14490
    },
    {
      "epoch": 2.7358490566037736,
      "grad_norm": 0.7536723613739014,
      "learning_rate": 0.00022643396226415093,
      "loss": 1.9459,
      "step": 14500
    },
    {
      "epoch": 2.7377358490566035,
      "grad_norm": 0.835480809211731,
      "learning_rate": 0.00022624528301886794,
      "loss": 1.8766,
      "step": 14510
    },
    {
      "epoch": 2.739622641509434,
      "grad_norm": 0.8619804978370667,
      "learning_rate": 0.00022605660377358493,
      "loss": 1.8036,
      "step": 14520
    },
    {
      "epoch": 2.741509433962264,
      "grad_norm": 0.7999184727668762,
      "learning_rate": 0.0002258679245283019,
      "loss": 1.8095,
      "step": 14530
    },
    {
      "epoch": 2.7433962264150944,
      "grad_norm": 0.7532323002815247,
      "learning_rate": 0.00022567924528301886,
      "loss": 1.8784,
      "step": 14540
    },
    {
      "epoch": 2.7452830188679247,
      "grad_norm": 0.8524046540260315,
      "learning_rate": 0.00022549056603773585,
      "loss": 1.8274,
      "step": 14550
    },
    {
      "epoch": 2.7471698113207546,
      "grad_norm": 0.7911373376846313,
      "learning_rate": 0.00022530188679245283,
      "loss": 1.8379,
      "step": 14560
    },
    {
      "epoch": 2.749056603773585,
      "grad_norm": 0.8023989796638489,
      "learning_rate": 0.00022511320754716984,
      "loss": 1.9608,
      "step": 14570
    },
    {
      "epoch": 2.750943396226415,
      "grad_norm": 0.7659674882888794,
      "learning_rate": 0.00022492452830188682,
      "loss": 1.9209,
      "step": 14580
    },
    {
      "epoch": 2.7528301886792454,
      "grad_norm": 0.7282704710960388,
      "learning_rate": 0.00022473584905660377,
      "loss": 1.8708,
      "step": 14590
    },
    {
      "epoch": 2.7547169811320753,
      "grad_norm": 0.7506849765777588,
      "learning_rate": 0.00022454716981132076,
      "loss": 1.86,
      "step": 14600
    },
    {
      "epoch": 2.7566037735849056,
      "grad_norm": 0.7727935910224915,
      "learning_rate": 0.00022435849056603774,
      "loss": 1.8581,
      "step": 14610
    },
    {
      "epoch": 2.758490566037736,
      "grad_norm": 0.813877522945404,
      "learning_rate": 0.00022416981132075472,
      "loss": 1.8171,
      "step": 14620
    },
    {
      "epoch": 2.760377358490566,
      "grad_norm": 0.8240907192230225,
      "learning_rate": 0.0002239811320754717,
      "loss": 1.9149,
      "step": 14630
    },
    {
      "epoch": 2.7622641509433965,
      "grad_norm": 0.7339969277381897,
      "learning_rate": 0.00022379245283018869,
      "loss": 1.8703,
      "step": 14640
    },
    {
      "epoch": 2.7641509433962264,
      "grad_norm": 0.8072651624679565,
      "learning_rate": 0.00022360377358490567,
      "loss": 1.869,
      "step": 14650
    },
    {
      "epoch": 2.7660377358490567,
      "grad_norm": 0.9110256433486938,
      "learning_rate": 0.00022341509433962265,
      "loss": 1.9076,
      "step": 14660
    },
    {
      "epoch": 2.767924528301887,
      "grad_norm": 0.9094961881637573,
      "learning_rate": 0.00022322641509433963,
      "loss": 1.8711,
      "step": 14670
    },
    {
      "epoch": 2.769811320754717,
      "grad_norm": 0.8888691067695618,
      "learning_rate": 0.00022303773584905661,
      "loss": 1.8565,
      "step": 14680
    },
    {
      "epoch": 2.771698113207547,
      "grad_norm": 0.7840433716773987,
      "learning_rate": 0.00022284905660377357,
      "loss": 1.9106,
      "step": 14690
    },
    {
      "epoch": 2.7735849056603774,
      "grad_norm": 0.7792736887931824,
      "learning_rate": 0.00022266037735849058,
      "loss": 1.9813,
      "step": 14700
    },
    {
      "epoch": 2.7754716981132077,
      "grad_norm": 0.8730819821357727,
      "learning_rate": 0.00022247169811320756,
      "loss": 1.8824,
      "step": 14710
    },
    {
      "epoch": 2.777358490566038,
      "grad_norm": 0.7981916666030884,
      "learning_rate": 0.00022228301886792454,
      "loss": 1.8665,
      "step": 14720
    },
    {
      "epoch": 2.779245283018868,
      "grad_norm": 0.810654878616333,
      "learning_rate": 0.0002220943396226415,
      "loss": 1.9007,
      "step": 14730
    },
    {
      "epoch": 2.781132075471698,
      "grad_norm": 0.7380334138870239,
      "learning_rate": 0.00022190566037735848,
      "loss": 1.9616,
      "step": 14740
    },
    {
      "epoch": 2.7830188679245285,
      "grad_norm": 0.7760314345359802,
      "learning_rate": 0.00022171698113207546,
      "loss": 1.892,
      "step": 14750
    },
    {
      "epoch": 2.7849056603773583,
      "grad_norm": 0.798617959022522,
      "learning_rate": 0.00022152830188679247,
      "loss": 1.8548,
      "step": 14760
    },
    {
      "epoch": 2.7867924528301886,
      "grad_norm": 0.7581434845924377,
      "learning_rate": 0.00022133962264150945,
      "loss": 1.9062,
      "step": 14770
    },
    {
      "epoch": 2.788679245283019,
      "grad_norm": 0.7326405048370361,
      "learning_rate": 0.0002211509433962264,
      "loss": 1.8236,
      "step": 14780
    },
    {
      "epoch": 2.790566037735849,
      "grad_norm": 0.8309805989265442,
      "learning_rate": 0.0002209622641509434,
      "loss": 1.932,
      "step": 14790
    },
    {
      "epoch": 2.7924528301886795,
      "grad_norm": 0.7088137865066528,
      "learning_rate": 0.00022077358490566037,
      "loss": 1.9112,
      "step": 14800
    },
    {
      "epoch": 2.7943396226415094,
      "grad_norm": 0.7776321768760681,
      "learning_rate": 0.00022058490566037735,
      "loss": 1.8382,
      "step": 14810
    },
    {
      "epoch": 2.7962264150943397,
      "grad_norm": 0.8159926533699036,
      "learning_rate": 0.00022039622641509436,
      "loss": 1.827,
      "step": 14820
    },
    {
      "epoch": 2.79811320754717,
      "grad_norm": 0.7096660137176514,
      "learning_rate": 0.00022020754716981132,
      "loss": 1.8609,
      "step": 14830
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.8153628706932068,
      "learning_rate": 0.0002200188679245283,
      "loss": 1.9218,
      "step": 14840
    },
    {
      "epoch": 2.80188679245283,
      "grad_norm": 0.7795896530151367,
      "learning_rate": 0.00021983018867924528,
      "loss": 1.8147,
      "step": 14850
    },
    {
      "epoch": 2.8037735849056604,
      "grad_norm": 0.7529227137565613,
      "learning_rate": 0.00021964150943396227,
      "loss": 1.8107,
      "step": 14860
    },
    {
      "epoch": 2.8056603773584907,
      "grad_norm": 0.6902428269386292,
      "learning_rate": 0.00021945283018867925,
      "loss": 1.9613,
      "step": 14870
    },
    {
      "epoch": 2.807547169811321,
      "grad_norm": 0.8164496421813965,
      "learning_rate": 0.00021926415094339623,
      "loss": 1.8602,
      "step": 14880
    },
    {
      "epoch": 2.809433962264151,
      "grad_norm": 1.016887903213501,
      "learning_rate": 0.0002190754716981132,
      "loss": 1.9285,
      "step": 14890
    },
    {
      "epoch": 2.811320754716981,
      "grad_norm": 0.8143917322158813,
      "learning_rate": 0.0002188867924528302,
      "loss": 1.9162,
      "step": 14900
    },
    {
      "epoch": 2.8132075471698115,
      "grad_norm": 0.741940975189209,
      "learning_rate": 0.00021869811320754718,
      "loss": 1.8173,
      "step": 14910
    },
    {
      "epoch": 2.8150943396226413,
      "grad_norm": 0.8983041644096375,
      "learning_rate": 0.00021850943396226416,
      "loss": 1.8273,
      "step": 14920
    },
    {
      "epoch": 2.8169811320754716,
      "grad_norm": 0.7244752645492554,
      "learning_rate": 0.00021832075471698111,
      "loss": 1.816,
      "step": 14930
    },
    {
      "epoch": 2.818867924528302,
      "grad_norm": 0.7969317436218262,
      "learning_rate": 0.00021813207547169812,
      "loss": 1.9311,
      "step": 14940
    },
    {
      "epoch": 2.8207547169811322,
      "grad_norm": 0.831924319267273,
      "learning_rate": 0.0002179433962264151,
      "loss": 1.8956,
      "step": 14950
    },
    {
      "epoch": 2.8226415094339625,
      "grad_norm": 0.9005240797996521,
      "learning_rate": 0.0002177547169811321,
      "loss": 1.87,
      "step": 14960
    },
    {
      "epoch": 2.8245283018867924,
      "grad_norm": 0.7820333242416382,
      "learning_rate": 0.00021756603773584907,
      "loss": 1.8019,
      "step": 14970
    },
    {
      "epoch": 2.8264150943396227,
      "grad_norm": 0.7733315229415894,
      "learning_rate": 0.00021737735849056602,
      "loss": 1.9095,
      "step": 14980
    },
    {
      "epoch": 2.828301886792453,
      "grad_norm": 0.8940817713737488,
      "learning_rate": 0.000217188679245283,
      "loss": 1.867,
      "step": 14990
    },
    {
      "epoch": 2.830188679245283,
      "grad_norm": 0.6774810552597046,
      "learning_rate": 0.00021700000000000002,
      "loss": 1.8736,
      "step": 15000
    },
    {
      "epoch": 2.832075471698113,
      "grad_norm": 0.729223906993866,
      "learning_rate": 0.000216811320754717,
      "loss": 1.9051,
      "step": 15010
    },
    {
      "epoch": 2.8339622641509434,
      "grad_norm": 0.7505308389663696,
      "learning_rate": 0.00021662264150943398,
      "loss": 1.8345,
      "step": 15020
    },
    {
      "epoch": 2.8358490566037737,
      "grad_norm": 0.8793871402740479,
      "learning_rate": 0.00021643396226415094,
      "loss": 1.7826,
      "step": 15030
    },
    {
      "epoch": 2.837735849056604,
      "grad_norm": 0.9666235446929932,
      "learning_rate": 0.00021624528301886792,
      "loss": 1.9429,
      "step": 15040
    },
    {
      "epoch": 2.839622641509434,
      "grad_norm": 0.8000511527061462,
      "learning_rate": 0.0002160566037735849,
      "loss": 1.8337,
      "step": 15050
    },
    {
      "epoch": 2.841509433962264,
      "grad_norm": 0.7383070588111877,
      "learning_rate": 0.00021586792452830188,
      "loss": 1.8486,
      "step": 15060
    },
    {
      "epoch": 2.8433962264150945,
      "grad_norm": 0.7467698454856873,
      "learning_rate": 0.0002156792452830189,
      "loss": 1.9403,
      "step": 15070
    },
    {
      "epoch": 2.8452830188679243,
      "grad_norm": 0.7435810565948486,
      "learning_rate": 0.00021549056603773585,
      "loss": 1.9303,
      "step": 15080
    },
    {
      "epoch": 2.8471698113207546,
      "grad_norm": 0.8969258666038513,
      "learning_rate": 0.00021530188679245283,
      "loss": 1.9187,
      "step": 15090
    },
    {
      "epoch": 2.849056603773585,
      "grad_norm": 0.7764090299606323,
      "learning_rate": 0.0002151132075471698,
      "loss": 1.9107,
      "step": 15100
    },
    {
      "epoch": 2.8509433962264152,
      "grad_norm": 0.7532317042350769,
      "learning_rate": 0.0002149245283018868,
      "loss": 1.8921,
      "step": 15110
    },
    {
      "epoch": 2.8528301886792455,
      "grad_norm": 0.7723332047462463,
      "learning_rate": 0.00021473584905660378,
      "loss": 1.818,
      "step": 15120
    },
    {
      "epoch": 2.8547169811320754,
      "grad_norm": 0.7219384908676147,
      "learning_rate": 0.00021454716981132076,
      "loss": 1.8559,
      "step": 15130
    },
    {
      "epoch": 2.8566037735849057,
      "grad_norm": 0.8515991568565369,
      "learning_rate": 0.00021435849056603774,
      "loss": 1.8288,
      "step": 15140
    },
    {
      "epoch": 2.858490566037736,
      "grad_norm": 0.8198641538619995,
      "learning_rate": 0.00021416981132075472,
      "loss": 1.9733,
      "step": 15150
    },
    {
      "epoch": 2.860377358490566,
      "grad_norm": 0.7668576836585999,
      "learning_rate": 0.0002139811320754717,
      "loss": 1.8508,
      "step": 15160
    },
    {
      "epoch": 2.862264150943396,
      "grad_norm": 0.7184456586837769,
      "learning_rate": 0.00021379245283018869,
      "loss": 1.8712,
      "step": 15170
    },
    {
      "epoch": 2.8641509433962264,
      "grad_norm": 0.8432120680809021,
      "learning_rate": 0.00021360377358490564,
      "loss": 1.9148,
      "step": 15180
    },
    {
      "epoch": 2.8660377358490567,
      "grad_norm": 0.8165632486343384,
      "learning_rate": 0.00021341509433962265,
      "loss": 1.931,
      "step": 15190
    },
    {
      "epoch": 2.867924528301887,
      "grad_norm": 0.8655936121940613,
      "learning_rate": 0.00021322641509433963,
      "loss": 1.8106,
      "step": 15200
    },
    {
      "epoch": 2.869811320754717,
      "grad_norm": 0.7068161964416504,
      "learning_rate": 0.00021303773584905661,
      "loss": 1.952,
      "step": 15210
    },
    {
      "epoch": 2.871698113207547,
      "grad_norm": 0.8695303201675415,
      "learning_rate": 0.0002128490566037736,
      "loss": 1.8576,
      "step": 15220
    },
    {
      "epoch": 2.8735849056603775,
      "grad_norm": 0.8649277687072754,
      "learning_rate": 0.00021266037735849055,
      "loss": 1.8818,
      "step": 15230
    },
    {
      "epoch": 2.8754716981132074,
      "grad_norm": 0.8219701647758484,
      "learning_rate": 0.00021247169811320753,
      "loss": 1.9166,
      "step": 15240
    },
    {
      "epoch": 2.8773584905660377,
      "grad_norm": 0.7444087266921997,
      "learning_rate": 0.00021228301886792454,
      "loss": 1.8518,
      "step": 15250
    },
    {
      "epoch": 2.879245283018868,
      "grad_norm": 0.8143804669380188,
      "learning_rate": 0.00021209433962264153,
      "loss": 1.9228,
      "step": 15260
    },
    {
      "epoch": 2.8811320754716983,
      "grad_norm": 0.8939698338508606,
      "learning_rate": 0.0002119056603773585,
      "loss": 1.8634,
      "step": 15270
    },
    {
      "epoch": 2.8830188679245285,
      "grad_norm": 0.7895088195800781,
      "learning_rate": 0.00021171698113207546,
      "loss": 1.8502,
      "step": 15280
    },
    {
      "epoch": 2.8849056603773584,
      "grad_norm": 0.8315234184265137,
      "learning_rate": 0.00021152830188679244,
      "loss": 1.9217,
      "step": 15290
    },
    {
      "epoch": 2.8867924528301887,
      "grad_norm": 0.7169894576072693,
      "learning_rate": 0.00021133962264150943,
      "loss": 1.8418,
      "step": 15300
    },
    {
      "epoch": 2.888679245283019,
      "grad_norm": 0.7695136070251465,
      "learning_rate": 0.00021115094339622644,
      "loss": 1.8548,
      "step": 15310
    },
    {
      "epoch": 2.890566037735849,
      "grad_norm": 1.0062260627746582,
      "learning_rate": 0.00021096226415094342,
      "loss": 1.8577,
      "step": 15320
    },
    {
      "epoch": 2.892452830188679,
      "grad_norm": 0.6678837537765503,
      "learning_rate": 0.00021077358490566037,
      "loss": 1.8883,
      "step": 15330
    },
    {
      "epoch": 2.8943396226415095,
      "grad_norm": 0.7897670865058899,
      "learning_rate": 0.00021058490566037736,
      "loss": 1.8938,
      "step": 15340
    },
    {
      "epoch": 2.8962264150943398,
      "grad_norm": 0.7737598419189453,
      "learning_rate": 0.00021039622641509434,
      "loss": 1.8185,
      "step": 15350
    },
    {
      "epoch": 2.89811320754717,
      "grad_norm": 0.837871253490448,
      "learning_rate": 0.00021020754716981132,
      "loss": 1.836,
      "step": 15360
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.7730334997177124,
      "learning_rate": 0.00021001886792452833,
      "loss": 1.9125,
      "step": 15370
    },
    {
      "epoch": 2.90188679245283,
      "grad_norm": 0.765897274017334,
      "learning_rate": 0.00020983018867924528,
      "loss": 1.8828,
      "step": 15380
    },
    {
      "epoch": 2.9037735849056605,
      "grad_norm": 0.8097134828567505,
      "learning_rate": 0.00020964150943396227,
      "loss": 1.7956,
      "step": 15390
    },
    {
      "epoch": 2.9056603773584904,
      "grad_norm": 0.7584580779075623,
      "learning_rate": 0.00020945283018867925,
      "loss": 1.8435,
      "step": 15400
    },
    {
      "epoch": 2.9075471698113207,
      "grad_norm": 0.9314226508140564,
      "learning_rate": 0.00020926415094339623,
      "loss": 1.8152,
      "step": 15410
    },
    {
      "epoch": 2.909433962264151,
      "grad_norm": 0.791854202747345,
      "learning_rate": 0.0002090754716981132,
      "loss": 1.8203,
      "step": 15420
    },
    {
      "epoch": 2.9113207547169813,
      "grad_norm": 0.7315292358398438,
      "learning_rate": 0.0002088867924528302,
      "loss": 1.8865,
      "step": 15430
    },
    {
      "epoch": 2.9132075471698116,
      "grad_norm": 0.8325687050819397,
      "learning_rate": 0.00020869811320754718,
      "loss": 1.9191,
      "step": 15440
    },
    {
      "epoch": 2.9150943396226414,
      "grad_norm": 0.7745741605758667,
      "learning_rate": 0.00020850943396226416,
      "loss": 1.8345,
      "step": 15450
    },
    {
      "epoch": 2.9169811320754717,
      "grad_norm": 0.7831622362136841,
      "learning_rate": 0.00020832075471698114,
      "loss": 1.8241,
      "step": 15460
    },
    {
      "epoch": 2.918867924528302,
      "grad_norm": 0.7480807900428772,
      "learning_rate": 0.00020813207547169812,
      "loss": 1.8262,
      "step": 15470
    },
    {
      "epoch": 2.920754716981132,
      "grad_norm": 0.756581723690033,
      "learning_rate": 0.00020794339622641508,
      "loss": 1.8979,
      "step": 15480
    },
    {
      "epoch": 2.922641509433962,
      "grad_norm": 0.6969853043556213,
      "learning_rate": 0.00020775471698113206,
      "loss": 1.8026,
      "step": 15490
    },
    {
      "epoch": 2.9245283018867925,
      "grad_norm": 0.7114677429199219,
      "learning_rate": 0.00020756603773584907,
      "loss": 1.8919,
      "step": 15500
    },
    {
      "epoch": 2.9264150943396228,
      "grad_norm": 0.7819491624832153,
      "learning_rate": 0.00020737735849056605,
      "loss": 1.9745,
      "step": 15510
    },
    {
      "epoch": 2.928301886792453,
      "grad_norm": 0.7347652316093445,
      "learning_rate": 0.00020718867924528303,
      "loss": 1.8893,
      "step": 15520
    },
    {
      "epoch": 2.930188679245283,
      "grad_norm": 0.8321644067764282,
      "learning_rate": 0.000207,
      "loss": 1.8988,
      "step": 15530
    },
    {
      "epoch": 2.9320754716981132,
      "grad_norm": 0.7555699348449707,
      "learning_rate": 0.00020681132075471697,
      "loss": 1.8418,
      "step": 15540
    },
    {
      "epoch": 2.9339622641509435,
      "grad_norm": 0.8284746408462524,
      "learning_rate": 0.00020662264150943395,
      "loss": 1.8346,
      "step": 15550
    },
    {
      "epoch": 2.9358490566037734,
      "grad_norm": 0.7448293566703796,
      "learning_rate": 0.00020643396226415096,
      "loss": 1.9537,
      "step": 15560
    },
    {
      "epoch": 2.9377358490566037,
      "grad_norm": 0.7666093111038208,
      "learning_rate": 0.00020624528301886795,
      "loss": 1.9973,
      "step": 15570
    },
    {
      "epoch": 2.939622641509434,
      "grad_norm": 0.7655808329582214,
      "learning_rate": 0.0002060566037735849,
      "loss": 1.8656,
      "step": 15580
    },
    {
      "epoch": 2.9415094339622643,
      "grad_norm": 0.7387259602546692,
      "learning_rate": 0.00020586792452830188,
      "loss": 1.8751,
      "step": 15590
    },
    {
      "epoch": 2.9433962264150946,
      "grad_norm": 0.7330650687217712,
      "learning_rate": 0.00020567924528301887,
      "loss": 1.9945,
      "step": 15600
    },
    {
      "epoch": 2.9452830188679244,
      "grad_norm": 0.8238416910171509,
      "learning_rate": 0.00020549056603773585,
      "loss": 1.8972,
      "step": 15610
    },
    {
      "epoch": 2.9471698113207547,
      "grad_norm": 0.7470048069953918,
      "learning_rate": 0.00020530188679245286,
      "loss": 1.8725,
      "step": 15620
    },
    {
      "epoch": 2.949056603773585,
      "grad_norm": 0.7956879138946533,
      "learning_rate": 0.0002051132075471698,
      "loss": 1.7739,
      "step": 15630
    },
    {
      "epoch": 2.950943396226415,
      "grad_norm": 0.8365610837936401,
      "learning_rate": 0.0002049245283018868,
      "loss": 1.9213,
      "step": 15640
    },
    {
      "epoch": 2.952830188679245,
      "grad_norm": 0.7909114956855774,
      "learning_rate": 0.00020473584905660378,
      "loss": 1.8727,
      "step": 15650
    },
    {
      "epoch": 2.9547169811320755,
      "grad_norm": 0.8670193552970886,
      "learning_rate": 0.00020454716981132076,
      "loss": 1.8789,
      "step": 15660
    },
    {
      "epoch": 2.956603773584906,
      "grad_norm": 0.7851898074150085,
      "learning_rate": 0.00020435849056603774,
      "loss": 1.8724,
      "step": 15670
    },
    {
      "epoch": 2.958490566037736,
      "grad_norm": 0.8873263001441956,
      "learning_rate": 0.00020416981132075472,
      "loss": 1.88,
      "step": 15680
    },
    {
      "epoch": 2.960377358490566,
      "grad_norm": 0.8328377604484558,
      "learning_rate": 0.0002039811320754717,
      "loss": 1.9178,
      "step": 15690
    },
    {
      "epoch": 2.9622641509433962,
      "grad_norm": 0.8041674494743347,
      "learning_rate": 0.0002037924528301887,
      "loss": 1.8539,
      "step": 15700
    },
    {
      "epoch": 2.9641509433962265,
      "grad_norm": 0.9337220191955566,
      "learning_rate": 0.00020360377358490567,
      "loss": 1.8642,
      "step": 15710
    },
    {
      "epoch": 2.9660377358490564,
      "grad_norm": 0.8402162790298462,
      "learning_rate": 0.00020341509433962265,
      "loss": 1.8419,
      "step": 15720
    },
    {
      "epoch": 2.9679245283018867,
      "grad_norm": 0.7990015745162964,
      "learning_rate": 0.0002032264150943396,
      "loss": 1.8531,
      "step": 15730
    },
    {
      "epoch": 2.969811320754717,
      "grad_norm": 0.8230666518211365,
      "learning_rate": 0.00020303773584905662,
      "loss": 1.8068,
      "step": 15740
    },
    {
      "epoch": 2.9716981132075473,
      "grad_norm": 0.8762806057929993,
      "learning_rate": 0.0002028490566037736,
      "loss": 1.9649,
      "step": 15750
    },
    {
      "epoch": 2.9735849056603776,
      "grad_norm": 0.7666979432106018,
      "learning_rate": 0.00020266037735849058,
      "loss": 1.8338,
      "step": 15760
    },
    {
      "epoch": 2.9754716981132074,
      "grad_norm": 0.796139121055603,
      "learning_rate": 0.00020247169811320756,
      "loss": 1.8512,
      "step": 15770
    },
    {
      "epoch": 2.9773584905660377,
      "grad_norm": 0.8737797141075134,
      "learning_rate": 0.00020228301886792452,
      "loss": 1.8471,
      "step": 15780
    },
    {
      "epoch": 2.979245283018868,
      "grad_norm": 0.9461063742637634,
      "learning_rate": 0.0002020943396226415,
      "loss": 1.8941,
      "step": 15790
    },
    {
      "epoch": 2.981132075471698,
      "grad_norm": 0.8940624594688416,
      "learning_rate": 0.0002019056603773585,
      "loss": 1.8218,
      "step": 15800
    },
    {
      "epoch": 2.983018867924528,
      "grad_norm": 0.69733726978302,
      "learning_rate": 0.0002017169811320755,
      "loss": 1.8495,
      "step": 15810
    },
    {
      "epoch": 2.9849056603773585,
      "grad_norm": 0.7692187428474426,
      "learning_rate": 0.00020152830188679247,
      "loss": 1.8259,
      "step": 15820
    },
    {
      "epoch": 2.986792452830189,
      "grad_norm": 0.7920985817909241,
      "learning_rate": 0.00020133962264150943,
      "loss": 1.8893,
      "step": 15830
    },
    {
      "epoch": 2.988679245283019,
      "grad_norm": 0.7888304591178894,
      "learning_rate": 0.0002011509433962264,
      "loss": 1.9348,
      "step": 15840
    },
    {
      "epoch": 2.990566037735849,
      "grad_norm": 0.7088717222213745,
      "learning_rate": 0.0002009622641509434,
      "loss": 1.8218,
      "step": 15850
    },
    {
      "epoch": 2.9924528301886792,
      "grad_norm": 0.793786346912384,
      "learning_rate": 0.0002007735849056604,
      "loss": 1.9549,
      "step": 15860
    },
    {
      "epoch": 2.9943396226415095,
      "grad_norm": 0.8013444542884827,
      "learning_rate": 0.00020058490566037738,
      "loss": 1.883,
      "step": 15870
    },
    {
      "epoch": 2.9962264150943394,
      "grad_norm": 0.8290245532989502,
      "learning_rate": 0.00020039622641509434,
      "loss": 1.886,
      "step": 15880
    },
    {
      "epoch": 2.9981132075471697,
      "grad_norm": 0.7106526494026184,
      "learning_rate": 0.00020020754716981132,
      "loss": 1.7837,
      "step": 15890
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.4442483186721802,
      "learning_rate": 0.0002000188679245283,
      "loss": 1.9164,
      "step": 15900
    },
    {
      "epoch": 3.0018867924528303,
      "grad_norm": 0.8070591688156128,
      "learning_rate": 0.00019983018867924529,
      "loss": 1.8358,
      "step": 15910
    },
    {
      "epoch": 3.0037735849056606,
      "grad_norm": 0.8642058372497559,
      "learning_rate": 0.0001996415094339623,
      "loss": 1.896,
      "step": 15920
    },
    {
      "epoch": 3.0056603773584905,
      "grad_norm": 0.848009467124939,
      "learning_rate": 0.00019945283018867925,
      "loss": 1.9549,
      "step": 15930
    },
    {
      "epoch": 3.0075471698113208,
      "grad_norm": 0.8278559446334839,
      "learning_rate": 0.00019926415094339623,
      "loss": 1.918,
      "step": 15940
    },
    {
      "epoch": 3.009433962264151,
      "grad_norm": 0.7657144665718079,
      "learning_rate": 0.00019907547169811321,
      "loss": 1.7868,
      "step": 15950
    },
    {
      "epoch": 3.0113207547169814,
      "grad_norm": 0.8552879095077515,
      "learning_rate": 0.0001988867924528302,
      "loss": 1.9197,
      "step": 15960
    },
    {
      "epoch": 3.013207547169811,
      "grad_norm": 0.7855097651481628,
      "learning_rate": 0.00019869811320754718,
      "loss": 1.8662,
      "step": 15970
    },
    {
      "epoch": 3.0150943396226415,
      "grad_norm": 0.8879498839378357,
      "learning_rate": 0.00019850943396226413,
      "loss": 1.811,
      "step": 15980
    },
    {
      "epoch": 3.016981132075472,
      "grad_norm": 0.8550102114677429,
      "learning_rate": 0.00019832075471698114,
      "loss": 1.8872,
      "step": 15990
    },
    {
      "epoch": 3.018867924528302,
      "grad_norm": 0.8523671627044678,
      "learning_rate": 0.00019813207547169812,
      "loss": 1.9225,
      "step": 16000
    },
    {
      "epoch": 3.020754716981132,
      "grad_norm": 0.8297029137611389,
      "learning_rate": 0.0001979433962264151,
      "loss": 1.9197,
      "step": 16010
    },
    {
      "epoch": 3.0226415094339623,
      "grad_norm": 0.78165203332901,
      "learning_rate": 0.0001977547169811321,
      "loss": 1.8424,
      "step": 16020
    },
    {
      "epoch": 3.0245283018867926,
      "grad_norm": 0.8230711221694946,
      "learning_rate": 0.00019756603773584904,
      "loss": 1.9278,
      "step": 16030
    },
    {
      "epoch": 3.026415094339623,
      "grad_norm": 0.8451960682868958,
      "learning_rate": 0.00019737735849056603,
      "loss": 1.8414,
      "step": 16040
    },
    {
      "epoch": 3.0283018867924527,
      "grad_norm": 0.8170414566993713,
      "learning_rate": 0.00019718867924528304,
      "loss": 1.8815,
      "step": 16050
    },
    {
      "epoch": 3.030188679245283,
      "grad_norm": 0.7894734144210815,
      "learning_rate": 0.00019700000000000002,
      "loss": 1.8696,
      "step": 16060
    },
    {
      "epoch": 3.0320754716981133,
      "grad_norm": 0.7846057415008545,
      "learning_rate": 0.00019681132075471697,
      "loss": 1.8306,
      "step": 16070
    },
    {
      "epoch": 3.0339622641509436,
      "grad_norm": 0.9326953887939453,
      "learning_rate": 0.00019662264150943396,
      "loss": 1.857,
      "step": 16080
    },
    {
      "epoch": 3.0358490566037735,
      "grad_norm": 0.7524373531341553,
      "learning_rate": 0.00019643396226415094,
      "loss": 1.8372,
      "step": 16090
    },
    {
      "epoch": 3.0377358490566038,
      "grad_norm": 0.7915159463882446,
      "learning_rate": 0.00019624528301886792,
      "loss": 1.8482,
      "step": 16100
    },
    {
      "epoch": 3.039622641509434,
      "grad_norm": 0.7548055052757263,
      "learning_rate": 0.00019605660377358493,
      "loss": 1.9313,
      "step": 16110
    },
    {
      "epoch": 3.0415094339622644,
      "grad_norm": 0.7319902181625366,
      "learning_rate": 0.00019586792452830188,
      "loss": 1.8948,
      "step": 16120
    },
    {
      "epoch": 3.043396226415094,
      "grad_norm": 0.7660513520240784,
      "learning_rate": 0.00019567924528301887,
      "loss": 1.8915,
      "step": 16130
    },
    {
      "epoch": 3.0452830188679245,
      "grad_norm": 0.8252423405647278,
      "learning_rate": 0.00019549056603773585,
      "loss": 1.8698,
      "step": 16140
    },
    {
      "epoch": 3.047169811320755,
      "grad_norm": 0.8058767318725586,
      "learning_rate": 0.00019530188679245283,
      "loss": 1.7837,
      "step": 16150
    },
    {
      "epoch": 3.049056603773585,
      "grad_norm": 0.870292067527771,
      "learning_rate": 0.0001951132075471698,
      "loss": 1.87,
      "step": 16160
    },
    {
      "epoch": 3.050943396226415,
      "grad_norm": 0.8733150362968445,
      "learning_rate": 0.0001949245283018868,
      "loss": 1.9544,
      "step": 16170
    },
    {
      "epoch": 3.0528301886792453,
      "grad_norm": 0.8434141874313354,
      "learning_rate": 0.00019473584905660378,
      "loss": 1.8942,
      "step": 16180
    },
    {
      "epoch": 3.0547169811320756,
      "grad_norm": 0.7675861716270447,
      "learning_rate": 0.00019454716981132076,
      "loss": 1.9476,
      "step": 16190
    },
    {
      "epoch": 3.056603773584906,
      "grad_norm": 0.8085566759109497,
      "learning_rate": 0.00019435849056603774,
      "loss": 1.8848,
      "step": 16200
    },
    {
      "epoch": 3.0584905660377357,
      "grad_norm": 0.7551815509796143,
      "learning_rate": 0.00019416981132075472,
      "loss": 1.8277,
      "step": 16210
    },
    {
      "epoch": 3.060377358490566,
      "grad_norm": 0.6658917665481567,
      "learning_rate": 0.00019398113207547168,
      "loss": 1.8413,
      "step": 16220
    },
    {
      "epoch": 3.0622641509433963,
      "grad_norm": 0.7893303036689758,
      "learning_rate": 0.0001937924528301887,
      "loss": 1.9049,
      "step": 16230
    },
    {
      "epoch": 3.0641509433962266,
      "grad_norm": 0.888648271560669,
      "learning_rate": 0.00019360377358490567,
      "loss": 1.8889,
      "step": 16240
    },
    {
      "epoch": 3.0660377358490565,
      "grad_norm": 0.7664127349853516,
      "learning_rate": 0.00019341509433962265,
      "loss": 1.8595,
      "step": 16250
    },
    {
      "epoch": 3.0679245283018868,
      "grad_norm": 0.7201463580131531,
      "learning_rate": 0.00019322641509433963,
      "loss": 1.8355,
      "step": 16260
    },
    {
      "epoch": 3.069811320754717,
      "grad_norm": 0.8350322246551514,
      "learning_rate": 0.0001930377358490566,
      "loss": 1.769,
      "step": 16270
    },
    {
      "epoch": 3.0716981132075474,
      "grad_norm": 0.8688954710960388,
      "learning_rate": 0.00019284905660377357,
      "loss": 1.9375,
      "step": 16280
    },
    {
      "epoch": 3.0735849056603772,
      "grad_norm": 0.7491747736930847,
      "learning_rate": 0.00019266037735849058,
      "loss": 1.831,
      "step": 16290
    },
    {
      "epoch": 3.0754716981132075,
      "grad_norm": 0.8343143463134766,
      "learning_rate": 0.00019247169811320756,
      "loss": 1.8253,
      "step": 16300
    },
    {
      "epoch": 3.077358490566038,
      "grad_norm": 0.7833440899848938,
      "learning_rate": 0.00019228301886792455,
      "loss": 1.8274,
      "step": 16310
    },
    {
      "epoch": 3.079245283018868,
      "grad_norm": 0.8396374583244324,
      "learning_rate": 0.0001920943396226415,
      "loss": 1.8591,
      "step": 16320
    },
    {
      "epoch": 3.081132075471698,
      "grad_norm": 0.8155761361122131,
      "learning_rate": 0.00019190566037735848,
      "loss": 1.7603,
      "step": 16330
    },
    {
      "epoch": 3.0830188679245283,
      "grad_norm": 0.8315261602401733,
      "learning_rate": 0.00019171698113207546,
      "loss": 1.8771,
      "step": 16340
    },
    {
      "epoch": 3.0849056603773586,
      "grad_norm": 0.7632434368133545,
      "learning_rate": 0.00019152830188679247,
      "loss": 1.7833,
      "step": 16350
    },
    {
      "epoch": 3.086792452830189,
      "grad_norm": 0.8279287815093994,
      "learning_rate": 0.00019133962264150946,
      "loss": 1.8948,
      "step": 16360
    },
    {
      "epoch": 3.0886792452830187,
      "grad_norm": 0.7964692115783691,
      "learning_rate": 0.0001911509433962264,
      "loss": 1.862,
      "step": 16370
    },
    {
      "epoch": 3.090566037735849,
      "grad_norm": 0.8231016993522644,
      "learning_rate": 0.0001909622641509434,
      "loss": 1.8255,
      "step": 16380
    },
    {
      "epoch": 3.0924528301886793,
      "grad_norm": 0.7603175044059753,
      "learning_rate": 0.00019077358490566038,
      "loss": 1.9086,
      "step": 16390
    },
    {
      "epoch": 3.0943396226415096,
      "grad_norm": 0.7598459720611572,
      "learning_rate": 0.00019058490566037736,
      "loss": 1.8833,
      "step": 16400
    },
    {
      "epoch": 3.0962264150943395,
      "grad_norm": 0.8949623107910156,
      "learning_rate": 0.00019039622641509437,
      "loss": 1.9288,
      "step": 16410
    },
    {
      "epoch": 3.09811320754717,
      "grad_norm": 0.9170042872428894,
      "learning_rate": 0.00019020754716981132,
      "loss": 1.8518,
      "step": 16420
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.7528060674667358,
      "learning_rate": 0.0001900188679245283,
      "loss": 1.8224,
      "step": 16430
    },
    {
      "epoch": 3.1018867924528304,
      "grad_norm": 0.8475425839424133,
      "learning_rate": 0.00018983018867924529,
      "loss": 1.9494,
      "step": 16440
    },
    {
      "epoch": 3.1037735849056602,
      "grad_norm": 0.7532005310058594,
      "learning_rate": 0.00018964150943396227,
      "loss": 1.8716,
      "step": 16450
    },
    {
      "epoch": 3.1056603773584905,
      "grad_norm": 0.8493580222129822,
      "learning_rate": 0.00018945283018867925,
      "loss": 1.8551,
      "step": 16460
    },
    {
      "epoch": 3.107547169811321,
      "grad_norm": 0.7749432921409607,
      "learning_rate": 0.0001892641509433962,
      "loss": 1.7747,
      "step": 16470
    },
    {
      "epoch": 3.109433962264151,
      "grad_norm": 0.811446487903595,
      "learning_rate": 0.00018907547169811321,
      "loss": 1.8299,
      "step": 16480
    },
    {
      "epoch": 3.111320754716981,
      "grad_norm": 0.9082091450691223,
      "learning_rate": 0.0001888867924528302,
      "loss": 1.95,
      "step": 16490
    },
    {
      "epoch": 3.1132075471698113,
      "grad_norm": 0.8607224225997925,
      "learning_rate": 0.00018869811320754718,
      "loss": 1.9245,
      "step": 16500
    },
    {
      "epoch": 3.1150943396226416,
      "grad_norm": 0.778487503528595,
      "learning_rate": 0.00018850943396226416,
      "loss": 1.9366,
      "step": 16510
    },
    {
      "epoch": 3.116981132075472,
      "grad_norm": 0.7783833146095276,
      "learning_rate": 0.00018832075471698112,
      "loss": 1.8597,
      "step": 16520
    },
    {
      "epoch": 3.1188679245283017,
      "grad_norm": 0.8045751452445984,
      "learning_rate": 0.0001881320754716981,
      "loss": 1.8153,
      "step": 16530
    },
    {
      "epoch": 3.120754716981132,
      "grad_norm": 0.8148561716079712,
      "learning_rate": 0.0001879433962264151,
      "loss": 1.8048,
      "step": 16540
    },
    {
      "epoch": 3.1226415094339623,
      "grad_norm": 0.8520436882972717,
      "learning_rate": 0.0001877547169811321,
      "loss": 1.8523,
      "step": 16550
    },
    {
      "epoch": 3.1245283018867926,
      "grad_norm": 0.9416424632072449,
      "learning_rate": 0.00018756603773584907,
      "loss": 1.9079,
      "step": 16560
    },
    {
      "epoch": 3.1264150943396225,
      "grad_norm": 0.8344646692276001,
      "learning_rate": 0.00018737735849056603,
      "loss": 1.831,
      "step": 16570
    },
    {
      "epoch": 3.128301886792453,
      "grad_norm": 0.7646380662918091,
      "learning_rate": 0.000187188679245283,
      "loss": 1.8576,
      "step": 16580
    },
    {
      "epoch": 3.130188679245283,
      "grad_norm": 0.8257002234458923,
      "learning_rate": 0.000187,
      "loss": 1.853,
      "step": 16590
    },
    {
      "epoch": 3.1320754716981134,
      "grad_norm": 0.8246698379516602,
      "learning_rate": 0.000186811320754717,
      "loss": 1.8737,
      "step": 16600
    },
    {
      "epoch": 3.1339622641509433,
      "grad_norm": 0.8511813879013062,
      "learning_rate": 0.00018662264150943398,
      "loss": 1.8261,
      "step": 16610
    },
    {
      "epoch": 3.1358490566037736,
      "grad_norm": 0.7940559387207031,
      "learning_rate": 0.00018643396226415094,
      "loss": 1.9272,
      "step": 16620
    },
    {
      "epoch": 3.137735849056604,
      "grad_norm": 0.8387691378593445,
      "learning_rate": 0.00018624528301886792,
      "loss": 1.8626,
      "step": 16630
    },
    {
      "epoch": 3.139622641509434,
      "grad_norm": 0.7932001352310181,
      "learning_rate": 0.0001860566037735849,
      "loss": 1.8942,
      "step": 16640
    },
    {
      "epoch": 3.141509433962264,
      "grad_norm": 0.8936154246330261,
      "learning_rate": 0.00018586792452830188,
      "loss": 1.8514,
      "step": 16650
    },
    {
      "epoch": 3.1433962264150943,
      "grad_norm": 0.8582718968391418,
      "learning_rate": 0.0001856792452830189,
      "loss": 1.8921,
      "step": 16660
    },
    {
      "epoch": 3.1452830188679246,
      "grad_norm": 0.8366660475730896,
      "learning_rate": 0.00018549056603773585,
      "loss": 1.85,
      "step": 16670
    },
    {
      "epoch": 3.147169811320755,
      "grad_norm": 0.7686052918434143,
      "learning_rate": 0.00018530188679245283,
      "loss": 1.8401,
      "step": 16680
    },
    {
      "epoch": 3.1490566037735848,
      "grad_norm": 0.8318099975585938,
      "learning_rate": 0.0001851132075471698,
      "loss": 1.8983,
      "step": 16690
    },
    {
      "epoch": 3.150943396226415,
      "grad_norm": 0.8473071455955505,
      "learning_rate": 0.0001849245283018868,
      "loss": 1.8408,
      "step": 16700
    },
    {
      "epoch": 3.1528301886792454,
      "grad_norm": 0.8518456220626831,
      "learning_rate": 0.00018473584905660378,
      "loss": 1.8564,
      "step": 16710
    },
    {
      "epoch": 3.1547169811320757,
      "grad_norm": 0.8417035341262817,
      "learning_rate": 0.00018454716981132076,
      "loss": 1.923,
      "step": 16720
    },
    {
      "epoch": 3.1566037735849055,
      "grad_norm": 0.855785071849823,
      "learning_rate": 0.00018435849056603774,
      "loss": 1.8833,
      "step": 16730
    },
    {
      "epoch": 3.158490566037736,
      "grad_norm": 0.8189526200294495,
      "learning_rate": 0.00018416981132075472,
      "loss": 1.9234,
      "step": 16740
    },
    {
      "epoch": 3.160377358490566,
      "grad_norm": 0.8729084730148315,
      "learning_rate": 0.0001839811320754717,
      "loss": 1.8508,
      "step": 16750
    },
    {
      "epoch": 3.1622641509433964,
      "grad_norm": 0.8379101753234863,
      "learning_rate": 0.0001837924528301887,
      "loss": 1.8148,
      "step": 16760
    },
    {
      "epoch": 3.1641509433962263,
      "grad_norm": 0.8763940334320068,
      "learning_rate": 0.00018360377358490564,
      "loss": 1.861,
      "step": 16770
    },
    {
      "epoch": 3.1660377358490566,
      "grad_norm": 0.7831141948699951,
      "learning_rate": 0.00018341509433962265,
      "loss": 1.9179,
      "step": 16780
    },
    {
      "epoch": 3.167924528301887,
      "grad_norm": 0.7530080676078796,
      "learning_rate": 0.00018322641509433964,
      "loss": 1.8315,
      "step": 16790
    },
    {
      "epoch": 3.169811320754717,
      "grad_norm": 0.7807313203811646,
      "learning_rate": 0.00018303773584905662,
      "loss": 1.8755,
      "step": 16800
    },
    {
      "epoch": 3.171698113207547,
      "grad_norm": 0.8074008226394653,
      "learning_rate": 0.0001828490566037736,
      "loss": 1.8454,
      "step": 16810
    },
    {
      "epoch": 3.1735849056603773,
      "grad_norm": 0.8356034755706787,
      "learning_rate": 0.00018266037735849055,
      "loss": 1.8891,
      "step": 16820
    },
    {
      "epoch": 3.1754716981132076,
      "grad_norm": 0.907623291015625,
      "learning_rate": 0.00018247169811320754,
      "loss": 1.8027,
      "step": 16830
    },
    {
      "epoch": 3.177358490566038,
      "grad_norm": 0.6809828877449036,
      "learning_rate": 0.00018228301886792455,
      "loss": 1.8593,
      "step": 16840
    },
    {
      "epoch": 3.1792452830188678,
      "grad_norm": 0.8394564986228943,
      "learning_rate": 0.00018209433962264153,
      "loss": 1.851,
      "step": 16850
    },
    {
      "epoch": 3.181132075471698,
      "grad_norm": 0.8106642961502075,
      "learning_rate": 0.0001819056603773585,
      "loss": 1.8688,
      "step": 16860
    },
    {
      "epoch": 3.1830188679245284,
      "grad_norm": 0.8837006688117981,
      "learning_rate": 0.00018171698113207547,
      "loss": 1.9175,
      "step": 16870
    },
    {
      "epoch": 3.1849056603773587,
      "grad_norm": 0.8392118811607361,
      "learning_rate": 0.00018152830188679245,
      "loss": 1.8858,
      "step": 16880
    },
    {
      "epoch": 3.1867924528301885,
      "grad_norm": 0.8478587865829468,
      "learning_rate": 0.00018133962264150943,
      "loss": 1.8839,
      "step": 16890
    },
    {
      "epoch": 3.188679245283019,
      "grad_norm": 0.7888919115066528,
      "learning_rate": 0.0001811509433962264,
      "loss": 1.8931,
      "step": 16900
    },
    {
      "epoch": 3.190566037735849,
      "grad_norm": 0.8752886056900024,
      "learning_rate": 0.00018096226415094342,
      "loss": 1.761,
      "step": 16910
    },
    {
      "epoch": 3.1924528301886794,
      "grad_norm": 0.8210168480873108,
      "learning_rate": 0.00018077358490566038,
      "loss": 1.888,
      "step": 16920
    },
    {
      "epoch": 3.1943396226415093,
      "grad_norm": 0.826300859451294,
      "learning_rate": 0.00018058490566037736,
      "loss": 1.9075,
      "step": 16930
    },
    {
      "epoch": 3.1962264150943396,
      "grad_norm": 0.7992745637893677,
      "learning_rate": 0.00018039622641509434,
      "loss": 1.854,
      "step": 16940
    },
    {
      "epoch": 3.19811320754717,
      "grad_norm": 0.7658740282058716,
      "learning_rate": 0.00018020754716981132,
      "loss": 1.8731,
      "step": 16950
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.734769344329834,
      "learning_rate": 0.0001800188679245283,
      "loss": 1.7918,
      "step": 16960
    },
    {
      "epoch": 3.20188679245283,
      "grad_norm": 0.7655510306358337,
      "learning_rate": 0.0001798301886792453,
      "loss": 1.878,
      "step": 16970
    },
    {
      "epoch": 3.2037735849056603,
      "grad_norm": 0.7502393126487732,
      "learning_rate": 0.00017964150943396227,
      "loss": 1.8435,
      "step": 16980
    },
    {
      "epoch": 3.2056603773584906,
      "grad_norm": 0.922943651676178,
      "learning_rate": 0.00017945283018867925,
      "loss": 1.9009,
      "step": 16990
    },
    {
      "epoch": 3.207547169811321,
      "grad_norm": 0.9227594137191772,
      "learning_rate": 0.00017926415094339623,
      "loss": 1.906,
      "step": 17000
    },
    {
      "epoch": 3.209433962264151,
      "grad_norm": 0.7506023049354553,
      "learning_rate": 0.00017907547169811322,
      "loss": 1.9635,
      "step": 17010
    },
    {
      "epoch": 3.211320754716981,
      "grad_norm": 0.9789392352104187,
      "learning_rate": 0.00017888679245283017,
      "loss": 1.8655,
      "step": 17020
    },
    {
      "epoch": 3.2132075471698114,
      "grad_norm": 0.8956019282341003,
      "learning_rate": 0.00017869811320754718,
      "loss": 1.9359,
      "step": 17030
    },
    {
      "epoch": 3.2150943396226417,
      "grad_norm": 0.8223049640655518,
      "learning_rate": 0.00017850943396226416,
      "loss": 1.9317,
      "step": 17040
    },
    {
      "epoch": 3.2169811320754715,
      "grad_norm": 0.7464955449104309,
      "learning_rate": 0.00017832075471698114,
      "loss": 1.8388,
      "step": 17050
    },
    {
      "epoch": 3.218867924528302,
      "grad_norm": 0.8237977623939514,
      "learning_rate": 0.00017813207547169813,
      "loss": 1.8665,
      "step": 17060
    },
    {
      "epoch": 3.220754716981132,
      "grad_norm": 0.8405898809432983,
      "learning_rate": 0.00017794339622641508,
      "loss": 1.8712,
      "step": 17070
    },
    {
      "epoch": 3.2226415094339624,
      "grad_norm": 0.8026865124702454,
      "learning_rate": 0.00017775471698113206,
      "loss": 1.8393,
      "step": 17080
    },
    {
      "epoch": 3.2245283018867923,
      "grad_norm": 0.7632151246070862,
      "learning_rate": 0.00017756603773584907,
      "loss": 1.8235,
      "step": 17090
    },
    {
      "epoch": 3.2264150943396226,
      "grad_norm": 0.8211879730224609,
      "learning_rate": 0.00017737735849056606,
      "loss": 1.8057,
      "step": 17100
    },
    {
      "epoch": 3.228301886792453,
      "grad_norm": 0.740506649017334,
      "learning_rate": 0.00017718867924528304,
      "loss": 1.9657,
      "step": 17110
    },
    {
      "epoch": 3.230188679245283,
      "grad_norm": 0.8828678131103516,
      "learning_rate": 0.000177,
      "loss": 1.936,
      "step": 17120
    },
    {
      "epoch": 3.232075471698113,
      "grad_norm": 0.8992137312889099,
      "learning_rate": 0.00017681132075471697,
      "loss": 1.8368,
      "step": 17130
    },
    {
      "epoch": 3.2339622641509433,
      "grad_norm": 0.9478635787963867,
      "learning_rate": 0.00017662264150943396,
      "loss": 1.9285,
      "step": 17140
    },
    {
      "epoch": 3.2358490566037736,
      "grad_norm": 0.8458114266395569,
      "learning_rate": 0.00017643396226415097,
      "loss": 1.8082,
      "step": 17150
    },
    {
      "epoch": 3.237735849056604,
      "grad_norm": 0.8018431663513184,
      "learning_rate": 0.00017624528301886795,
      "loss": 1.874,
      "step": 17160
    },
    {
      "epoch": 3.239622641509434,
      "grad_norm": 0.7644487619400024,
      "learning_rate": 0.0001760566037735849,
      "loss": 1.7969,
      "step": 17170
    },
    {
      "epoch": 3.241509433962264,
      "grad_norm": 0.7581010460853577,
      "learning_rate": 0.00017586792452830189,
      "loss": 1.8445,
      "step": 17180
    },
    {
      "epoch": 3.2433962264150944,
      "grad_norm": 0.8008180260658264,
      "learning_rate": 0.00017567924528301887,
      "loss": 1.9036,
      "step": 17190
    },
    {
      "epoch": 3.2452830188679247,
      "grad_norm": 0.8760478496551514,
      "learning_rate": 0.00017549056603773585,
      "loss": 1.8216,
      "step": 17200
    },
    {
      "epoch": 3.2471698113207546,
      "grad_norm": 0.7761963605880737,
      "learning_rate": 0.00017530188679245286,
      "loss": 1.8618,
      "step": 17210
    },
    {
      "epoch": 3.249056603773585,
      "grad_norm": 0.6923676133155823,
      "learning_rate": 0.00017511320754716981,
      "loss": 1.8803,
      "step": 17220
    },
    {
      "epoch": 3.250943396226415,
      "grad_norm": 0.8082215785980225,
      "learning_rate": 0.0001749245283018868,
      "loss": 1.8593,
      "step": 17230
    },
    {
      "epoch": 3.2528301886792454,
      "grad_norm": 0.8199975490570068,
      "learning_rate": 0.00017473584905660378,
      "loss": 1.8245,
      "step": 17240
    },
    {
      "epoch": 3.2547169811320753,
      "grad_norm": 0.776249349117279,
      "learning_rate": 0.00017454716981132076,
      "loss": 1.8173,
      "step": 17250
    },
    {
      "epoch": 3.2566037735849056,
      "grad_norm": 0.904272198677063,
      "learning_rate": 0.00017435849056603774,
      "loss": 1.8379,
      "step": 17260
    },
    {
      "epoch": 3.258490566037736,
      "grad_norm": 0.8243971467018127,
      "learning_rate": 0.00017416981132075473,
      "loss": 1.898,
      "step": 17270
    },
    {
      "epoch": 3.260377358490566,
      "grad_norm": 0.6920029520988464,
      "learning_rate": 0.0001739811320754717,
      "loss": 1.7976,
      "step": 17280
    },
    {
      "epoch": 3.262264150943396,
      "grad_norm": 0.7471284866333008,
      "learning_rate": 0.0001737924528301887,
      "loss": 1.822,
      "step": 17290
    },
    {
      "epoch": 3.2641509433962264,
      "grad_norm": 0.853020966053009,
      "learning_rate": 0.00017360377358490567,
      "loss": 1.7639,
      "step": 17300
    },
    {
      "epoch": 3.2660377358490567,
      "grad_norm": 0.7856210470199585,
      "learning_rate": 0.00017341509433962265,
      "loss": 1.8054,
      "step": 17310
    },
    {
      "epoch": 3.267924528301887,
      "grad_norm": 0.8308922648429871,
      "learning_rate": 0.0001732264150943396,
      "loss": 1.9249,
      "step": 17320
    },
    {
      "epoch": 3.269811320754717,
      "grad_norm": 0.7993676662445068,
      "learning_rate": 0.0001730377358490566,
      "loss": 1.8791,
      "step": 17330
    },
    {
      "epoch": 3.271698113207547,
      "grad_norm": 0.866345226764679,
      "learning_rate": 0.0001728490566037736,
      "loss": 1.9563,
      "step": 17340
    },
    {
      "epoch": 3.2735849056603774,
      "grad_norm": 0.865101158618927,
      "learning_rate": 0.00017266037735849058,
      "loss": 1.8539,
      "step": 17350
    },
    {
      "epoch": 3.2754716981132077,
      "grad_norm": 0.8744583129882812,
      "learning_rate": 0.00017247169811320754,
      "loss": 1.95,
      "step": 17360
    },
    {
      "epoch": 3.2773584905660376,
      "grad_norm": 0.781523585319519,
      "learning_rate": 0.00017228301886792452,
      "loss": 1.8602,
      "step": 17370
    },
    {
      "epoch": 3.279245283018868,
      "grad_norm": 0.7677537798881531,
      "learning_rate": 0.0001720943396226415,
      "loss": 1.8587,
      "step": 17380
    },
    {
      "epoch": 3.281132075471698,
      "grad_norm": 0.7064343094825745,
      "learning_rate": 0.00017190566037735848,
      "loss": 1.9177,
      "step": 17390
    },
    {
      "epoch": 3.2830188679245285,
      "grad_norm": 0.7708050608634949,
      "learning_rate": 0.0001717169811320755,
      "loss": 1.9034,
      "step": 17400
    },
    {
      "epoch": 3.2849056603773583,
      "grad_norm": 0.7513856887817383,
      "learning_rate": 0.00017152830188679245,
      "loss": 1.8078,
      "step": 17410
    },
    {
      "epoch": 3.2867924528301886,
      "grad_norm": 0.8359315395355225,
      "learning_rate": 0.00017133962264150943,
      "loss": 1.8668,
      "step": 17420
    },
    {
      "epoch": 3.288679245283019,
      "grad_norm": 0.8632795214653015,
      "learning_rate": 0.0001711509433962264,
      "loss": 1.857,
      "step": 17430
    },
    {
      "epoch": 3.290566037735849,
      "grad_norm": 0.859057605266571,
      "learning_rate": 0.0001709622641509434,
      "loss": 1.855,
      "step": 17440
    },
    {
      "epoch": 3.292452830188679,
      "grad_norm": 0.7834110856056213,
      "learning_rate": 0.00017077358490566038,
      "loss": 1.8832,
      "step": 17450
    },
    {
      "epoch": 3.2943396226415094,
      "grad_norm": 0.817672848701477,
      "learning_rate": 0.00017058490566037736,
      "loss": 1.7567,
      "step": 17460
    },
    {
      "epoch": 3.2962264150943397,
      "grad_norm": 0.8287608623504639,
      "learning_rate": 0.00017039622641509434,
      "loss": 1.7436,
      "step": 17470
    },
    {
      "epoch": 3.29811320754717,
      "grad_norm": 0.7489547729492188,
      "learning_rate": 0.00017020754716981132,
      "loss": 1.8031,
      "step": 17480
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.7711194753646851,
      "learning_rate": 0.0001700188679245283,
      "loss": 1.8058,
      "step": 17490
    },
    {
      "epoch": 3.30188679245283,
      "grad_norm": 0.9604339599609375,
      "learning_rate": 0.0001698301886792453,
      "loss": 1.864,
      "step": 17500
    },
    {
      "epoch": 3.3037735849056604,
      "grad_norm": 0.8060937523841858,
      "learning_rate": 0.00016964150943396224,
      "loss": 1.7334,
      "step": 17510
    },
    {
      "epoch": 3.3056603773584907,
      "grad_norm": 0.9649773240089417,
      "learning_rate": 0.00016945283018867925,
      "loss": 1.8345,
      "step": 17520
    },
    {
      "epoch": 3.3075471698113206,
      "grad_norm": 0.7107403874397278,
      "learning_rate": 0.00016926415094339623,
      "loss": 1.8456,
      "step": 17530
    },
    {
      "epoch": 3.309433962264151,
      "grad_norm": 0.7486902475357056,
      "learning_rate": 0.00016907547169811322,
      "loss": 1.8258,
      "step": 17540
    },
    {
      "epoch": 3.311320754716981,
      "grad_norm": 0.7513342499732971,
      "learning_rate": 0.0001688867924528302,
      "loss": 1.9041,
      "step": 17550
    },
    {
      "epoch": 3.3132075471698115,
      "grad_norm": 0.8718507885932922,
      "learning_rate": 0.00016869811320754715,
      "loss": 1.9221,
      "step": 17560
    },
    {
      "epoch": 3.3150943396226413,
      "grad_norm": 0.8519847989082336,
      "learning_rate": 0.00016850943396226414,
      "loss": 1.7895,
      "step": 17570
    },
    {
      "epoch": 3.3169811320754716,
      "grad_norm": 0.8421048521995544,
      "learning_rate": 0.00016832075471698115,
      "loss": 1.7785,
      "step": 17580
    },
    {
      "epoch": 3.318867924528302,
      "grad_norm": 0.7651190161705017,
      "learning_rate": 0.00016813207547169813,
      "loss": 1.8421,
      "step": 17590
    },
    {
      "epoch": 3.3207547169811322,
      "grad_norm": 0.7961164712905884,
      "learning_rate": 0.0001679433962264151,
      "loss": 1.8143,
      "step": 17600
    },
    {
      "epoch": 3.322641509433962,
      "grad_norm": 0.7832209467887878,
      "learning_rate": 0.00016775471698113206,
      "loss": 1.855,
      "step": 17610
    },
    {
      "epoch": 3.3245283018867924,
      "grad_norm": 0.7974072098731995,
      "learning_rate": 0.00016756603773584905,
      "loss": 1.7983,
      "step": 17620
    },
    {
      "epoch": 3.3264150943396227,
      "grad_norm": 0.7464333772659302,
      "learning_rate": 0.00016737735849056603,
      "loss": 1.8217,
      "step": 17630
    },
    {
      "epoch": 3.328301886792453,
      "grad_norm": 0.8223545551300049,
      "learning_rate": 0.00016718867924528304,
      "loss": 1.9157,
      "step": 17640
    },
    {
      "epoch": 3.330188679245283,
      "grad_norm": 0.7585579752922058,
      "learning_rate": 0.00016700000000000002,
      "loss": 1.8955,
      "step": 17650
    },
    {
      "epoch": 3.332075471698113,
      "grad_norm": 0.9049440622329712,
      "learning_rate": 0.00016681132075471698,
      "loss": 1.8298,
      "step": 17660
    },
    {
      "epoch": 3.3339622641509434,
      "grad_norm": 0.865186333656311,
      "learning_rate": 0.00016662264150943396,
      "loss": 1.8635,
      "step": 17670
    },
    {
      "epoch": 3.3358490566037737,
      "grad_norm": 0.8180955648422241,
      "learning_rate": 0.00016643396226415094,
      "loss": 1.7918,
      "step": 17680
    },
    {
      "epoch": 3.3377358490566036,
      "grad_norm": 0.8256021738052368,
      "learning_rate": 0.00016624528301886792,
      "loss": 1.8436,
      "step": 17690
    },
    {
      "epoch": 3.339622641509434,
      "grad_norm": 0.7494956851005554,
      "learning_rate": 0.00016605660377358493,
      "loss": 1.8557,
      "step": 17700
    },
    {
      "epoch": 3.341509433962264,
      "grad_norm": 0.8140242099761963,
      "learning_rate": 0.0001658679245283019,
      "loss": 1.8758,
      "step": 17710
    },
    {
      "epoch": 3.3433962264150945,
      "grad_norm": 0.8536402583122253,
      "learning_rate": 0.00016567924528301887,
      "loss": 1.9027,
      "step": 17720
    },
    {
      "epoch": 3.3452830188679243,
      "grad_norm": 1.0297001600265503,
      "learning_rate": 0.00016549056603773585,
      "loss": 1.8702,
      "step": 17730
    },
    {
      "epoch": 3.3471698113207546,
      "grad_norm": 0.9844260811805725,
      "learning_rate": 0.00016530188679245283,
      "loss": 1.9087,
      "step": 17740
    },
    {
      "epoch": 3.349056603773585,
      "grad_norm": 0.7520501017570496,
      "learning_rate": 0.00016511320754716982,
      "loss": 1.8696,
      "step": 17750
    },
    {
      "epoch": 3.3509433962264152,
      "grad_norm": 0.9677971005439758,
      "learning_rate": 0.00016492452830188677,
      "loss": 1.9791,
      "step": 17760
    },
    {
      "epoch": 3.352830188679245,
      "grad_norm": 0.9419147372245789,
      "learning_rate": 0.00016473584905660378,
      "loss": 1.9527,
      "step": 17770
    },
    {
      "epoch": 3.3547169811320754,
      "grad_norm": 0.8602772355079651,
      "learning_rate": 0.00016454716981132076,
      "loss": 1.8812,
      "step": 17780
    },
    {
      "epoch": 3.3566037735849057,
      "grad_norm": 0.7851124405860901,
      "learning_rate": 0.00016435849056603774,
      "loss": 1.7407,
      "step": 17790
    },
    {
      "epoch": 3.358490566037736,
      "grad_norm": 0.8574419021606445,
      "learning_rate": 0.00016416981132075473,
      "loss": 1.9107,
      "step": 17800
    },
    {
      "epoch": 3.360377358490566,
      "grad_norm": 0.803146243095398,
      "learning_rate": 0.00016398113207547168,
      "loss": 1.8788,
      "step": 17810
    },
    {
      "epoch": 3.362264150943396,
      "grad_norm": 0.8443467020988464,
      "learning_rate": 0.00016379245283018866,
      "loss": 1.8698,
      "step": 17820
    },
    {
      "epoch": 3.3641509433962264,
      "grad_norm": 0.7255287766456604,
      "learning_rate": 0.00016360377358490567,
      "loss": 1.8311,
      "step": 17830
    },
    {
      "epoch": 3.3660377358490567,
      "grad_norm": 0.8137145638465881,
      "learning_rate": 0.00016341509433962265,
      "loss": 1.8202,
      "step": 17840
    },
    {
      "epoch": 3.3679245283018866,
      "grad_norm": 0.7701957821846008,
      "learning_rate": 0.00016322641509433964,
      "loss": 1.9434,
      "step": 17850
    },
    {
      "epoch": 3.369811320754717,
      "grad_norm": 0.7782483100891113,
      "learning_rate": 0.0001630377358490566,
      "loss": 1.8281,
      "step": 17860
    },
    {
      "epoch": 3.371698113207547,
      "grad_norm": 0.7161052823066711,
      "learning_rate": 0.00016284905660377357,
      "loss": 1.9158,
      "step": 17870
    },
    {
      "epoch": 3.3735849056603775,
      "grad_norm": 0.7900659441947937,
      "learning_rate": 0.00016266037735849056,
      "loss": 1.8849,
      "step": 17880
    },
    {
      "epoch": 3.3754716981132074,
      "grad_norm": 0.7283694744110107,
      "learning_rate": 0.00016247169811320757,
      "loss": 1.8563,
      "step": 17890
    },
    {
      "epoch": 3.3773584905660377,
      "grad_norm": 0.8131792545318604,
      "learning_rate": 0.00016228301886792455,
      "loss": 1.8514,
      "step": 17900
    },
    {
      "epoch": 3.379245283018868,
      "grad_norm": 0.7939345836639404,
      "learning_rate": 0.0001620943396226415,
      "loss": 1.8133,
      "step": 17910
    },
    {
      "epoch": 3.3811320754716983,
      "grad_norm": 0.8629750609397888,
      "learning_rate": 0.00016190566037735849,
      "loss": 1.8283,
      "step": 17920
    },
    {
      "epoch": 3.3830188679245285,
      "grad_norm": 0.7872363328933716,
      "learning_rate": 0.00016171698113207547,
      "loss": 1.9084,
      "step": 17930
    },
    {
      "epoch": 3.3849056603773584,
      "grad_norm": 0.7074556350708008,
      "learning_rate": 0.00016152830188679245,
      "loss": 1.7992,
      "step": 17940
    },
    {
      "epoch": 3.3867924528301887,
      "grad_norm": 0.8959633708000183,
      "learning_rate": 0.00016133962264150946,
      "loss": 1.8312,
      "step": 17950
    },
    {
      "epoch": 3.388679245283019,
      "grad_norm": 0.8927733898162842,
      "learning_rate": 0.00016115094339622641,
      "loss": 1.9223,
      "step": 17960
    },
    {
      "epoch": 3.390566037735849,
      "grad_norm": 0.9006034135818481,
      "learning_rate": 0.0001609622641509434,
      "loss": 1.7664,
      "step": 17970
    },
    {
      "epoch": 3.392452830188679,
      "grad_norm": 0.9207081198692322,
      "learning_rate": 0.00016077358490566038,
      "loss": 1.7829,
      "step": 17980
    },
    {
      "epoch": 3.3943396226415095,
      "grad_norm": 0.7958781123161316,
      "learning_rate": 0.00016058490566037736,
      "loss": 1.9013,
      "step": 17990
    },
    {
      "epoch": 3.3962264150943398,
      "grad_norm": 0.8895425200462341,
      "learning_rate": 0.00016039622641509434,
      "loss": 1.8605,
      "step": 18000
    },
    {
      "epoch": 3.39811320754717,
      "grad_norm": 0.8284428715705872,
      "learning_rate": 0.00016020754716981132,
      "loss": 1.8988,
      "step": 18010
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.7081229090690613,
      "learning_rate": 0.0001600188679245283,
      "loss": 1.8256,
      "step": 18020
    },
    {
      "epoch": 3.40188679245283,
      "grad_norm": 0.7828006148338318,
      "learning_rate": 0.0001598301886792453,
      "loss": 1.8267,
      "step": 18030
    },
    {
      "epoch": 3.4037735849056605,
      "grad_norm": 0.7415569424629211,
      "learning_rate": 0.00015964150943396227,
      "loss": 1.8281,
      "step": 18040
    },
    {
      "epoch": 3.4056603773584904,
      "grad_norm": 0.9338963031768799,
      "learning_rate": 0.00015945283018867925,
      "loss": 1.8653,
      "step": 18050
    },
    {
      "epoch": 3.4075471698113207,
      "grad_norm": 0.8269209265708923,
      "learning_rate": 0.0001592641509433962,
      "loss": 1.7995,
      "step": 18060
    },
    {
      "epoch": 3.409433962264151,
      "grad_norm": 0.9357410669326782,
      "learning_rate": 0.00015907547169811322,
      "loss": 1.8429,
      "step": 18070
    },
    {
      "epoch": 3.4113207547169813,
      "grad_norm": 0.7730777263641357,
      "learning_rate": 0.0001588867924528302,
      "loss": 1.8331,
      "step": 18080
    },
    {
      "epoch": 3.4132075471698116,
      "grad_norm": 0.836241602897644,
      "learning_rate": 0.00015869811320754718,
      "loss": 1.8846,
      "step": 18090
    },
    {
      "epoch": 3.4150943396226414,
      "grad_norm": 0.9276447296142578,
      "learning_rate": 0.00015850943396226416,
      "loss": 1.7843,
      "step": 18100
    },
    {
      "epoch": 3.4169811320754717,
      "grad_norm": 0.9151869416236877,
      "learning_rate": 0.00015832075471698112,
      "loss": 1.7641,
      "step": 18110
    },
    {
      "epoch": 3.418867924528302,
      "grad_norm": 0.7655113935470581,
      "learning_rate": 0.0001581320754716981,
      "loss": 1.7429,
      "step": 18120
    },
    {
      "epoch": 3.420754716981132,
      "grad_norm": 0.7801246643066406,
      "learning_rate": 0.0001579433962264151,
      "loss": 1.796,
      "step": 18130
    },
    {
      "epoch": 3.422641509433962,
      "grad_norm": 0.8688569068908691,
      "learning_rate": 0.0001577547169811321,
      "loss": 1.8172,
      "step": 18140
    },
    {
      "epoch": 3.4245283018867925,
      "grad_norm": 0.7520060539245605,
      "learning_rate": 0.00015756603773584907,
      "loss": 1.8868,
      "step": 18150
    },
    {
      "epoch": 3.4264150943396228,
      "grad_norm": 0.8726895451545715,
      "learning_rate": 0.00015737735849056603,
      "loss": 1.9031,
      "step": 18160
    },
    {
      "epoch": 3.428301886792453,
      "grad_norm": 0.8008487820625305,
      "learning_rate": 0.000157188679245283,
      "loss": 1.895,
      "step": 18170
    },
    {
      "epoch": 3.430188679245283,
      "grad_norm": 0.7502877116203308,
      "learning_rate": 0.000157,
      "loss": 1.872,
      "step": 18180
    },
    {
      "epoch": 3.4320754716981132,
      "grad_norm": 0.8076291680335999,
      "learning_rate": 0.000156811320754717,
      "loss": 1.9173,
      "step": 18190
    },
    {
      "epoch": 3.4339622641509435,
      "grad_norm": 0.7352770566940308,
      "learning_rate": 0.00015662264150943399,
      "loss": 1.7949,
      "step": 18200
    },
    {
      "epoch": 3.4358490566037734,
      "grad_norm": 0.816261887550354,
      "learning_rate": 0.00015643396226415094,
      "loss": 1.9603,
      "step": 18210
    },
    {
      "epoch": 3.4377358490566037,
      "grad_norm": 0.8014911413192749,
      "learning_rate": 0.00015624528301886792,
      "loss": 1.8435,
      "step": 18220
    },
    {
      "epoch": 3.439622641509434,
      "grad_norm": 0.8789017200469971,
      "learning_rate": 0.0001560566037735849,
      "loss": 1.9026,
      "step": 18230
    },
    {
      "epoch": 3.4415094339622643,
      "grad_norm": 0.760611891746521,
      "learning_rate": 0.0001558679245283019,
      "loss": 1.8668,
      "step": 18240
    },
    {
      "epoch": 3.4433962264150946,
      "grad_norm": 0.8037618398666382,
      "learning_rate": 0.0001556792452830189,
      "loss": 1.8139,
      "step": 18250
    },
    {
      "epoch": 3.4452830188679244,
      "grad_norm": 0.8315200209617615,
      "learning_rate": 0.00015549056603773585,
      "loss": 1.7947,
      "step": 18260
    },
    {
      "epoch": 3.4471698113207547,
      "grad_norm": 0.7636698484420776,
      "learning_rate": 0.00015530188679245283,
      "loss": 1.8058,
      "step": 18270
    },
    {
      "epoch": 3.449056603773585,
      "grad_norm": 0.7437319159507751,
      "learning_rate": 0.00015511320754716982,
      "loss": 1.914,
      "step": 18280
    },
    {
      "epoch": 3.450943396226415,
      "grad_norm": 0.8259168863296509,
      "learning_rate": 0.0001549245283018868,
      "loss": 1.8862,
      "step": 18290
    },
    {
      "epoch": 3.452830188679245,
      "grad_norm": 0.7520139813423157,
      "learning_rate": 0.00015473584905660378,
      "loss": 1.8332,
      "step": 18300
    },
    {
      "epoch": 3.4547169811320755,
      "grad_norm": 0.794930100440979,
      "learning_rate": 0.00015454716981132074,
      "loss": 1.7614,
      "step": 18310
    },
    {
      "epoch": 3.456603773584906,
      "grad_norm": 0.8780924677848816,
      "learning_rate": 0.00015435849056603774,
      "loss": 1.7945,
      "step": 18320
    },
    {
      "epoch": 3.458490566037736,
      "grad_norm": 0.7887584567070007,
      "learning_rate": 0.00015416981132075473,
      "loss": 1.9106,
      "step": 18330
    },
    {
      "epoch": 3.460377358490566,
      "grad_norm": 0.8169363141059875,
      "learning_rate": 0.0001539811320754717,
      "loss": 1.7879,
      "step": 18340
    },
    {
      "epoch": 3.4622641509433962,
      "grad_norm": 0.8168694972991943,
      "learning_rate": 0.0001537924528301887,
      "loss": 1.7871,
      "step": 18350
    },
    {
      "epoch": 3.4641509433962265,
      "grad_norm": 0.883516788482666,
      "learning_rate": 0.00015360377358490565,
      "loss": 1.8292,
      "step": 18360
    },
    {
      "epoch": 3.4660377358490564,
      "grad_norm": 0.8207910060882568,
      "learning_rate": 0.00015341509433962263,
      "loss": 1.898,
      "step": 18370
    },
    {
      "epoch": 3.4679245283018867,
      "grad_norm": 0.8631359934806824,
      "learning_rate": 0.00015322641509433964,
      "loss": 1.9255,
      "step": 18380
    },
    {
      "epoch": 3.469811320754717,
      "grad_norm": 0.7712914943695068,
      "learning_rate": 0.00015303773584905662,
      "loss": 1.8777,
      "step": 18390
    },
    {
      "epoch": 3.4716981132075473,
      "grad_norm": 0.8101208806037903,
      "learning_rate": 0.0001528490566037736,
      "loss": 1.8404,
      "step": 18400
    },
    {
      "epoch": 3.4735849056603776,
      "grad_norm": 0.8740575909614563,
      "learning_rate": 0.00015266037735849056,
      "loss": 1.8821,
      "step": 18410
    },
    {
      "epoch": 3.4754716981132074,
      "grad_norm": 0.7664214968681335,
      "learning_rate": 0.00015247169811320754,
      "loss": 1.9583,
      "step": 18420
    },
    {
      "epoch": 3.4773584905660377,
      "grad_norm": 0.8029385209083557,
      "learning_rate": 0.00015228301886792452,
      "loss": 1.818,
      "step": 18430
    },
    {
      "epoch": 3.479245283018868,
      "grad_norm": 0.8278443813323975,
      "learning_rate": 0.00015209433962264153,
      "loss": 1.907,
      "step": 18440
    },
    {
      "epoch": 3.481132075471698,
      "grad_norm": 0.8247520327568054,
      "learning_rate": 0.0001519056603773585,
      "loss": 1.8702,
      "step": 18450
    },
    {
      "epoch": 3.483018867924528,
      "grad_norm": 0.8039612770080566,
      "learning_rate": 0.00015171698113207547,
      "loss": 1.8454,
      "step": 18460
    },
    {
      "epoch": 3.4849056603773585,
      "grad_norm": 0.836695671081543,
      "learning_rate": 0.00015152830188679245,
      "loss": 1.9025,
      "step": 18470
    },
    {
      "epoch": 3.486792452830189,
      "grad_norm": 0.8984798789024353,
      "learning_rate": 0.00015133962264150943,
      "loss": 1.8257,
      "step": 18480
    },
    {
      "epoch": 3.488679245283019,
      "grad_norm": 1.0927540063858032,
      "learning_rate": 0.00015115094339622641,
      "loss": 1.8202,
      "step": 18490
    },
    {
      "epoch": 3.490566037735849,
      "grad_norm": 0.852000892162323,
      "learning_rate": 0.00015096226415094342,
      "loss": 1.8294,
      "step": 18500
    },
    {
      "epoch": 3.4924528301886792,
      "grad_norm": 0.778105616569519,
      "learning_rate": 0.00015077358490566038,
      "loss": 1.786,
      "step": 18510
    },
    {
      "epoch": 3.4943396226415095,
      "grad_norm": 0.8741381764411926,
      "learning_rate": 0.00015058490566037736,
      "loss": 1.9312,
      "step": 18520
    },
    {
      "epoch": 3.4962264150943394,
      "grad_norm": 0.6588963270187378,
      "learning_rate": 0.00015039622641509434,
      "loss": 1.7675,
      "step": 18530
    },
    {
      "epoch": 3.4981132075471697,
      "grad_norm": 0.7201229333877563,
      "learning_rate": 0.00015020754716981133,
      "loss": 1.8446,
      "step": 18540
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.8604983687400818,
      "learning_rate": 0.0001500188679245283,
      "loss": 1.8966,
      "step": 18550
    },
    {
      "epoch": 3.5018867924528303,
      "grad_norm": 0.7127488255500793,
      "learning_rate": 0.0001498301886792453,
      "loss": 1.8393,
      "step": 18560
    },
    {
      "epoch": 3.5037735849056606,
      "grad_norm": 0.8492354154586792,
      "learning_rate": 0.00014964150943396227,
      "loss": 1.8625,
      "step": 18570
    },
    {
      "epoch": 3.5056603773584905,
      "grad_norm": 0.7520440816879272,
      "learning_rate": 0.00014945283018867925,
      "loss": 1.7974,
      "step": 18580
    },
    {
      "epoch": 3.5075471698113208,
      "grad_norm": 0.7762837409973145,
      "learning_rate": 0.00014926415094339624,
      "loss": 1.7959,
      "step": 18590
    },
    {
      "epoch": 3.509433962264151,
      "grad_norm": 0.7794302701950073,
      "learning_rate": 0.00014907547169811322,
      "loss": 1.8846,
      "step": 18600
    },
    {
      "epoch": 3.511320754716981,
      "grad_norm": 0.8033839464187622,
      "learning_rate": 0.00014888679245283017,
      "loss": 1.837,
      "step": 18610
    },
    {
      "epoch": 3.513207547169811,
      "grad_norm": 0.8704739809036255,
      "learning_rate": 0.00014869811320754718,
      "loss": 1.9129,
      "step": 18620
    },
    {
      "epoch": 3.5150943396226415,
      "grad_norm": 0.8611562848091125,
      "learning_rate": 0.00014850943396226416,
      "loss": 1.9074,
      "step": 18630
    },
    {
      "epoch": 3.516981132075472,
      "grad_norm": 0.7778855562210083,
      "learning_rate": 0.00014832075471698115,
      "loss": 1.8628,
      "step": 18640
    },
    {
      "epoch": 3.518867924528302,
      "grad_norm": 0.7742135524749756,
      "learning_rate": 0.00014813207547169813,
      "loss": 1.8092,
      "step": 18650
    },
    {
      "epoch": 3.520754716981132,
      "grad_norm": 0.8657431602478027,
      "learning_rate": 0.00014794339622641508,
      "loss": 1.9034,
      "step": 18660
    },
    {
      "epoch": 3.5226415094339623,
      "grad_norm": 0.7133699059486389,
      "learning_rate": 0.00014775471698113207,
      "loss": 1.8299,
      "step": 18670
    },
    {
      "epoch": 3.5245283018867926,
      "grad_norm": 0.811030387878418,
      "learning_rate": 0.00014756603773584908,
      "loss": 1.8997,
      "step": 18680
    },
    {
      "epoch": 3.5264150943396224,
      "grad_norm": 0.8533532619476318,
      "learning_rate": 0.00014737735849056606,
      "loss": 1.8749,
      "step": 18690
    },
    {
      "epoch": 3.5283018867924527,
      "grad_norm": 0.90358567237854,
      "learning_rate": 0.000147188679245283,
      "loss": 1.8782,
      "step": 18700
    },
    {
      "epoch": 3.530188679245283,
      "grad_norm": 0.9515085220336914,
      "learning_rate": 0.000147,
      "loss": 1.8276,
      "step": 18710
    },
    {
      "epoch": 3.5320754716981133,
      "grad_norm": 0.797546923160553,
      "learning_rate": 0.00014681132075471698,
      "loss": 1.8061,
      "step": 18720
    },
    {
      "epoch": 3.5339622641509436,
      "grad_norm": 0.8727725148200989,
      "learning_rate": 0.00014662264150943396,
      "loss": 1.8247,
      "step": 18730
    },
    {
      "epoch": 3.5358490566037735,
      "grad_norm": 0.8800103664398193,
      "learning_rate": 0.00014643396226415094,
      "loss": 1.8334,
      "step": 18740
    },
    {
      "epoch": 3.5377358490566038,
      "grad_norm": 0.7817875146865845,
      "learning_rate": 0.00014624528301886792,
      "loss": 1.9513,
      "step": 18750
    },
    {
      "epoch": 3.539622641509434,
      "grad_norm": 0.8307719230651855,
      "learning_rate": 0.0001460566037735849,
      "loss": 1.8,
      "step": 18760
    },
    {
      "epoch": 3.541509433962264,
      "grad_norm": 0.9027508497238159,
      "learning_rate": 0.0001458679245283019,
      "loss": 1.8268,
      "step": 18770
    },
    {
      "epoch": 3.543396226415094,
      "grad_norm": 0.9806700944900513,
      "learning_rate": 0.00014567924528301887,
      "loss": 1.8092,
      "step": 18780
    },
    {
      "epoch": 3.5452830188679245,
      "grad_norm": 0.9074327349662781,
      "learning_rate": 0.00014549056603773585,
      "loss": 1.9787,
      "step": 18790
    },
    {
      "epoch": 3.547169811320755,
      "grad_norm": 0.9091421961784363,
      "learning_rate": 0.0001453018867924528,
      "loss": 1.8729,
      "step": 18800
    },
    {
      "epoch": 3.549056603773585,
      "grad_norm": 0.7409951686859131,
      "learning_rate": 0.00014511320754716982,
      "loss": 1.9174,
      "step": 18810
    },
    {
      "epoch": 3.550943396226415,
      "grad_norm": 0.8255106806755066,
      "learning_rate": 0.0001449245283018868,
      "loss": 1.8577,
      "step": 18820
    },
    {
      "epoch": 3.5528301886792453,
      "grad_norm": 0.8531159162521362,
      "learning_rate": 0.00014473584905660378,
      "loss": 1.84,
      "step": 18830
    },
    {
      "epoch": 3.5547169811320756,
      "grad_norm": 0.9354495406150818,
      "learning_rate": 0.00014454716981132076,
      "loss": 1.7975,
      "step": 18840
    },
    {
      "epoch": 3.5566037735849054,
      "grad_norm": 0.8251839876174927,
      "learning_rate": 0.00014435849056603772,
      "loss": 1.7973,
      "step": 18850
    },
    {
      "epoch": 3.5584905660377357,
      "grad_norm": 0.7682227492332458,
      "learning_rate": 0.0001441698113207547,
      "loss": 1.9074,
      "step": 18860
    },
    {
      "epoch": 3.560377358490566,
      "grad_norm": 0.8584514856338501,
      "learning_rate": 0.0001439811320754717,
      "loss": 1.9128,
      "step": 18870
    },
    {
      "epoch": 3.5622641509433963,
      "grad_norm": 0.8938374519348145,
      "learning_rate": 0.0001437924528301887,
      "loss": 1.8467,
      "step": 18880
    },
    {
      "epoch": 3.5641509433962266,
      "grad_norm": 0.830817699432373,
      "learning_rate": 0.00014360377358490567,
      "loss": 1.8489,
      "step": 18890
    },
    {
      "epoch": 3.5660377358490565,
      "grad_norm": 0.8007476329803467,
      "learning_rate": 0.00014341509433962263,
      "loss": 1.8546,
      "step": 18900
    },
    {
      "epoch": 3.5679245283018868,
      "grad_norm": 0.8577645421028137,
      "learning_rate": 0.0001432264150943396,
      "loss": 1.8019,
      "step": 18910
    },
    {
      "epoch": 3.569811320754717,
      "grad_norm": 0.8448774814605713,
      "learning_rate": 0.0001430377358490566,
      "loss": 1.8944,
      "step": 18920
    },
    {
      "epoch": 3.571698113207547,
      "grad_norm": 0.8034762740135193,
      "learning_rate": 0.0001428490566037736,
      "loss": 1.8583,
      "step": 18930
    },
    {
      "epoch": 3.5735849056603772,
      "grad_norm": 0.8086367845535278,
      "learning_rate": 0.00014266037735849059,
      "loss": 1.8101,
      "step": 18940
    },
    {
      "epoch": 3.5754716981132075,
      "grad_norm": 0.7341545820236206,
      "learning_rate": 0.00014247169811320754,
      "loss": 1.8647,
      "step": 18950
    },
    {
      "epoch": 3.577358490566038,
      "grad_norm": 0.7621036767959595,
      "learning_rate": 0.00014228301886792452,
      "loss": 1.8334,
      "step": 18960
    },
    {
      "epoch": 3.579245283018868,
      "grad_norm": 0.7504666447639465,
      "learning_rate": 0.0001420943396226415,
      "loss": 1.7736,
      "step": 18970
    },
    {
      "epoch": 3.581132075471698,
      "grad_norm": 0.7761404514312744,
      "learning_rate": 0.0001419056603773585,
      "loss": 1.793,
      "step": 18980
    },
    {
      "epoch": 3.5830188679245283,
      "grad_norm": 0.8181442022323608,
      "learning_rate": 0.0001417169811320755,
      "loss": 1.8727,
      "step": 18990
    },
    {
      "epoch": 3.5849056603773586,
      "grad_norm": 0.7925540208816528,
      "learning_rate": 0.00014152830188679245,
      "loss": 1.8282,
      "step": 19000
    },
    {
      "epoch": 3.5867924528301884,
      "grad_norm": 0.8099820613861084,
      "learning_rate": 0.00014133962264150943,
      "loss": 1.7314,
      "step": 19010
    },
    {
      "epoch": 3.5886792452830187,
      "grad_norm": 0.7550435066223145,
      "learning_rate": 0.00014115094339622642,
      "loss": 1.8652,
      "step": 19020
    },
    {
      "epoch": 3.590566037735849,
      "grad_norm": 0.7638104557991028,
      "learning_rate": 0.0001409622641509434,
      "loss": 1.8322,
      "step": 19030
    },
    {
      "epoch": 3.5924528301886793,
      "grad_norm": 0.7843398451805115,
      "learning_rate": 0.00014077358490566038,
      "loss": 1.7837,
      "step": 19040
    },
    {
      "epoch": 3.5943396226415096,
      "grad_norm": 0.7425926327705383,
      "learning_rate": 0.00014058490566037736,
      "loss": 1.8787,
      "step": 19050
    },
    {
      "epoch": 3.5962264150943395,
      "grad_norm": 0.8458809852600098,
      "learning_rate": 0.00014039622641509434,
      "loss": 1.8503,
      "step": 19060
    },
    {
      "epoch": 3.59811320754717,
      "grad_norm": 0.8191152811050415,
      "learning_rate": 0.00014020754716981133,
      "loss": 1.8471,
      "step": 19070
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.8167368769645691,
      "learning_rate": 0.0001400188679245283,
      "loss": 1.8189,
      "step": 19080
    },
    {
      "epoch": 3.60188679245283,
      "grad_norm": 0.7898261547088623,
      "learning_rate": 0.0001398301886792453,
      "loss": 1.8664,
      "step": 19090
    },
    {
      "epoch": 3.6037735849056602,
      "grad_norm": 0.8651010394096375,
      "learning_rate": 0.00013964150943396225,
      "loss": 1.9161,
      "step": 19100
    },
    {
      "epoch": 3.6056603773584905,
      "grad_norm": 0.744470477104187,
      "learning_rate": 0.00013945283018867926,
      "loss": 1.8839,
      "step": 19110
    },
    {
      "epoch": 3.607547169811321,
      "grad_norm": 0.8804255127906799,
      "learning_rate": 0.00013926415094339624,
      "loss": 1.9337,
      "step": 19120
    },
    {
      "epoch": 3.609433962264151,
      "grad_norm": 0.7689313292503357,
      "learning_rate": 0.00013907547169811322,
      "loss": 1.9373,
      "step": 19130
    },
    {
      "epoch": 3.611320754716981,
      "grad_norm": 0.8847023248672485,
      "learning_rate": 0.0001388867924528302,
      "loss": 1.9136,
      "step": 19140
    },
    {
      "epoch": 3.6132075471698113,
      "grad_norm": 0.814574122428894,
      "learning_rate": 0.00013869811320754716,
      "loss": 1.7596,
      "step": 19150
    },
    {
      "epoch": 3.6150943396226416,
      "grad_norm": 0.7225764989852905,
      "learning_rate": 0.00013850943396226414,
      "loss": 1.9417,
      "step": 19160
    },
    {
      "epoch": 3.6169811320754715,
      "grad_norm": 0.8115091323852539,
      "learning_rate": 0.00013832075471698112,
      "loss": 1.8962,
      "step": 19170
    },
    {
      "epoch": 3.6188679245283017,
      "grad_norm": 0.8381028175354004,
      "learning_rate": 0.00013813207547169813,
      "loss": 1.9147,
      "step": 19180
    },
    {
      "epoch": 3.620754716981132,
      "grad_norm": 0.7499554753303528,
      "learning_rate": 0.0001379433962264151,
      "loss": 1.9092,
      "step": 19190
    },
    {
      "epoch": 3.6226415094339623,
      "grad_norm": 0.8333317637443542,
      "learning_rate": 0.00013775471698113207,
      "loss": 1.8467,
      "step": 19200
    },
    {
      "epoch": 3.6245283018867926,
      "grad_norm": 0.7717934250831604,
      "learning_rate": 0.00013756603773584905,
      "loss": 1.7082,
      "step": 19210
    },
    {
      "epoch": 3.6264150943396225,
      "grad_norm": 0.8367282152175903,
      "learning_rate": 0.00013737735849056603,
      "loss": 1.8063,
      "step": 19220
    },
    {
      "epoch": 3.628301886792453,
      "grad_norm": 0.7931785583496094,
      "learning_rate": 0.00013718867924528301,
      "loss": 1.8964,
      "step": 19230
    },
    {
      "epoch": 3.630188679245283,
      "grad_norm": 0.9493769407272339,
      "learning_rate": 0.00013700000000000002,
      "loss": 1.8408,
      "step": 19240
    },
    {
      "epoch": 3.632075471698113,
      "grad_norm": 0.7996138334274292,
      "learning_rate": 0.00013681132075471698,
      "loss": 1.8434,
      "step": 19250
    },
    {
      "epoch": 3.6339622641509433,
      "grad_norm": 0.8737677931785583,
      "learning_rate": 0.00013662264150943396,
      "loss": 1.8495,
      "step": 19260
    },
    {
      "epoch": 3.6358490566037736,
      "grad_norm": 0.8712400794029236,
      "learning_rate": 0.00013643396226415094,
      "loss": 1.832,
      "step": 19270
    },
    {
      "epoch": 3.637735849056604,
      "grad_norm": 0.7956781387329102,
      "learning_rate": 0.00013624528301886792,
      "loss": 1.8167,
      "step": 19280
    },
    {
      "epoch": 3.639622641509434,
      "grad_norm": 0.7901103496551514,
      "learning_rate": 0.0001360566037735849,
      "loss": 1.7989,
      "step": 19290
    },
    {
      "epoch": 3.641509433962264,
      "grad_norm": 0.6925720572471619,
      "learning_rate": 0.0001358679245283019,
      "loss": 1.8641,
      "step": 19300
    },
    {
      "epoch": 3.6433962264150943,
      "grad_norm": 0.7473238110542297,
      "learning_rate": 0.00013567924528301887,
      "loss": 1.8433,
      "step": 19310
    },
    {
      "epoch": 3.6452830188679246,
      "grad_norm": 0.8973062634468079,
      "learning_rate": 0.00013549056603773585,
      "loss": 1.719,
      "step": 19320
    },
    {
      "epoch": 3.6471698113207545,
      "grad_norm": 0.7944762706756592,
      "learning_rate": 0.00013530188679245284,
      "loss": 1.9158,
      "step": 19330
    },
    {
      "epoch": 3.6490566037735848,
      "grad_norm": 0.8672405481338501,
      "learning_rate": 0.00013511320754716982,
      "loss": 1.9015,
      "step": 19340
    },
    {
      "epoch": 3.650943396226415,
      "grad_norm": 0.8334699273109436,
      "learning_rate": 0.00013492452830188677,
      "loss": 1.7949,
      "step": 19350
    },
    {
      "epoch": 3.6528301886792454,
      "grad_norm": 0.8840474486351013,
      "learning_rate": 0.00013473584905660378,
      "loss": 1.787,
      "step": 19360
    },
    {
      "epoch": 3.6547169811320757,
      "grad_norm": 0.8275963068008423,
      "learning_rate": 0.00013454716981132076,
      "loss": 1.7283,
      "step": 19370
    },
    {
      "epoch": 3.6566037735849055,
      "grad_norm": 0.8198412656784058,
      "learning_rate": 0.00013435849056603775,
      "loss": 1.7783,
      "step": 19380
    },
    {
      "epoch": 3.658490566037736,
      "grad_norm": 0.8467249870300293,
      "learning_rate": 0.00013416981132075473,
      "loss": 1.8544,
      "step": 19390
    },
    {
      "epoch": 3.660377358490566,
      "grad_norm": 1.078765869140625,
      "learning_rate": 0.00013398113207547168,
      "loss": 1.7814,
      "step": 19400
    },
    {
      "epoch": 3.662264150943396,
      "grad_norm": 0.9033663272857666,
      "learning_rate": 0.00013379245283018867,
      "loss": 1.9272,
      "step": 19410
    },
    {
      "epoch": 3.6641509433962263,
      "grad_norm": 0.8090852499008179,
      "learning_rate": 0.00013360377358490568,
      "loss": 1.8468,
      "step": 19420
    },
    {
      "epoch": 3.6660377358490566,
      "grad_norm": 0.8092348575592041,
      "learning_rate": 0.00013341509433962266,
      "loss": 1.8172,
      "step": 19430
    },
    {
      "epoch": 3.667924528301887,
      "grad_norm": 0.7907317876815796,
      "learning_rate": 0.00013322641509433964,
      "loss": 1.8047,
      "step": 19440
    },
    {
      "epoch": 3.669811320754717,
      "grad_norm": 0.7668476700782776,
      "learning_rate": 0.0001330377358490566,
      "loss": 1.9242,
      "step": 19450
    },
    {
      "epoch": 3.671698113207547,
      "grad_norm": 0.7880358099937439,
      "learning_rate": 0.00013284905660377358,
      "loss": 1.8525,
      "step": 19460
    },
    {
      "epoch": 3.6735849056603773,
      "grad_norm": 0.827298104763031,
      "learning_rate": 0.00013266037735849056,
      "loss": 1.74,
      "step": 19470
    },
    {
      "epoch": 3.6754716981132076,
      "grad_norm": 0.799538791179657,
      "learning_rate": 0.00013247169811320757,
      "loss": 1.8256,
      "step": 19480
    },
    {
      "epoch": 3.6773584905660375,
      "grad_norm": 0.9143769145011902,
      "learning_rate": 0.00013228301886792455,
      "loss": 1.7836,
      "step": 19490
    },
    {
      "epoch": 3.6792452830188678,
      "grad_norm": 0.8729014992713928,
      "learning_rate": 0.0001320943396226415,
      "loss": 1.8913,
      "step": 19500
    },
    {
      "epoch": 3.681132075471698,
      "grad_norm": 0.7960715889930725,
      "learning_rate": 0.0001319056603773585,
      "loss": 1.77,
      "step": 19510
    },
    {
      "epoch": 3.6830188679245284,
      "grad_norm": 0.8624520897865295,
      "learning_rate": 0.00013171698113207547,
      "loss": 1.8901,
      "step": 19520
    },
    {
      "epoch": 3.6849056603773587,
      "grad_norm": 0.8056563138961792,
      "learning_rate": 0.00013152830188679245,
      "loss": 1.9258,
      "step": 19530
    },
    {
      "epoch": 3.6867924528301885,
      "grad_norm": 0.7775716781616211,
      "learning_rate": 0.00013133962264150946,
      "loss": 1.8762,
      "step": 19540
    },
    {
      "epoch": 3.688679245283019,
      "grad_norm": 0.8306500315666199,
      "learning_rate": 0.00013115094339622642,
      "loss": 1.9537,
      "step": 19550
    },
    {
      "epoch": 3.690566037735849,
      "grad_norm": 0.8021546006202698,
      "learning_rate": 0.0001309622641509434,
      "loss": 1.8541,
      "step": 19560
    },
    {
      "epoch": 3.692452830188679,
      "grad_norm": 0.8121210932731628,
      "learning_rate": 0.00013077358490566038,
      "loss": 1.983,
      "step": 19570
    },
    {
      "epoch": 3.6943396226415093,
      "grad_norm": 0.7657583951950073,
      "learning_rate": 0.00013058490566037736,
      "loss": 1.9218,
      "step": 19580
    },
    {
      "epoch": 3.6962264150943396,
      "grad_norm": 0.895125687122345,
      "learning_rate": 0.00013039622641509435,
      "loss": 1.8849,
      "step": 19590
    },
    {
      "epoch": 3.69811320754717,
      "grad_norm": 0.777866542339325,
      "learning_rate": 0.0001302075471698113,
      "loss": 1.7886,
      "step": 19600
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.7711963057518005,
      "learning_rate": 0.0001300188679245283,
      "loss": 1.8567,
      "step": 19610
    },
    {
      "epoch": 3.70188679245283,
      "grad_norm": 0.7455881834030151,
      "learning_rate": 0.0001298301886792453,
      "loss": 1.8626,
      "step": 19620
    },
    {
      "epoch": 3.7037735849056603,
      "grad_norm": 0.7257091999053955,
      "learning_rate": 0.00012964150943396227,
      "loss": 1.8454,
      "step": 19630
    },
    {
      "epoch": 3.7056603773584906,
      "grad_norm": 0.8169651627540588,
      "learning_rate": 0.00012945283018867926,
      "loss": 1.8514,
      "step": 19640
    },
    {
      "epoch": 3.7075471698113205,
      "grad_norm": 0.8783521056175232,
      "learning_rate": 0.0001292641509433962,
      "loss": 1.827,
      "step": 19650
    },
    {
      "epoch": 3.709433962264151,
      "grad_norm": 0.8177294731140137,
      "learning_rate": 0.0001290754716981132,
      "loss": 1.8695,
      "step": 19660
    },
    {
      "epoch": 3.711320754716981,
      "grad_norm": 0.8913573026657104,
      "learning_rate": 0.0001288867924528302,
      "loss": 1.9977,
      "step": 19670
    },
    {
      "epoch": 3.7132075471698114,
      "grad_norm": 0.8103871941566467,
      "learning_rate": 0.00012869811320754718,
      "loss": 1.8537,
      "step": 19680
    },
    {
      "epoch": 3.7150943396226417,
      "grad_norm": 0.7412546277046204,
      "learning_rate": 0.00012850943396226417,
      "loss": 1.8416,
      "step": 19690
    },
    {
      "epoch": 3.7169811320754715,
      "grad_norm": 0.8316771984100342,
      "learning_rate": 0.00012832075471698112,
      "loss": 1.8681,
      "step": 19700
    },
    {
      "epoch": 3.718867924528302,
      "grad_norm": 0.7182799577713013,
      "learning_rate": 0.0001281320754716981,
      "loss": 1.78,
      "step": 19710
    },
    {
      "epoch": 3.720754716981132,
      "grad_norm": 0.7669441103935242,
      "learning_rate": 0.00012794339622641509,
      "loss": 1.8027,
      "step": 19720
    },
    {
      "epoch": 3.722641509433962,
      "grad_norm": 0.8922712206840515,
      "learning_rate": 0.0001277547169811321,
      "loss": 1.8251,
      "step": 19730
    },
    {
      "epoch": 3.7245283018867923,
      "grad_norm": 0.9320836067199707,
      "learning_rate": 0.00012756603773584908,
      "loss": 1.8893,
      "step": 19740
    },
    {
      "epoch": 3.7264150943396226,
      "grad_norm": 0.8247078061103821,
      "learning_rate": 0.00012737735849056603,
      "loss": 1.7781,
      "step": 19750
    },
    {
      "epoch": 3.728301886792453,
      "grad_norm": 0.8301324248313904,
      "learning_rate": 0.00012718867924528301,
      "loss": 1.8412,
      "step": 19760
    },
    {
      "epoch": 3.730188679245283,
      "grad_norm": 0.7964597344398499,
      "learning_rate": 0.000127,
      "loss": 1.8711,
      "step": 19770
    },
    {
      "epoch": 3.732075471698113,
      "grad_norm": 0.7368829250335693,
      "learning_rate": 0.00012681132075471698,
      "loss": 1.8045,
      "step": 19780
    },
    {
      "epoch": 3.7339622641509433,
      "grad_norm": 0.7838090658187866,
      "learning_rate": 0.000126622641509434,
      "loss": 1.9128,
      "step": 19790
    },
    {
      "epoch": 3.7358490566037736,
      "grad_norm": 0.8256115913391113,
      "learning_rate": 0.00012643396226415094,
      "loss": 1.9046,
      "step": 19800
    },
    {
      "epoch": 3.7377358490566035,
      "grad_norm": 0.8850380778312683,
      "learning_rate": 0.00012624528301886793,
      "loss": 1.9394,
      "step": 19810
    },
    {
      "epoch": 3.739622641509434,
      "grad_norm": 0.8408551216125488,
      "learning_rate": 0.0001260566037735849,
      "loss": 1.886,
      "step": 19820
    },
    {
      "epoch": 3.741509433962264,
      "grad_norm": 0.8066020011901855,
      "learning_rate": 0.0001258679245283019,
      "loss": 1.7911,
      "step": 19830
    },
    {
      "epoch": 3.7433962264150944,
      "grad_norm": 0.8336591720581055,
      "learning_rate": 0.00012567924528301887,
      "loss": 1.8337,
      "step": 19840
    },
    {
      "epoch": 3.7452830188679247,
      "grad_norm": 0.8518580794334412,
      "learning_rate": 0.00012549056603773585,
      "loss": 1.933,
      "step": 19850
    },
    {
      "epoch": 3.7471698113207546,
      "grad_norm": 0.7780158519744873,
      "learning_rate": 0.00012530188679245284,
      "loss": 1.8515,
      "step": 19860
    },
    {
      "epoch": 3.749056603773585,
      "grad_norm": 0.7546852231025696,
      "learning_rate": 0.00012511320754716982,
      "loss": 1.8724,
      "step": 19870
    },
    {
      "epoch": 3.750943396226415,
      "grad_norm": 0.8595632910728455,
      "learning_rate": 0.0001249245283018868,
      "loss": 1.8254,
      "step": 19880
    },
    {
      "epoch": 3.7528301886792454,
      "grad_norm": 0.8225597143173218,
      "learning_rate": 0.00012473584905660378,
      "loss": 1.7939,
      "step": 19890
    },
    {
      "epoch": 3.7547169811320753,
      "grad_norm": 0.86504065990448,
      "learning_rate": 0.00012454716981132077,
      "loss": 1.8033,
      "step": 19900
    },
    {
      "epoch": 3.7566037735849056,
      "grad_norm": 0.7335496544837952,
      "learning_rate": 0.00012435849056603775,
      "loss": 1.8465,
      "step": 19910
    },
    {
      "epoch": 3.758490566037736,
      "grad_norm": 0.8151665925979614,
      "learning_rate": 0.00012416981132075473,
      "loss": 1.8943,
      "step": 19920
    },
    {
      "epoch": 3.760377358490566,
      "grad_norm": 0.7993392944335938,
      "learning_rate": 0.00012398113207547168,
      "loss": 1.9508,
      "step": 19930
    },
    {
      "epoch": 3.7622641509433965,
      "grad_norm": 0.741899847984314,
      "learning_rate": 0.0001237924528301887,
      "loss": 1.8499,
      "step": 19940
    },
    {
      "epoch": 3.7641509433962264,
      "grad_norm": 0.8401479721069336,
      "learning_rate": 0.00012360377358490568,
      "loss": 1.8939,
      "step": 19950
    },
    {
      "epoch": 3.7660377358490567,
      "grad_norm": 0.8917144536972046,
      "learning_rate": 0.00012341509433962263,
      "loss": 1.9593,
      "step": 19960
    },
    {
      "epoch": 3.767924528301887,
      "grad_norm": 0.8207452297210693,
      "learning_rate": 0.00012322641509433964,
      "loss": 1.803,
      "step": 19970
    },
    {
      "epoch": 3.769811320754717,
      "grad_norm": 0.7837172150611877,
      "learning_rate": 0.0001230377358490566,
      "loss": 1.8596,
      "step": 19980
    },
    {
      "epoch": 3.771698113207547,
      "grad_norm": 0.8783859610557556,
      "learning_rate": 0.00012284905660377358,
      "loss": 1.8973,
      "step": 19990
    },
    {
      "epoch": 3.7735849056603774,
      "grad_norm": 0.7788439989089966,
      "learning_rate": 0.0001226603773584906,
      "loss": 1.858,
      "step": 20000
    },
    {
      "epoch": 3.7754716981132077,
      "grad_norm": 0.7743694186210632,
      "learning_rate": 0.00012247169811320754,
      "loss": 1.9009,
      "step": 20010
    },
    {
      "epoch": 3.777358490566038,
      "grad_norm": 0.7923519611358643,
      "learning_rate": 0.00012228301886792452,
      "loss": 1.8846,
      "step": 20020
    },
    {
      "epoch": 3.779245283018868,
      "grad_norm": 0.7599974870681763,
      "learning_rate": 0.0001220943396226415,
      "loss": 1.9131,
      "step": 20030
    },
    {
      "epoch": 3.781132075471698,
      "grad_norm": 0.9504392147064209,
      "learning_rate": 0.00012190566037735849,
      "loss": 1.8427,
      "step": 20040
    },
    {
      "epoch": 3.7830188679245285,
      "grad_norm": 0.8458485007286072,
      "learning_rate": 0.00012171698113207548,
      "loss": 1.8923,
      "step": 20050
    },
    {
      "epoch": 3.7849056603773583,
      "grad_norm": 0.7921173572540283,
      "learning_rate": 0.00012152830188679245,
      "loss": 1.8473,
      "step": 20060
    },
    {
      "epoch": 3.7867924528301886,
      "grad_norm": 0.7543637156486511,
      "learning_rate": 0.00012133962264150944,
      "loss": 1.7845,
      "step": 20070
    },
    {
      "epoch": 3.788679245283019,
      "grad_norm": 0.8270566463470459,
      "learning_rate": 0.00012115094339622642,
      "loss": 1.8788,
      "step": 20080
    },
    {
      "epoch": 3.790566037735849,
      "grad_norm": 0.7568537592887878,
      "learning_rate": 0.0001209622641509434,
      "loss": 1.8982,
      "step": 20090
    },
    {
      "epoch": 3.7924528301886795,
      "grad_norm": 0.8644561767578125,
      "learning_rate": 0.00012077358490566038,
      "loss": 1.8653,
      "step": 20100
    },
    {
      "epoch": 3.7943396226415094,
      "grad_norm": 0.8139064908027649,
      "learning_rate": 0.00012058490566037736,
      "loss": 1.8007,
      "step": 20110
    },
    {
      "epoch": 3.7962264150943397,
      "grad_norm": 0.8151786923408508,
      "learning_rate": 0.00012039622641509435,
      "loss": 1.9084,
      "step": 20120
    },
    {
      "epoch": 3.79811320754717,
      "grad_norm": 0.977019727230072,
      "learning_rate": 0.00012020754716981131,
      "loss": 1.8034,
      "step": 20130
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.8560419678688049,
      "learning_rate": 0.00012001886792452831,
      "loss": 1.7846,
      "step": 20140
    },
    {
      "epoch": 3.80188679245283,
      "grad_norm": 0.7887096405029297,
      "learning_rate": 0.00011983018867924529,
      "loss": 1.8382,
      "step": 20150
    },
    {
      "epoch": 3.8037735849056604,
      "grad_norm": 0.7635801434516907,
      "learning_rate": 0.00011964150943396226,
      "loss": 1.8004,
      "step": 20160
    },
    {
      "epoch": 3.8056603773584907,
      "grad_norm": 0.7739700078964233,
      "learning_rate": 0.00011945283018867926,
      "loss": 1.8302,
      "step": 20170
    },
    {
      "epoch": 3.807547169811321,
      "grad_norm": 0.8848263025283813,
      "learning_rate": 0.00011926415094339623,
      "loss": 1.7648,
      "step": 20180
    },
    {
      "epoch": 3.809433962264151,
      "grad_norm": 0.7217925190925598,
      "learning_rate": 0.00011907547169811321,
      "loss": 1.8198,
      "step": 20190
    },
    {
      "epoch": 3.811320754716981,
      "grad_norm": 0.8910982608795166,
      "learning_rate": 0.0001188867924528302,
      "loss": 1.8747,
      "step": 20200
    },
    {
      "epoch": 3.8132075471698115,
      "grad_norm": 0.8778749704360962,
      "learning_rate": 0.00011869811320754717,
      "loss": 1.913,
      "step": 20210
    },
    {
      "epoch": 3.8150943396226413,
      "grad_norm": 0.9346067309379578,
      "learning_rate": 0.00011850943396226415,
      "loss": 1.7614,
      "step": 20220
    },
    {
      "epoch": 3.8169811320754716,
      "grad_norm": 0.8386136889457703,
      "learning_rate": 0.00011832075471698114,
      "loss": 1.8719,
      "step": 20230
    },
    {
      "epoch": 3.818867924528302,
      "grad_norm": 0.7821153998374939,
      "learning_rate": 0.00011813207547169812,
      "loss": 1.7885,
      "step": 20240
    },
    {
      "epoch": 3.8207547169811322,
      "grad_norm": 0.7133088111877441,
      "learning_rate": 0.0001179433962264151,
      "loss": 1.8238,
      "step": 20250
    },
    {
      "epoch": 3.8226415094339625,
      "grad_norm": 0.797978401184082,
      "learning_rate": 0.00011775471698113208,
      "loss": 1.9208,
      "step": 20260
    },
    {
      "epoch": 3.8245283018867924,
      "grad_norm": 0.892734944820404,
      "learning_rate": 0.00011756603773584906,
      "loss": 1.8336,
      "step": 20270
    },
    {
      "epoch": 3.8264150943396227,
      "grad_norm": 0.7505276203155518,
      "learning_rate": 0.00011737735849056603,
      "loss": 1.8251,
      "step": 20280
    },
    {
      "epoch": 3.828301886792453,
      "grad_norm": 0.8909086585044861,
      "learning_rate": 0.00011718867924528303,
      "loss": 1.8216,
      "step": 20290
    },
    {
      "epoch": 3.830188679245283,
      "grad_norm": 0.820095419883728,
      "learning_rate": 0.00011700000000000001,
      "loss": 1.7927,
      "step": 20300
    },
    {
      "epoch": 3.832075471698113,
      "grad_norm": 0.8945077061653137,
      "learning_rate": 0.00011681132075471698,
      "loss": 1.8064,
      "step": 20310
    },
    {
      "epoch": 3.8339622641509434,
      "grad_norm": 0.8387607932090759,
      "learning_rate": 0.00011662264150943396,
      "loss": 1.8807,
      "step": 20320
    },
    {
      "epoch": 3.8358490566037737,
      "grad_norm": 0.7330275774002075,
      "learning_rate": 0.00011643396226415094,
      "loss": 1.8824,
      "step": 20330
    },
    {
      "epoch": 3.837735849056604,
      "grad_norm": 0.8038601279258728,
      "learning_rate": 0.00011624528301886793,
      "loss": 1.843,
      "step": 20340
    },
    {
      "epoch": 3.839622641509434,
      "grad_norm": 0.7678459286689758,
      "learning_rate": 0.0001160566037735849,
      "loss": 1.8445,
      "step": 20350
    },
    {
      "epoch": 3.841509433962264,
      "grad_norm": 0.8020889163017273,
      "learning_rate": 0.00011586792452830189,
      "loss": 1.8739,
      "step": 20360
    },
    {
      "epoch": 3.8433962264150945,
      "grad_norm": 0.8421677947044373,
      "learning_rate": 0.00011567924528301887,
      "loss": 1.9087,
      "step": 20370
    },
    {
      "epoch": 3.8452830188679243,
      "grad_norm": 0.9117034673690796,
      "learning_rate": 0.00011549056603773584,
      "loss": 1.9095,
      "step": 20380
    },
    {
      "epoch": 3.8471698113207546,
      "grad_norm": 0.734904408454895,
      "learning_rate": 0.00011530188679245284,
      "loss": 1.8505,
      "step": 20390
    },
    {
      "epoch": 3.849056603773585,
      "grad_norm": 0.8079743385314941,
      "learning_rate": 0.0001151132075471698,
      "loss": 1.8248,
      "step": 20400
    },
    {
      "epoch": 3.8509433962264152,
      "grad_norm": 0.8091715574264526,
      "learning_rate": 0.00011492452830188679,
      "loss": 1.7922,
      "step": 20410
    },
    {
      "epoch": 3.8528301886792455,
      "grad_norm": 0.8350256681442261,
      "learning_rate": 0.00011473584905660378,
      "loss": 1.7568,
      "step": 20420
    },
    {
      "epoch": 3.8547169811320754,
      "grad_norm": 0.8158444166183472,
      "learning_rate": 0.00011454716981132075,
      "loss": 1.7953,
      "step": 20430
    },
    {
      "epoch": 3.8566037735849057,
      "grad_norm": 0.8720728754997253,
      "learning_rate": 0.00011435849056603773,
      "loss": 1.8738,
      "step": 20440
    },
    {
      "epoch": 3.858490566037736,
      "grad_norm": 0.8148509860038757,
      "learning_rate": 0.00011416981132075472,
      "loss": 1.7877,
      "step": 20450
    },
    {
      "epoch": 3.860377358490566,
      "grad_norm": 0.847859799861908,
      "learning_rate": 0.0001139811320754717,
      "loss": 1.8473,
      "step": 20460
    },
    {
      "epoch": 3.862264150943396,
      "grad_norm": 0.829644501209259,
      "learning_rate": 0.00011379245283018868,
      "loss": 1.8845,
      "step": 20470
    },
    {
      "epoch": 3.8641509433962264,
      "grad_norm": 0.8347358107566833,
      "learning_rate": 0.00011360377358490566,
      "loss": 1.8272,
      "step": 20480
    },
    {
      "epoch": 3.8660377358490567,
      "grad_norm": 0.7643141150474548,
      "learning_rate": 0.00011341509433962265,
      "loss": 1.8333,
      "step": 20490
    },
    {
      "epoch": 3.867924528301887,
      "grad_norm": 0.7767614722251892,
      "learning_rate": 0.00011322641509433961,
      "loss": 1.8988,
      "step": 20500
    },
    {
      "epoch": 3.869811320754717,
      "grad_norm": 0.8343205451965332,
      "learning_rate": 0.00011303773584905661,
      "loss": 1.8233,
      "step": 20510
    },
    {
      "epoch": 3.871698113207547,
      "grad_norm": 0.8551310300827026,
      "learning_rate": 0.00011284905660377359,
      "loss": 1.8993,
      "step": 20520
    },
    {
      "epoch": 3.8735849056603775,
      "grad_norm": 0.851521909236908,
      "learning_rate": 0.00011266037735849056,
      "loss": 1.866,
      "step": 20530
    },
    {
      "epoch": 3.8754716981132074,
      "grad_norm": 0.9584332704544067,
      "learning_rate": 0.00011247169811320756,
      "loss": 1.8038,
      "step": 20540
    },
    {
      "epoch": 3.8773584905660377,
      "grad_norm": 0.8453662395477295,
      "learning_rate": 0.00011228301886792453,
      "loss": 1.8525,
      "step": 20550
    },
    {
      "epoch": 3.879245283018868,
      "grad_norm": 0.8161022663116455,
      "learning_rate": 0.00011209433962264151,
      "loss": 1.8481,
      "step": 20560
    },
    {
      "epoch": 3.8811320754716983,
      "grad_norm": 0.7821977734565735,
      "learning_rate": 0.0001119056603773585,
      "loss": 1.8778,
      "step": 20570
    },
    {
      "epoch": 3.8830188679245285,
      "grad_norm": 0.7813518643379211,
      "learning_rate": 0.00011171698113207547,
      "loss": 1.8457,
      "step": 20580
    },
    {
      "epoch": 3.8849056603773584,
      "grad_norm": 0.8163041472434998,
      "learning_rate": 0.00011152830188679245,
      "loss": 1.8336,
      "step": 20590
    },
    {
      "epoch": 3.8867924528301887,
      "grad_norm": 0.8964802622795105,
      "learning_rate": 0.00011133962264150944,
      "loss": 1.7863,
      "step": 20600
    },
    {
      "epoch": 3.888679245283019,
      "grad_norm": 0.7456125617027283,
      "learning_rate": 0.00011115094339622642,
      "loss": 1.8469,
      "step": 20610
    },
    {
      "epoch": 3.890566037735849,
      "grad_norm": 0.8143985867500305,
      "learning_rate": 0.0001109622641509434,
      "loss": 1.8636,
      "step": 20620
    },
    {
      "epoch": 3.892452830188679,
      "grad_norm": 0.7927140593528748,
      "learning_rate": 0.00011077358490566038,
      "loss": 1.8571,
      "step": 20630
    },
    {
      "epoch": 3.8943396226415095,
      "grad_norm": 0.8009970188140869,
      "learning_rate": 0.00011058490566037736,
      "loss": 1.8512,
      "step": 20640
    },
    {
      "epoch": 3.8962264150943398,
      "grad_norm": 0.9581419229507446,
      "learning_rate": 0.00011039622641509433,
      "loss": 1.7963,
      "step": 20650
    },
    {
      "epoch": 3.89811320754717,
      "grad_norm": 0.8628864288330078,
      "learning_rate": 0.00011020754716981133,
      "loss": 1.8047,
      "step": 20660
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.8615045547485352,
      "learning_rate": 0.00011001886792452831,
      "loss": 1.7993,
      "step": 20670
    },
    {
      "epoch": 3.90188679245283,
      "grad_norm": 0.7623258233070374,
      "learning_rate": 0.00010983018867924528,
      "loss": 1.9564,
      "step": 20680
    },
    {
      "epoch": 3.9037735849056605,
      "grad_norm": 0.901988685131073,
      "learning_rate": 0.00010964150943396228,
      "loss": 1.8716,
      "step": 20690
    },
    {
      "epoch": 3.9056603773584904,
      "grad_norm": 0.8686201572418213,
      "learning_rate": 0.00010945283018867924,
      "loss": 1.9178,
      "step": 20700
    },
    {
      "epoch": 3.9075471698113207,
      "grad_norm": 0.831664502620697,
      "learning_rate": 0.00010926415094339623,
      "loss": 1.864,
      "step": 20710
    },
    {
      "epoch": 3.909433962264151,
      "grad_norm": 0.7969473004341125,
      "learning_rate": 0.00010907547169811322,
      "loss": 1.8541,
      "step": 20720
    },
    {
      "epoch": 3.9113207547169813,
      "grad_norm": 0.8309666514396667,
      "learning_rate": 0.00010888679245283019,
      "loss": 1.8515,
      "step": 20730
    },
    {
      "epoch": 3.9132075471698116,
      "grad_norm": 0.892138659954071,
      "learning_rate": 0.00010869811320754717,
      "loss": 1.8656,
      "step": 20740
    },
    {
      "epoch": 3.9150943396226414,
      "grad_norm": 0.8389678001403809,
      "learning_rate": 0.00010850943396226414,
      "loss": 1.8527,
      "step": 20750
    },
    {
      "epoch": 3.9169811320754717,
      "grad_norm": 0.7302448749542236,
      "learning_rate": 0.00010832075471698114,
      "loss": 1.885,
      "step": 20760
    },
    {
      "epoch": 3.918867924528302,
      "grad_norm": 0.7686953544616699,
      "learning_rate": 0.00010813207547169812,
      "loss": 1.8274,
      "step": 20770
    },
    {
      "epoch": 3.920754716981132,
      "grad_norm": 0.78834468126297,
      "learning_rate": 0.00010794339622641509,
      "loss": 1.8518,
      "step": 20780
    },
    {
      "epoch": 3.922641509433962,
      "grad_norm": 0.817051112651825,
      "learning_rate": 0.00010775471698113208,
      "loss": 1.8808,
      "step": 20790
    },
    {
      "epoch": 3.9245283018867925,
      "grad_norm": 0.7750361561775208,
      "learning_rate": 0.00010756603773584905,
      "loss": 1.781,
      "step": 20800
    },
    {
      "epoch": 3.9264150943396228,
      "grad_norm": 0.8211176991462708,
      "learning_rate": 0.00010737735849056603,
      "loss": 1.9105,
      "step": 20810
    },
    {
      "epoch": 3.928301886792453,
      "grad_norm": 0.8270232677459717,
      "learning_rate": 0.00010718867924528303,
      "loss": 1.8593,
      "step": 20820
    },
    {
      "epoch": 3.930188679245283,
      "grad_norm": 0.9547997117042542,
      "learning_rate": 0.000107,
      "loss": 1.8133,
      "step": 20830
    },
    {
      "epoch": 3.9320754716981132,
      "grad_norm": 0.8232306241989136,
      "learning_rate": 0.00010681132075471698,
      "loss": 1.8601,
      "step": 20840
    },
    {
      "epoch": 3.9339622641509435,
      "grad_norm": 0.8239782452583313,
      "learning_rate": 0.00010662264150943396,
      "loss": 1.8536,
      "step": 20850
    },
    {
      "epoch": 3.9358490566037734,
      "grad_norm": 0.694983184337616,
      "learning_rate": 0.00010643396226415095,
      "loss": 1.7357,
      "step": 20860
    },
    {
      "epoch": 3.9377358490566037,
      "grad_norm": 0.738598644733429,
      "learning_rate": 0.00010624528301886793,
      "loss": 1.799,
      "step": 20870
    },
    {
      "epoch": 3.939622641509434,
      "grad_norm": 0.9277070164680481,
      "learning_rate": 0.00010605660377358491,
      "loss": 1.8835,
      "step": 20880
    },
    {
      "epoch": 3.9415094339622643,
      "grad_norm": 0.8084003925323486,
      "learning_rate": 0.00010586792452830189,
      "loss": 1.7944,
      "step": 20890
    },
    {
      "epoch": 3.9433962264150946,
      "grad_norm": 0.7609396576881409,
      "learning_rate": 0.00010567924528301886,
      "loss": 1.8374,
      "step": 20900
    },
    {
      "epoch": 3.9452830188679244,
      "grad_norm": 0.7513819336891174,
      "learning_rate": 0.00010549056603773586,
      "loss": 1.8212,
      "step": 20910
    },
    {
      "epoch": 3.9471698113207547,
      "grad_norm": 0.8329731822013855,
      "learning_rate": 0.00010530188679245284,
      "loss": 1.8911,
      "step": 20920
    },
    {
      "epoch": 3.949056603773585,
      "grad_norm": 0.7774487137794495,
      "learning_rate": 0.00010511320754716981,
      "loss": 1.7877,
      "step": 20930
    },
    {
      "epoch": 3.950943396226415,
      "grad_norm": 0.8019568920135498,
      "learning_rate": 0.0001049245283018868,
      "loss": 1.9078,
      "step": 20940
    },
    {
      "epoch": 3.952830188679245,
      "grad_norm": 0.8892008662223816,
      "learning_rate": 0.00010473584905660377,
      "loss": 1.85,
      "step": 20950
    },
    {
      "epoch": 3.9547169811320755,
      "grad_norm": 0.8325986266136169,
      "learning_rate": 0.00010454716981132075,
      "loss": 1.8857,
      "step": 20960
    },
    {
      "epoch": 3.956603773584906,
      "grad_norm": 0.8150606751441956,
      "learning_rate": 0.00010435849056603775,
      "loss": 1.9249,
      "step": 20970
    },
    {
      "epoch": 3.958490566037736,
      "grad_norm": 0.7052995562553406,
      "learning_rate": 0.00010416981132075472,
      "loss": 1.8762,
      "step": 20980
    },
    {
      "epoch": 3.960377358490566,
      "grad_norm": 0.7918252944946289,
      "learning_rate": 0.0001039811320754717,
      "loss": 1.8406,
      "step": 20990
    },
    {
      "epoch": 3.9622641509433962,
      "grad_norm": 0.8342841267585754,
      "learning_rate": 0.00010379245283018868,
      "loss": 1.8778,
      "step": 21000
    },
    {
      "epoch": 3.9641509433962265,
      "grad_norm": 0.8297361731529236,
      "learning_rate": 0.00010360377358490566,
      "loss": 1.9042,
      "step": 21010
    },
    {
      "epoch": 3.9660377358490564,
      "grad_norm": 0.783398449420929,
      "learning_rate": 0.00010341509433962263,
      "loss": 1.914,
      "step": 21020
    },
    {
      "epoch": 3.9679245283018867,
      "grad_norm": 0.763325035572052,
      "learning_rate": 0.00010322641509433963,
      "loss": 1.8935,
      "step": 21030
    },
    {
      "epoch": 3.969811320754717,
      "grad_norm": 0.9366652369499207,
      "learning_rate": 0.00010303773584905661,
      "loss": 1.7848,
      "step": 21040
    },
    {
      "epoch": 3.9716981132075473,
      "grad_norm": 0.8883286714553833,
      "learning_rate": 0.00010284905660377358,
      "loss": 1.8035,
      "step": 21050
    },
    {
      "epoch": 3.9735849056603776,
      "grad_norm": 0.8035120368003845,
      "learning_rate": 0.00010266037735849058,
      "loss": 1.8906,
      "step": 21060
    },
    {
      "epoch": 3.9754716981132074,
      "grad_norm": 0.8674721717834473,
      "learning_rate": 0.00010247169811320754,
      "loss": 1.8897,
      "step": 21070
    },
    {
      "epoch": 3.9773584905660377,
      "grad_norm": 0.8771507740020752,
      "learning_rate": 0.00010228301886792453,
      "loss": 1.8106,
      "step": 21080
    },
    {
      "epoch": 3.979245283018868,
      "grad_norm": 0.7928372025489807,
      "learning_rate": 0.00010209433962264152,
      "loss": 1.923,
      "step": 21090
    },
    {
      "epoch": 3.981132075471698,
      "grad_norm": 0.8879068493843079,
      "learning_rate": 0.00010190566037735849,
      "loss": 1.9005,
      "step": 21100
    },
    {
      "epoch": 3.983018867924528,
      "grad_norm": 0.8081448674201965,
      "learning_rate": 0.00010171698113207547,
      "loss": 1.9602,
      "step": 21110
    },
    {
      "epoch": 3.9849056603773585,
      "grad_norm": 0.8759456872940063,
      "learning_rate": 0.00010152830188679245,
      "loss": 2.0036,
      "step": 21120
    },
    {
      "epoch": 3.986792452830189,
      "grad_norm": 0.7454626560211182,
      "learning_rate": 0.00010133962264150944,
      "loss": 1.7974,
      "step": 21130
    },
    {
      "epoch": 3.988679245283019,
      "grad_norm": 0.9007256031036377,
      "learning_rate": 0.00010115094339622642,
      "loss": 1.8646,
      "step": 21140
    },
    {
      "epoch": 3.990566037735849,
      "grad_norm": 0.9366403222084045,
      "learning_rate": 0.0001009622641509434,
      "loss": 1.8791,
      "step": 21150
    },
    {
      "epoch": 3.9924528301886792,
      "grad_norm": 0.8465025424957275,
      "learning_rate": 0.00010077358490566038,
      "loss": 1.8356,
      "step": 21160
    },
    {
      "epoch": 3.9943396226415095,
      "grad_norm": 0.829300582408905,
      "learning_rate": 0.00010058490566037735,
      "loss": 1.8187,
      "step": 21170
    },
    {
      "epoch": 3.9962264150943394,
      "grad_norm": 0.8050073981285095,
      "learning_rate": 0.00010039622641509435,
      "loss": 1.9977,
      "step": 21180
    },
    {
      "epoch": 3.9981132075471697,
      "grad_norm": 0.9349005818367004,
      "learning_rate": 0.00010020754716981133,
      "loss": 1.8933,
      "step": 21190
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.9048314094543457,
      "learning_rate": 0.0001000188679245283,
      "loss": 1.9041,
      "step": 21200
    },
    {
      "epoch": 4.00188679245283,
      "grad_norm": 0.8505833745002747,
      "learning_rate": 9.983018867924528e-05,
      "loss": 1.8115,
      "step": 21210
    },
    {
      "epoch": 4.003773584905661,
      "grad_norm": 0.8306086659431458,
      "learning_rate": 9.964150943396226e-05,
      "loss": 1.9357,
      "step": 21220
    },
    {
      "epoch": 4.005660377358491,
      "grad_norm": 0.7843639254570007,
      "learning_rate": 9.945283018867924e-05,
      "loss": 1.8818,
      "step": 21230
    },
    {
      "epoch": 4.007547169811321,
      "grad_norm": 0.7603231072425842,
      "learning_rate": 9.926415094339623e-05,
      "loss": 1.8787,
      "step": 21240
    },
    {
      "epoch": 4.009433962264151,
      "grad_norm": 0.7547023296356201,
      "learning_rate": 9.907547169811321e-05,
      "loss": 1.8002,
      "step": 21250
    },
    {
      "epoch": 4.011320754716981,
      "grad_norm": 0.8180878758430481,
      "learning_rate": 9.888679245283019e-05,
      "loss": 1.9121,
      "step": 21260
    },
    {
      "epoch": 4.013207547169811,
      "grad_norm": 0.851706326007843,
      "learning_rate": 9.869811320754716e-05,
      "loss": 1.8401,
      "step": 21270
    },
    {
      "epoch": 4.0150943396226415,
      "grad_norm": 0.7904766201972961,
      "learning_rate": 9.850943396226416e-05,
      "loss": 1.7789,
      "step": 21280
    },
    {
      "epoch": 4.016981132075472,
      "grad_norm": 0.8701714277267456,
      "learning_rate": 9.832075471698114e-05,
      "loss": 1.8897,
      "step": 21290
    },
    {
      "epoch": 4.018867924528302,
      "grad_norm": 0.8115870356559753,
      "learning_rate": 9.81320754716981e-05,
      "loss": 1.8857,
      "step": 21300
    },
    {
      "epoch": 4.020754716981132,
      "grad_norm": 0.7526608109474182,
      "learning_rate": 9.79433962264151e-05,
      "loss": 1.8301,
      "step": 21310
    },
    {
      "epoch": 4.022641509433963,
      "grad_norm": 0.8500029444694519,
      "learning_rate": 9.775471698113207e-05,
      "loss": 1.82,
      "step": 21320
    },
    {
      "epoch": 4.024528301886792,
      "grad_norm": 0.7529662847518921,
      "learning_rate": 9.756603773584905e-05,
      "loss": 1.8971,
      "step": 21330
    },
    {
      "epoch": 4.026415094339622,
      "grad_norm": 0.8884759545326233,
      "learning_rate": 9.737735849056605e-05,
      "loss": 1.7905,
      "step": 21340
    },
    {
      "epoch": 4.028301886792453,
      "grad_norm": 0.8698198795318604,
      "learning_rate": 9.718867924528302e-05,
      "loss": 1.8472,
      "step": 21350
    },
    {
      "epoch": 4.030188679245283,
      "grad_norm": 0.7687035799026489,
      "learning_rate": 9.7e-05,
      "loss": 1.8813,
      "step": 21360
    },
    {
      "epoch": 4.032075471698113,
      "grad_norm": 0.8256320357322693,
      "learning_rate": 9.681132075471698e-05,
      "loss": 1.8572,
      "step": 21370
    },
    {
      "epoch": 4.033962264150944,
      "grad_norm": 0.7752434611320496,
      "learning_rate": 9.662264150943396e-05,
      "loss": 1.8601,
      "step": 21380
    },
    {
      "epoch": 4.035849056603774,
      "grad_norm": 0.7671857476234436,
      "learning_rate": 9.643396226415095e-05,
      "loss": 1.8996,
      "step": 21390
    },
    {
      "epoch": 4.037735849056604,
      "grad_norm": 0.7803894877433777,
      "learning_rate": 9.624528301886793e-05,
      "loss": 1.8146,
      "step": 21400
    },
    {
      "epoch": 4.039622641509434,
      "grad_norm": 0.7710849642753601,
      "learning_rate": 9.605660377358491e-05,
      "loss": 1.8521,
      "step": 21410
    },
    {
      "epoch": 4.041509433962264,
      "grad_norm": 0.8576511740684509,
      "learning_rate": 9.586792452830188e-05,
      "loss": 1.8305,
      "step": 21420
    },
    {
      "epoch": 4.043396226415094,
      "grad_norm": 0.871239960193634,
      "learning_rate": 9.567924528301887e-05,
      "loss": 1.8557,
      "step": 21430
    },
    {
      "epoch": 4.0452830188679245,
      "grad_norm": 0.7963132858276367,
      "learning_rate": 9.549056603773586e-05,
      "loss": 1.8726,
      "step": 21440
    },
    {
      "epoch": 4.047169811320755,
      "grad_norm": 0.8184362649917603,
      "learning_rate": 9.530188679245283e-05,
      "loss": 1.7729,
      "step": 21450
    },
    {
      "epoch": 4.049056603773585,
      "grad_norm": 0.8326864838600159,
      "learning_rate": 9.511320754716982e-05,
      "loss": 1.8954,
      "step": 21460
    },
    {
      "epoch": 4.050943396226415,
      "grad_norm": 0.8336164951324463,
      "learning_rate": 9.492452830188679e-05,
      "loss": 1.7766,
      "step": 21470
    },
    {
      "epoch": 4.052830188679246,
      "grad_norm": 0.8132776021957397,
      "learning_rate": 9.473584905660377e-05,
      "loss": 1.8332,
      "step": 21480
    },
    {
      "epoch": 4.054716981132075,
      "grad_norm": 0.8698025941848755,
      "learning_rate": 9.454716981132077e-05,
      "loss": 1.7754,
      "step": 21490
    },
    {
      "epoch": 4.056603773584905,
      "grad_norm": 0.8682761192321777,
      "learning_rate": 9.435849056603774e-05,
      "loss": 1.8836,
      "step": 21500
    },
    {
      "epoch": 4.058490566037736,
      "grad_norm": 0.9610140919685364,
      "learning_rate": 9.416981132075472e-05,
      "loss": 1.9198,
      "step": 21510
    },
    {
      "epoch": 4.060377358490566,
      "grad_norm": 0.9108216762542725,
      "learning_rate": 9.39811320754717e-05,
      "loss": 1.8866,
      "step": 21520
    },
    {
      "epoch": 4.062264150943396,
      "grad_norm": 0.8557566404342651,
      "learning_rate": 9.379245283018868e-05,
      "loss": 1.7926,
      "step": 21530
    },
    {
      "epoch": 4.064150943396227,
      "grad_norm": 0.8006730675697327,
      "learning_rate": 9.360377358490567e-05,
      "loss": 1.8207,
      "step": 21540
    },
    {
      "epoch": 4.066037735849057,
      "grad_norm": 0.8166882395744324,
      "learning_rate": 9.341509433962265e-05,
      "loss": 1.8882,
      "step": 21550
    },
    {
      "epoch": 4.067924528301887,
      "grad_norm": 0.7285929918289185,
      "learning_rate": 9.322641509433963e-05,
      "loss": 1.9159,
      "step": 21560
    },
    {
      "epoch": 4.069811320754717,
      "grad_norm": 0.8793572187423706,
      "learning_rate": 9.30377358490566e-05,
      "loss": 1.7791,
      "step": 21570
    },
    {
      "epoch": 4.071698113207547,
      "grad_norm": 0.7727200984954834,
      "learning_rate": 9.28490566037736e-05,
      "loss": 1.8548,
      "step": 21580
    },
    {
      "epoch": 4.073584905660377,
      "grad_norm": 0.8466039299964905,
      "learning_rate": 9.266037735849058e-05,
      "loss": 1.8366,
      "step": 21590
    },
    {
      "epoch": 4.0754716981132075,
      "grad_norm": 0.831288754940033,
      "learning_rate": 9.247169811320754e-05,
      "loss": 1.9476,
      "step": 21600
    },
    {
      "epoch": 4.077358490566038,
      "grad_norm": 0.8224418759346008,
      "learning_rate": 9.228301886792454e-05,
      "loss": 1.8479,
      "step": 21610
    },
    {
      "epoch": 4.079245283018868,
      "grad_norm": 0.8949670791625977,
      "learning_rate": 9.209433962264151e-05,
      "loss": 1.9091,
      "step": 21620
    },
    {
      "epoch": 4.081132075471698,
      "grad_norm": 0.7647736668586731,
      "learning_rate": 9.190566037735849e-05,
      "loss": 1.8409,
      "step": 21630
    },
    {
      "epoch": 4.083018867924529,
      "grad_norm": 0.7512795925140381,
      "learning_rate": 9.171698113207549e-05,
      "loss": 1.8569,
      "step": 21640
    },
    {
      "epoch": 4.084905660377358,
      "grad_norm": 0.7732886672019958,
      "learning_rate": 9.152830188679246e-05,
      "loss": 1.9095,
      "step": 21650
    },
    {
      "epoch": 4.086792452830188,
      "grad_norm": 0.8553864359855652,
      "learning_rate": 9.133962264150944e-05,
      "loss": 1.847,
      "step": 21660
    },
    {
      "epoch": 4.088679245283019,
      "grad_norm": 0.8339052796363831,
      "learning_rate": 9.11509433962264e-05,
      "loss": 1.7995,
      "step": 21670
    },
    {
      "epoch": 4.090566037735849,
      "grad_norm": 0.9597161412239075,
      "learning_rate": 9.09622641509434e-05,
      "loss": 1.8827,
      "step": 21680
    },
    {
      "epoch": 4.092452830188679,
      "grad_norm": 0.8302937746047974,
      "learning_rate": 9.077358490566037e-05,
      "loss": 1.8379,
      "step": 21690
    },
    {
      "epoch": 4.09433962264151,
      "grad_norm": 0.8405085802078247,
      "learning_rate": 9.058490566037735e-05,
      "loss": 1.75,
      "step": 21700
    },
    {
      "epoch": 4.09622641509434,
      "grad_norm": 0.8447683453559875,
      "learning_rate": 9.039622641509435e-05,
      "loss": 1.835,
      "step": 21710
    },
    {
      "epoch": 4.09811320754717,
      "grad_norm": 0.8944113850593567,
      "learning_rate": 9.020754716981132e-05,
      "loss": 1.8178,
      "step": 21720
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.8307560682296753,
      "learning_rate": 9.00188679245283e-05,
      "loss": 1.9699,
      "step": 21730
    },
    {
      "epoch": 4.10188679245283,
      "grad_norm": 0.7280920743942261,
      "learning_rate": 8.983018867924528e-05,
      "loss": 1.8268,
      "step": 21740
    },
    {
      "epoch": 4.10377358490566,
      "grad_norm": 0.764529824256897,
      "learning_rate": 8.964150943396226e-05,
      "loss": 1.7903,
      "step": 21750
    },
    {
      "epoch": 4.1056603773584905,
      "grad_norm": 0.7853668332099915,
      "learning_rate": 8.945283018867925e-05,
      "loss": 1.7071,
      "step": 21760
    },
    {
      "epoch": 4.107547169811321,
      "grad_norm": 0.8327977657318115,
      "learning_rate": 8.926415094339623e-05,
      "loss": 1.8282,
      "step": 21770
    },
    {
      "epoch": 4.109433962264151,
      "grad_norm": 0.8633725643157959,
      "learning_rate": 8.907547169811321e-05,
      "loss": 1.8183,
      "step": 21780
    },
    {
      "epoch": 4.111320754716981,
      "grad_norm": 0.82839035987854,
      "learning_rate": 8.888679245283018e-05,
      "loss": 1.8841,
      "step": 21790
    },
    {
      "epoch": 4.113207547169812,
      "grad_norm": 0.7631463408470154,
      "learning_rate": 8.869811320754717e-05,
      "loss": 1.834,
      "step": 21800
    },
    {
      "epoch": 4.115094339622641,
      "grad_norm": 0.896831750869751,
      "learning_rate": 8.850943396226416e-05,
      "loss": 1.8556,
      "step": 21810
    },
    {
      "epoch": 4.1169811320754715,
      "grad_norm": 0.8590627908706665,
      "learning_rate": 8.832075471698113e-05,
      "loss": 1.8957,
      "step": 21820
    },
    {
      "epoch": 4.118867924528302,
      "grad_norm": 0.8394264578819275,
      "learning_rate": 8.813207547169812e-05,
      "loss": 1.8726,
      "step": 21830
    },
    {
      "epoch": 4.120754716981132,
      "grad_norm": 0.8527789115905762,
      "learning_rate": 8.794339622641509e-05,
      "loss": 1.8037,
      "step": 21840
    },
    {
      "epoch": 4.122641509433962,
      "grad_norm": 0.7589428424835205,
      "learning_rate": 8.775471698113207e-05,
      "loss": 1.9032,
      "step": 21850
    },
    {
      "epoch": 4.124528301886793,
      "grad_norm": 0.8327270150184631,
      "learning_rate": 8.756603773584907e-05,
      "loss": 1.9087,
      "step": 21860
    },
    {
      "epoch": 4.126415094339623,
      "grad_norm": 0.8341127038002014,
      "learning_rate": 8.737735849056604e-05,
      "loss": 1.7872,
      "step": 21870
    },
    {
      "epoch": 4.128301886792453,
      "grad_norm": 0.8118706941604614,
      "learning_rate": 8.718867924528302e-05,
      "loss": 1.9214,
      "step": 21880
    },
    {
      "epoch": 4.130188679245283,
      "grad_norm": 0.9760428667068481,
      "learning_rate": 8.7e-05,
      "loss": 1.8408,
      "step": 21890
    },
    {
      "epoch": 4.132075471698113,
      "grad_norm": 0.8268833756446838,
      "learning_rate": 8.681132075471698e-05,
      "loss": 1.8068,
      "step": 21900
    },
    {
      "epoch": 4.133962264150943,
      "grad_norm": 0.8501021862030029,
      "learning_rate": 8.662264150943396e-05,
      "loss": 1.8561,
      "step": 21910
    },
    {
      "epoch": 4.1358490566037736,
      "grad_norm": 0.8582231402397156,
      "learning_rate": 8.643396226415095e-05,
      "loss": 1.794,
      "step": 21920
    },
    {
      "epoch": 4.137735849056604,
      "grad_norm": 0.826632022857666,
      "learning_rate": 8.624528301886793e-05,
      "loss": 1.8279,
      "step": 21930
    },
    {
      "epoch": 4.139622641509434,
      "grad_norm": 0.8660826086997986,
      "learning_rate": 8.60566037735849e-05,
      "loss": 1.8425,
      "step": 21940
    },
    {
      "epoch": 4.1415094339622645,
      "grad_norm": 0.8925737738609314,
      "learning_rate": 8.58679245283019e-05,
      "loss": 1.9128,
      "step": 21950
    },
    {
      "epoch": 4.143396226415095,
      "grad_norm": 0.9102052450180054,
      "learning_rate": 8.567924528301888e-05,
      "loss": 1.9025,
      "step": 21960
    },
    {
      "epoch": 4.145283018867924,
      "grad_norm": 0.7560571432113647,
      "learning_rate": 8.549056603773584e-05,
      "loss": 1.8556,
      "step": 21970
    },
    {
      "epoch": 4.1471698113207545,
      "grad_norm": 0.8711465001106262,
      "learning_rate": 8.530188679245284e-05,
      "loss": 1.8242,
      "step": 21980
    },
    {
      "epoch": 4.149056603773585,
      "grad_norm": 0.8613055944442749,
      "learning_rate": 8.511320754716981e-05,
      "loss": 1.8204,
      "step": 21990
    },
    {
      "epoch": 4.150943396226415,
      "grad_norm": 0.7910537719726562,
      "learning_rate": 8.492452830188679e-05,
      "loss": 1.8328,
      "step": 22000
    },
    {
      "epoch": 4.152830188679245,
      "grad_norm": 0.7879205346107483,
      "learning_rate": 8.473584905660379e-05,
      "loss": 1.8332,
      "step": 22010
    },
    {
      "epoch": 4.154716981132076,
      "grad_norm": 0.8455570936203003,
      "learning_rate": 8.454716981132076e-05,
      "loss": 1.8426,
      "step": 22020
    },
    {
      "epoch": 4.156603773584906,
      "grad_norm": 0.9214540123939514,
      "learning_rate": 8.435849056603774e-05,
      "loss": 1.966,
      "step": 22030
    },
    {
      "epoch": 4.158490566037736,
      "grad_norm": 0.8124169707298279,
      "learning_rate": 8.416981132075472e-05,
      "loss": 1.9585,
      "step": 22040
    },
    {
      "epoch": 4.160377358490566,
      "grad_norm": 0.811385989189148,
      "learning_rate": 8.39811320754717e-05,
      "loss": 1.8231,
      "step": 22050
    },
    {
      "epoch": 4.162264150943396,
      "grad_norm": 0.8989755511283875,
      "learning_rate": 8.379245283018868e-05,
      "loss": 1.8455,
      "step": 22060
    },
    {
      "epoch": 4.164150943396226,
      "grad_norm": 0.8371307253837585,
      "learning_rate": 8.360377358490567e-05,
      "loss": 1.9218,
      "step": 22070
    },
    {
      "epoch": 4.166037735849057,
      "grad_norm": 0.8422160148620605,
      "learning_rate": 8.341509433962265e-05,
      "loss": 1.8661,
      "step": 22080
    },
    {
      "epoch": 4.167924528301887,
      "grad_norm": 0.8707537651062012,
      "learning_rate": 8.322641509433962e-05,
      "loss": 1.7718,
      "step": 22090
    },
    {
      "epoch": 4.169811320754717,
      "grad_norm": 0.7902156710624695,
      "learning_rate": 8.303773584905661e-05,
      "loss": 1.8676,
      "step": 22100
    },
    {
      "epoch": 4.1716981132075475,
      "grad_norm": 0.8886914849281311,
      "learning_rate": 8.28490566037736e-05,
      "loss": 1.8466,
      "step": 22110
    },
    {
      "epoch": 4.173584905660378,
      "grad_norm": 0.8599412441253662,
      "learning_rate": 8.266037735849056e-05,
      "loss": 1.9321,
      "step": 22120
    },
    {
      "epoch": 4.175471698113207,
      "grad_norm": 0.753005862236023,
      "learning_rate": 8.247169811320755e-05,
      "loss": 1.8579,
      "step": 22130
    },
    {
      "epoch": 4.1773584905660375,
      "grad_norm": 0.8149605989456177,
      "learning_rate": 8.228301886792453e-05,
      "loss": 1.8704,
      "step": 22140
    },
    {
      "epoch": 4.179245283018868,
      "grad_norm": 0.7498798370361328,
      "learning_rate": 8.209433962264151e-05,
      "loss": 1.8886,
      "step": 22150
    },
    {
      "epoch": 4.181132075471698,
      "grad_norm": 0.8276345133781433,
      "learning_rate": 8.190566037735849e-05,
      "loss": 1.8328,
      "step": 22160
    },
    {
      "epoch": 4.183018867924528,
      "grad_norm": 0.845294713973999,
      "learning_rate": 8.171698113207547e-05,
      "loss": 1.7973,
      "step": 22170
    },
    {
      "epoch": 4.184905660377359,
      "grad_norm": 0.9462301135063171,
      "learning_rate": 8.152830188679246e-05,
      "loss": 1.8405,
      "step": 22180
    },
    {
      "epoch": 4.186792452830189,
      "grad_norm": 0.8407399654388428,
      "learning_rate": 8.133962264150942e-05,
      "loss": 1.8204,
      "step": 22190
    },
    {
      "epoch": 4.188679245283019,
      "grad_norm": 1.0030372142791748,
      "learning_rate": 8.115094339622642e-05,
      "loss": 1.8284,
      "step": 22200
    },
    {
      "epoch": 4.190566037735849,
      "grad_norm": 0.8918828368186951,
      "learning_rate": 8.09622641509434e-05,
      "loss": 1.7759,
      "step": 22210
    },
    {
      "epoch": 4.192452830188679,
      "grad_norm": 0.9460028409957886,
      "learning_rate": 8.077358490566037e-05,
      "loss": 1.8344,
      "step": 22220
    },
    {
      "epoch": 4.194339622641509,
      "grad_norm": 0.8174428343772888,
      "learning_rate": 8.058490566037737e-05,
      "loss": 1.8754,
      "step": 22230
    },
    {
      "epoch": 4.19622641509434,
      "grad_norm": 0.814316987991333,
      "learning_rate": 8.039622641509434e-05,
      "loss": 1.8818,
      "step": 22240
    },
    {
      "epoch": 4.19811320754717,
      "grad_norm": 0.8532671928405762,
      "learning_rate": 8.020754716981132e-05,
      "loss": 1.8487,
      "step": 22250
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.794851541519165,
      "learning_rate": 8.001886792452831e-05,
      "loss": 1.7534,
      "step": 22260
    },
    {
      "epoch": 4.2018867924528305,
      "grad_norm": 0.8383100032806396,
      "learning_rate": 7.983018867924528e-05,
      "loss": 1.7869,
      "step": 22270
    },
    {
      "epoch": 4.203773584905661,
      "grad_norm": 0.7707429528236389,
      "learning_rate": 7.964150943396226e-05,
      "loss": 1.819,
      "step": 22280
    },
    {
      "epoch": 4.20566037735849,
      "grad_norm": 0.8261497020721436,
      "learning_rate": 7.945283018867925e-05,
      "loss": 1.9012,
      "step": 22290
    },
    {
      "epoch": 4.2075471698113205,
      "grad_norm": 0.9171282052993774,
      "learning_rate": 7.926415094339623e-05,
      "loss": 1.8603,
      "step": 22300
    },
    {
      "epoch": 4.209433962264151,
      "grad_norm": 0.8642660975456238,
      "learning_rate": 7.907547169811321e-05,
      "loss": 1.8877,
      "step": 22310
    },
    {
      "epoch": 4.211320754716981,
      "grad_norm": 0.8433939814567566,
      "learning_rate": 7.88867924528302e-05,
      "loss": 1.886,
      "step": 22320
    },
    {
      "epoch": 4.213207547169811,
      "grad_norm": 0.8502520322799683,
      "learning_rate": 7.869811320754718e-05,
      "loss": 1.763,
      "step": 22330
    },
    {
      "epoch": 4.215094339622642,
      "grad_norm": 0.7857716679573059,
      "learning_rate": 7.850943396226414e-05,
      "loss": 1.9122,
      "step": 22340
    },
    {
      "epoch": 4.216981132075472,
      "grad_norm": 0.891865611076355,
      "learning_rate": 7.832075471698114e-05,
      "loss": 1.8677,
      "step": 22350
    },
    {
      "epoch": 4.218867924528302,
      "grad_norm": 0.7596664428710938,
      "learning_rate": 7.813207547169811e-05,
      "loss": 1.9173,
      "step": 22360
    },
    {
      "epoch": 4.220754716981132,
      "grad_norm": 0.8298019170761108,
      "learning_rate": 7.794339622641509e-05,
      "loss": 1.8258,
      "step": 22370
    },
    {
      "epoch": 4.222641509433962,
      "grad_norm": 0.8232115507125854,
      "learning_rate": 7.775471698113209e-05,
      "loss": 1.7746,
      "step": 22380
    },
    {
      "epoch": 4.224528301886792,
      "grad_norm": 0.8094289898872375,
      "learning_rate": 7.756603773584905e-05,
      "loss": 1.7989,
      "step": 22390
    },
    {
      "epoch": 4.226415094339623,
      "grad_norm": 0.8968861699104309,
      "learning_rate": 7.737735849056604e-05,
      "loss": 1.9181,
      "step": 22400
    },
    {
      "epoch": 4.228301886792453,
      "grad_norm": 0.865617036819458,
      "learning_rate": 7.718867924528302e-05,
      "loss": 1.8132,
      "step": 22410
    },
    {
      "epoch": 4.230188679245283,
      "grad_norm": 0.7670232057571411,
      "learning_rate": 7.7e-05,
      "loss": 1.8279,
      "step": 22420
    },
    {
      "epoch": 4.2320754716981135,
      "grad_norm": 1.0069282054901123,
      "learning_rate": 7.681132075471698e-05,
      "loss": 1.8564,
      "step": 22430
    },
    {
      "epoch": 4.233962264150944,
      "grad_norm": 0.7349680066108704,
      "learning_rate": 7.662264150943397e-05,
      "loss": 2.0165,
      "step": 22440
    },
    {
      "epoch": 4.235849056603773,
      "grad_norm": 0.9641805291175842,
      "learning_rate": 7.643396226415095e-05,
      "loss": 1.8027,
      "step": 22450
    },
    {
      "epoch": 4.2377358490566035,
      "grad_norm": 0.810654878616333,
      "learning_rate": 7.624528301886792e-05,
      "loss": 1.9005,
      "step": 22460
    },
    {
      "epoch": 4.239622641509434,
      "grad_norm": 0.8740787506103516,
      "learning_rate": 7.605660377358491e-05,
      "loss": 1.8914,
      "step": 22470
    },
    {
      "epoch": 4.241509433962264,
      "grad_norm": 0.9694032073020935,
      "learning_rate": 7.58679245283019e-05,
      "loss": 1.8316,
      "step": 22480
    },
    {
      "epoch": 4.243396226415094,
      "grad_norm": 0.8084163665771484,
      "learning_rate": 7.567924528301886e-05,
      "loss": 1.7976,
      "step": 22490
    },
    {
      "epoch": 4.245283018867925,
      "grad_norm": 0.9198867678642273,
      "learning_rate": 7.549056603773586e-05,
      "loss": 1.8786,
      "step": 22500
    },
    {
      "epoch": 4.247169811320755,
      "grad_norm": 0.8098231554031372,
      "learning_rate": 7.530188679245283e-05,
      "loss": 1.87,
      "step": 22510
    },
    {
      "epoch": 4.249056603773585,
      "grad_norm": 0.824088990688324,
      "learning_rate": 7.511320754716981e-05,
      "loss": 1.7976,
      "step": 22520
    },
    {
      "epoch": 4.250943396226415,
      "grad_norm": 0.8361585140228271,
      "learning_rate": 7.49245283018868e-05,
      "loss": 1.7945,
      "step": 22530
    },
    {
      "epoch": 4.252830188679245,
      "grad_norm": 0.9514603018760681,
      "learning_rate": 7.473584905660377e-05,
      "loss": 1.9222,
      "step": 22540
    },
    {
      "epoch": 4.254716981132075,
      "grad_norm": 0.8138401508331299,
      "learning_rate": 7.454716981132076e-05,
      "loss": 1.9216,
      "step": 22550
    },
    {
      "epoch": 4.256603773584906,
      "grad_norm": 0.7754433155059814,
      "learning_rate": 7.435849056603772e-05,
      "loss": 1.8583,
      "step": 22560
    },
    {
      "epoch": 4.258490566037736,
      "grad_norm": 0.8397847414016724,
      "learning_rate": 7.416981132075472e-05,
      "loss": 1.7706,
      "step": 22570
    },
    {
      "epoch": 4.260377358490566,
      "grad_norm": 0.7630648612976074,
      "learning_rate": 7.39811320754717e-05,
      "loss": 1.8929,
      "step": 22580
    },
    {
      "epoch": 4.2622641509433965,
      "grad_norm": 0.8580607175827026,
      "learning_rate": 7.379245283018867e-05,
      "loss": 1.8049,
      "step": 22590
    },
    {
      "epoch": 4.264150943396227,
      "grad_norm": 0.8668701648712158,
      "learning_rate": 7.360377358490567e-05,
      "loss": 1.8843,
      "step": 22600
    },
    {
      "epoch": 4.266037735849056,
      "grad_norm": 0.8120017647743225,
      "learning_rate": 7.341509433962264e-05,
      "loss": 1.833,
      "step": 22610
    },
    {
      "epoch": 4.2679245283018865,
      "grad_norm": 0.8450627326965332,
      "learning_rate": 7.322641509433962e-05,
      "loss": 1.8036,
      "step": 22620
    },
    {
      "epoch": 4.269811320754717,
      "grad_norm": 0.8455654382705688,
      "learning_rate": 7.303773584905661e-05,
      "loss": 1.762,
      "step": 22630
    },
    {
      "epoch": 4.271698113207547,
      "grad_norm": 0.9074849486351013,
      "learning_rate": 7.284905660377358e-05,
      "loss": 1.8762,
      "step": 22640
    },
    {
      "epoch": 4.273584905660377,
      "grad_norm": 0.8919714689254761,
      "learning_rate": 7.266037735849056e-05,
      "loss": 1.9095,
      "step": 22650
    },
    {
      "epoch": 4.275471698113208,
      "grad_norm": 0.8112391233444214,
      "learning_rate": 7.247169811320755e-05,
      "loss": 1.8823,
      "step": 22660
    },
    {
      "epoch": 4.277358490566038,
      "grad_norm": 0.8128962516784668,
      "learning_rate": 7.228301886792453e-05,
      "loss": 1.7974,
      "step": 22670
    },
    {
      "epoch": 4.279245283018868,
      "grad_norm": 0.7800317406654358,
      "learning_rate": 7.209433962264151e-05,
      "loss": 1.8306,
      "step": 22680
    },
    {
      "epoch": 4.281132075471698,
      "grad_norm": 0.804566502571106,
      "learning_rate": 7.190566037735849e-05,
      "loss": 1.8306,
      "step": 22690
    },
    {
      "epoch": 4.283018867924528,
      "grad_norm": 0.8091212511062622,
      "learning_rate": 7.171698113207548e-05,
      "loss": 1.8255,
      "step": 22700
    },
    {
      "epoch": 4.284905660377358,
      "grad_norm": 0.7677069902420044,
      "learning_rate": 7.152830188679244e-05,
      "loss": 1.8948,
      "step": 22710
    },
    {
      "epoch": 4.286792452830189,
      "grad_norm": 0.7870184779167175,
      "learning_rate": 7.133962264150944e-05,
      "loss": 1.7964,
      "step": 22720
    },
    {
      "epoch": 4.288679245283019,
      "grad_norm": 0.8243333697319031,
      "learning_rate": 7.115094339622642e-05,
      "loss": 1.77,
      "step": 22730
    },
    {
      "epoch": 4.290566037735849,
      "grad_norm": 0.8120871186256409,
      "learning_rate": 7.096226415094339e-05,
      "loss": 1.8597,
      "step": 22740
    },
    {
      "epoch": 4.2924528301886795,
      "grad_norm": 0.7807713150978088,
      "learning_rate": 7.077358490566039e-05,
      "loss": 1.7963,
      "step": 22750
    },
    {
      "epoch": 4.29433962264151,
      "grad_norm": 0.7871518731117249,
      "learning_rate": 7.058490566037735e-05,
      "loss": 1.9099,
      "step": 22760
    },
    {
      "epoch": 4.296226415094339,
      "grad_norm": 0.7998724579811096,
      "learning_rate": 7.039622641509434e-05,
      "loss": 1.8884,
      "step": 22770
    },
    {
      "epoch": 4.2981132075471695,
      "grad_norm": 0.8306642770767212,
      "learning_rate": 7.020754716981133e-05,
      "loss": 1.8381,
      "step": 22780
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.8224890828132629,
      "learning_rate": 7.00188679245283e-05,
      "loss": 1.8738,
      "step": 22790
    },
    {
      "epoch": 4.30188679245283,
      "grad_norm": 0.7786837220191956,
      "learning_rate": 6.983018867924528e-05,
      "loss": 1.8935,
      "step": 22800
    },
    {
      "epoch": 4.30377358490566,
      "grad_norm": 0.8183304071426392,
      "learning_rate": 6.964150943396227e-05,
      "loss": 1.7873,
      "step": 22810
    },
    {
      "epoch": 4.305660377358491,
      "grad_norm": 0.9164419770240784,
      "learning_rate": 6.945283018867925e-05,
      "loss": 1.825,
      "step": 22820
    },
    {
      "epoch": 4.307547169811321,
      "grad_norm": 0.8046131134033203,
      "learning_rate": 6.926415094339623e-05,
      "loss": 1.8793,
      "step": 22830
    },
    {
      "epoch": 4.309433962264151,
      "grad_norm": 0.7714215517044067,
      "learning_rate": 6.907547169811321e-05,
      "loss": 1.7968,
      "step": 22840
    },
    {
      "epoch": 4.311320754716981,
      "grad_norm": 0.9007762670516968,
      "learning_rate": 6.88867924528302e-05,
      "loss": 1.789,
      "step": 22850
    },
    {
      "epoch": 4.313207547169811,
      "grad_norm": 0.7899248600006104,
      "learning_rate": 6.869811320754716e-05,
      "loss": 1.9151,
      "step": 22860
    },
    {
      "epoch": 4.315094339622641,
      "grad_norm": 0.8150270581245422,
      "learning_rate": 6.850943396226416e-05,
      "loss": 1.7918,
      "step": 22870
    },
    {
      "epoch": 4.316981132075472,
      "grad_norm": 0.8378624320030212,
      "learning_rate": 6.832075471698114e-05,
      "loss": 1.8207,
      "step": 22880
    },
    {
      "epoch": 4.318867924528302,
      "grad_norm": 0.9124002456665039,
      "learning_rate": 6.813207547169811e-05,
      "loss": 1.8754,
      "step": 22890
    },
    {
      "epoch": 4.320754716981132,
      "grad_norm": 0.9152912497520447,
      "learning_rate": 6.79433962264151e-05,
      "loss": 1.7982,
      "step": 22900
    },
    {
      "epoch": 4.3226415094339625,
      "grad_norm": 0.807782769203186,
      "learning_rate": 6.775471698113207e-05,
      "loss": 1.8455,
      "step": 22910
    },
    {
      "epoch": 4.324528301886793,
      "grad_norm": 0.8195067644119263,
      "learning_rate": 6.756603773584906e-05,
      "loss": 1.7819,
      "step": 22920
    },
    {
      "epoch": 4.326415094339622,
      "grad_norm": 0.8692758679389954,
      "learning_rate": 6.737735849056605e-05,
      "loss": 1.772,
      "step": 22930
    },
    {
      "epoch": 4.3283018867924525,
      "grad_norm": 0.8199929594993591,
      "learning_rate": 6.718867924528302e-05,
      "loss": 1.8398,
      "step": 22940
    },
    {
      "epoch": 4.330188679245283,
      "grad_norm": 0.8956172466278076,
      "learning_rate": 6.7e-05,
      "loss": 1.8344,
      "step": 22950
    },
    {
      "epoch": 4.332075471698113,
      "grad_norm": 0.8700723648071289,
      "learning_rate": 6.681132075471698e-05,
      "loss": 1.818,
      "step": 22960
    },
    {
      "epoch": 4.333962264150943,
      "grad_norm": 0.8223277926445007,
      "learning_rate": 6.662264150943397e-05,
      "loss": 1.827,
      "step": 22970
    },
    {
      "epoch": 4.335849056603774,
      "grad_norm": 0.7835814356803894,
      "learning_rate": 6.643396226415095e-05,
      "loss": 1.7337,
      "step": 22980
    },
    {
      "epoch": 4.337735849056604,
      "grad_norm": 0.7529476284980774,
      "learning_rate": 6.624528301886793e-05,
      "loss": 1.8363,
      "step": 22990
    },
    {
      "epoch": 4.339622641509434,
      "grad_norm": 0.9092597365379333,
      "learning_rate": 6.605660377358491e-05,
      "loss": 1.8727,
      "step": 23000
    },
    {
      "epoch": 4.341509433962264,
      "grad_norm": 0.8536440134048462,
      "learning_rate": 6.586792452830188e-05,
      "loss": 1.8599,
      "step": 23010
    },
    {
      "epoch": 4.343396226415094,
      "grad_norm": 0.7575751543045044,
      "learning_rate": 6.567924528301888e-05,
      "loss": 1.8437,
      "step": 23020
    },
    {
      "epoch": 4.345283018867924,
      "grad_norm": 0.9154210686683655,
      "learning_rate": 6.549056603773585e-05,
      "loss": 1.8656,
      "step": 23030
    },
    {
      "epoch": 4.347169811320755,
      "grad_norm": 0.8754929304122925,
      "learning_rate": 6.530188679245283e-05,
      "loss": 1.8761,
      "step": 23040
    },
    {
      "epoch": 4.349056603773585,
      "grad_norm": 0.7643241882324219,
      "learning_rate": 6.511320754716981e-05,
      "loss": 1.8826,
      "step": 23050
    },
    {
      "epoch": 4.350943396226415,
      "grad_norm": 0.829908549785614,
      "learning_rate": 6.492452830188679e-05,
      "loss": 1.8477,
      "step": 23060
    },
    {
      "epoch": 4.3528301886792455,
      "grad_norm": 0.7682484984397888,
      "learning_rate": 6.473584905660377e-05,
      "loss": 1.8035,
      "step": 23070
    },
    {
      "epoch": 4.354716981132076,
      "grad_norm": 0.8830827474594116,
      "learning_rate": 6.454716981132074e-05,
      "loss": 1.8789,
      "step": 23080
    },
    {
      "epoch": 4.356603773584905,
      "grad_norm": 0.7722912430763245,
      "learning_rate": 6.435849056603774e-05,
      "loss": 1.7613,
      "step": 23090
    },
    {
      "epoch": 4.3584905660377355,
      "grad_norm": 0.9159267544746399,
      "learning_rate": 6.416981132075472e-05,
      "loss": 1.9162,
      "step": 23100
    },
    {
      "epoch": 4.360377358490566,
      "grad_norm": 0.8110901713371277,
      "learning_rate": 6.398113207547169e-05,
      "loss": 1.7965,
      "step": 23110
    },
    {
      "epoch": 4.362264150943396,
      "grad_norm": 0.8251380920410156,
      "learning_rate": 6.379245283018869e-05,
      "loss": 1.8141,
      "step": 23120
    },
    {
      "epoch": 4.3641509433962264,
      "grad_norm": 0.7378061413764954,
      "learning_rate": 6.360377358490565e-05,
      "loss": 1.831,
      "step": 23130
    },
    {
      "epoch": 4.366037735849057,
      "grad_norm": 0.9479081034660339,
      "learning_rate": 6.341509433962264e-05,
      "loss": 1.809,
      "step": 23140
    },
    {
      "epoch": 4.367924528301887,
      "grad_norm": 0.8264332413673401,
      "learning_rate": 6.322641509433963e-05,
      "loss": 1.8231,
      "step": 23150
    },
    {
      "epoch": 4.369811320754717,
      "grad_norm": 0.8370187282562256,
      "learning_rate": 6.30377358490566e-05,
      "loss": 1.8164,
      "step": 23160
    },
    {
      "epoch": 4.371698113207547,
      "grad_norm": 0.9241850972175598,
      "learning_rate": 6.284905660377358e-05,
      "loss": 1.8624,
      "step": 23170
    },
    {
      "epoch": 4.373584905660377,
      "grad_norm": 0.9384285807609558,
      "learning_rate": 6.266037735849057e-05,
      "loss": 1.8071,
      "step": 23180
    },
    {
      "epoch": 4.375471698113207,
      "grad_norm": 0.7704098224639893,
      "learning_rate": 6.247169811320755e-05,
      "loss": 1.8336,
      "step": 23190
    },
    {
      "epoch": 4.377358490566038,
      "grad_norm": 0.7392914891242981,
      "learning_rate": 6.228301886792453e-05,
      "loss": 1.6948,
      "step": 23200
    },
    {
      "epoch": 4.379245283018868,
      "grad_norm": 0.8857674598693848,
      "learning_rate": 6.209433962264151e-05,
      "loss": 1.8197,
      "step": 23210
    },
    {
      "epoch": 4.381132075471698,
      "grad_norm": 0.8573009967803955,
      "learning_rate": 6.19056603773585e-05,
      "loss": 1.7867,
      "step": 23220
    },
    {
      "epoch": 4.3830188679245285,
      "grad_norm": 0.8477413058280945,
      "learning_rate": 6.171698113207548e-05,
      "loss": 1.8573,
      "step": 23230
    },
    {
      "epoch": 4.384905660377359,
      "grad_norm": 0.8477188944816589,
      "learning_rate": 6.152830188679246e-05,
      "loss": 1.8193,
      "step": 23240
    },
    {
      "epoch": 4.386792452830189,
      "grad_norm": 0.8340094685554504,
      "learning_rate": 6.133962264150944e-05,
      "loss": 1.7947,
      "step": 23250
    },
    {
      "epoch": 4.388679245283019,
      "grad_norm": 0.7910133600234985,
      "learning_rate": 6.115094339622641e-05,
      "loss": 1.8135,
      "step": 23260
    },
    {
      "epoch": 4.390566037735849,
      "grad_norm": 0.7510846853256226,
      "learning_rate": 6.09622641509434e-05,
      "loss": 1.8432,
      "step": 23270
    },
    {
      "epoch": 4.392452830188679,
      "grad_norm": 0.7875452041625977,
      "learning_rate": 6.077358490566038e-05,
      "loss": 1.9012,
      "step": 23280
    },
    {
      "epoch": 4.3943396226415095,
      "grad_norm": 0.7973111867904663,
      "learning_rate": 6.058490566037736e-05,
      "loss": 1.8261,
      "step": 23290
    },
    {
      "epoch": 4.39622641509434,
      "grad_norm": 0.7959917783737183,
      "learning_rate": 6.039622641509434e-05,
      "loss": 1.7172,
      "step": 23300
    },
    {
      "epoch": 4.39811320754717,
      "grad_norm": 0.871900200843811,
      "learning_rate": 6.020754716981132e-05,
      "loss": 1.8531,
      "step": 23310
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.9315992593765259,
      "learning_rate": 6.001886792452831e-05,
      "loss": 1.8822,
      "step": 23320
    },
    {
      "epoch": 4.40188679245283,
      "grad_norm": 0.865709662437439,
      "learning_rate": 5.9830188679245284e-05,
      "loss": 1.885,
      "step": 23330
    },
    {
      "epoch": 4.40377358490566,
      "grad_norm": 0.8108213543891907,
      "learning_rate": 5.9641509433962266e-05,
      "loss": 1.8323,
      "step": 23340
    },
    {
      "epoch": 4.40566037735849,
      "grad_norm": 0.8098715543746948,
      "learning_rate": 5.945283018867925e-05,
      "loss": 1.8531,
      "step": 23350
    },
    {
      "epoch": 4.407547169811321,
      "grad_norm": 0.8074396252632141,
      "learning_rate": 5.9264150943396224e-05,
      "loss": 1.8793,
      "step": 23360
    },
    {
      "epoch": 4.409433962264151,
      "grad_norm": 0.7298198342323303,
      "learning_rate": 5.907547169811321e-05,
      "loss": 1.8459,
      "step": 23370
    },
    {
      "epoch": 4.411320754716981,
      "grad_norm": 0.7497568130493164,
      "learning_rate": 5.888679245283019e-05,
      "loss": 1.7688,
      "step": 23380
    },
    {
      "epoch": 4.413207547169812,
      "grad_norm": 0.8804759383201599,
      "learning_rate": 5.869811320754717e-05,
      "loss": 1.8757,
      "step": 23390
    },
    {
      "epoch": 4.415094339622642,
      "grad_norm": 0.7953980565071106,
      "learning_rate": 5.850943396226415e-05,
      "loss": 1.8127,
      "step": 23400
    },
    {
      "epoch": 4.416981132075472,
      "grad_norm": 0.8255484700202942,
      "learning_rate": 5.832075471698113e-05,
      "loss": 1.8791,
      "step": 23410
    },
    {
      "epoch": 4.418867924528302,
      "grad_norm": 0.8091731667518616,
      "learning_rate": 5.813207547169812e-05,
      "loss": 1.8464,
      "step": 23420
    },
    {
      "epoch": 4.420754716981132,
      "grad_norm": 0.916444718837738,
      "learning_rate": 5.79433962264151e-05,
      "loss": 1.7804,
      "step": 23430
    },
    {
      "epoch": 4.422641509433962,
      "grad_norm": 0.8360894322395325,
      "learning_rate": 5.7754716981132074e-05,
      "loss": 1.8141,
      "step": 23440
    },
    {
      "epoch": 4.4245283018867925,
      "grad_norm": 0.8535788059234619,
      "learning_rate": 5.756603773584906e-05,
      "loss": 1.8808,
      "step": 23450
    },
    {
      "epoch": 4.426415094339623,
      "grad_norm": 0.8966323733329773,
      "learning_rate": 5.737735849056604e-05,
      "loss": 1.8825,
      "step": 23460
    },
    {
      "epoch": 4.428301886792453,
      "grad_norm": 0.8176464438438416,
      "learning_rate": 5.718867924528302e-05,
      "loss": 1.7693,
      "step": 23470
    },
    {
      "epoch": 4.430188679245283,
      "grad_norm": 0.8357034921646118,
      "learning_rate": 5.7e-05,
      "loss": 1.8593,
      "step": 23480
    },
    {
      "epoch": 4.432075471698113,
      "grad_norm": 0.9633771181106567,
      "learning_rate": 5.6811320754716985e-05,
      "loss": 1.8664,
      "step": 23490
    },
    {
      "epoch": 4.433962264150943,
      "grad_norm": 0.8641947507858276,
      "learning_rate": 5.662264150943396e-05,
      "loss": 1.9864,
      "step": 23500
    },
    {
      "epoch": 4.435849056603773,
      "grad_norm": 0.8299365043640137,
      "learning_rate": 5.643396226415094e-05,
      "loss": 1.7489,
      "step": 23510
    },
    {
      "epoch": 4.437735849056604,
      "grad_norm": 0.7673210501670837,
      "learning_rate": 5.6245283018867925e-05,
      "loss": 1.7924,
      "step": 23520
    },
    {
      "epoch": 4.439622641509434,
      "grad_norm": 0.9720706343650818,
      "learning_rate": 5.605660377358491e-05,
      "loss": 1.8332,
      "step": 23530
    },
    {
      "epoch": 4.441509433962264,
      "grad_norm": 0.8959614038467407,
      "learning_rate": 5.586792452830189e-05,
      "loss": 1.9221,
      "step": 23540
    },
    {
      "epoch": 4.443396226415095,
      "grad_norm": 0.7441970705986023,
      "learning_rate": 5.567924528301887e-05,
      "loss": 1.7851,
      "step": 23550
    },
    {
      "epoch": 4.445283018867925,
      "grad_norm": 0.8361460566520691,
      "learning_rate": 5.549056603773585e-05,
      "loss": 1.7931,
      "step": 23560
    },
    {
      "epoch": 4.447169811320755,
      "grad_norm": 0.9213616251945496,
      "learning_rate": 5.530188679245283e-05,
      "loss": 1.8735,
      "step": 23570
    },
    {
      "epoch": 4.449056603773585,
      "grad_norm": 1.0163122415542603,
      "learning_rate": 5.511320754716982e-05,
      "loss": 1.8399,
      "step": 23580
    },
    {
      "epoch": 4.450943396226415,
      "grad_norm": 0.8123090267181396,
      "learning_rate": 5.4924528301886793e-05,
      "loss": 1.8445,
      "step": 23590
    },
    {
      "epoch": 4.452830188679245,
      "grad_norm": 0.7665212154388428,
      "learning_rate": 5.4735849056603776e-05,
      "loss": 1.9359,
      "step": 23600
    },
    {
      "epoch": 4.4547169811320755,
      "grad_norm": 0.7736069560050964,
      "learning_rate": 5.454716981132075e-05,
      "loss": 1.8654,
      "step": 23610
    },
    {
      "epoch": 4.456603773584906,
      "grad_norm": 0.7278690338134766,
      "learning_rate": 5.435849056603773e-05,
      "loss": 1.8767,
      "step": 23620
    },
    {
      "epoch": 4.458490566037736,
      "grad_norm": 0.8648747205734253,
      "learning_rate": 5.416981132075472e-05,
      "loss": 1.8059,
      "step": 23630
    },
    {
      "epoch": 4.460377358490566,
      "grad_norm": 0.789692759513855,
      "learning_rate": 5.39811320754717e-05,
      "loss": 1.8421,
      "step": 23640
    },
    {
      "epoch": 4.462264150943396,
      "grad_norm": 0.7669757604598999,
      "learning_rate": 5.379245283018868e-05,
      "loss": 1.9232,
      "step": 23650
    },
    {
      "epoch": 4.464150943396226,
      "grad_norm": 0.9550576210021973,
      "learning_rate": 5.360377358490566e-05,
      "loss": 1.8529,
      "step": 23660
    },
    {
      "epoch": 4.466037735849056,
      "grad_norm": 0.7768329381942749,
      "learning_rate": 5.341509433962264e-05,
      "loss": 1.8471,
      "step": 23670
    },
    {
      "epoch": 4.467924528301887,
      "grad_norm": 0.8582779765129089,
      "learning_rate": 5.3226415094339626e-05,
      "loss": 1.8227,
      "step": 23680
    },
    {
      "epoch": 4.469811320754717,
      "grad_norm": 0.8649022579193115,
      "learning_rate": 5.303773584905661e-05,
      "loss": 1.8587,
      "step": 23690
    },
    {
      "epoch": 4.471698113207547,
      "grad_norm": 0.717828094959259,
      "learning_rate": 5.2849056603773584e-05,
      "loss": 1.8717,
      "step": 23700
    },
    {
      "epoch": 4.473584905660378,
      "grad_norm": 0.8825533986091614,
      "learning_rate": 5.2660377358490566e-05,
      "loss": 1.7817,
      "step": 23710
    },
    {
      "epoch": 4.475471698113208,
      "grad_norm": 0.7743639945983887,
      "learning_rate": 5.247169811320755e-05,
      "loss": 1.8458,
      "step": 23720
    },
    {
      "epoch": 4.477358490566038,
      "grad_norm": 0.9056215882301331,
      "learning_rate": 5.228301886792453e-05,
      "loss": 1.9144,
      "step": 23730
    },
    {
      "epoch": 4.479245283018868,
      "grad_norm": 0.7903825640678406,
      "learning_rate": 5.209433962264151e-05,
      "loss": 1.8738,
      "step": 23740
    },
    {
      "epoch": 4.481132075471698,
      "grad_norm": 0.8027487397193909,
      "learning_rate": 5.1905660377358495e-05,
      "loss": 1.7674,
      "step": 23750
    },
    {
      "epoch": 4.483018867924528,
      "grad_norm": 0.8544895648956299,
      "learning_rate": 5.171698113207547e-05,
      "loss": 1.7961,
      "step": 23760
    },
    {
      "epoch": 4.4849056603773585,
      "grad_norm": 0.9969236254692078,
      "learning_rate": 5.152830188679245e-05,
      "loss": 1.8344,
      "step": 23770
    },
    {
      "epoch": 4.486792452830189,
      "grad_norm": 0.827660322189331,
      "learning_rate": 5.133962264150944e-05,
      "loss": 1.8437,
      "step": 23780
    },
    {
      "epoch": 4.488679245283019,
      "grad_norm": 0.9311479330062866,
      "learning_rate": 5.1150943396226417e-05,
      "loss": 1.767,
      "step": 23790
    },
    {
      "epoch": 4.490566037735849,
      "grad_norm": 0.7963433265686035,
      "learning_rate": 5.09622641509434e-05,
      "loss": 1.7648,
      "step": 23800
    },
    {
      "epoch": 4.492452830188679,
      "grad_norm": 0.8211677670478821,
      "learning_rate": 5.077358490566038e-05,
      "loss": 1.8745,
      "step": 23810
    },
    {
      "epoch": 4.494339622641509,
      "grad_norm": 0.9149044156074524,
      "learning_rate": 5.0584905660377356e-05,
      "loss": 1.847,
      "step": 23820
    },
    {
      "epoch": 4.496226415094339,
      "grad_norm": 0.8633175492286682,
      "learning_rate": 5.039622641509434e-05,
      "loss": 1.7752,
      "step": 23830
    },
    {
      "epoch": 4.49811320754717,
      "grad_norm": 0.7934204936027527,
      "learning_rate": 5.020754716981132e-05,
      "loss": 1.7762,
      "step": 23840
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.8490323424339294,
      "learning_rate": 5.00188679245283e-05,
      "loss": 1.8233,
      "step": 23850
    },
    {
      "epoch": 4.50188679245283,
      "grad_norm": 0.8503009080886841,
      "learning_rate": 4.9830188679245285e-05,
      "loss": 1.8648,
      "step": 23860
    },
    {
      "epoch": 4.503773584905661,
      "grad_norm": 1.007946252822876,
      "learning_rate": 4.964150943396226e-05,
      "loss": 1.873,
      "step": 23870
    },
    {
      "epoch": 4.505660377358491,
      "grad_norm": 0.8138508200645447,
      "learning_rate": 4.945283018867924e-05,
      "loss": 1.8258,
      "step": 23880
    },
    {
      "epoch": 4.507547169811321,
      "grad_norm": 0.7482244372367859,
      "learning_rate": 4.926415094339623e-05,
      "loss": 1.8476,
      "step": 23890
    },
    {
      "epoch": 4.509433962264151,
      "grad_norm": 0.8632346987724304,
      "learning_rate": 4.907547169811321e-05,
      "loss": 1.7981,
      "step": 23900
    },
    {
      "epoch": 4.511320754716981,
      "grad_norm": 0.9657918810844421,
      "learning_rate": 4.888679245283019e-05,
      "loss": 1.8416,
      "step": 23910
    },
    {
      "epoch": 4.513207547169811,
      "grad_norm": 0.8120434880256653,
      "learning_rate": 4.869811320754717e-05,
      "loss": 1.8218,
      "step": 23920
    },
    {
      "epoch": 4.5150943396226415,
      "grad_norm": 0.9228957295417786,
      "learning_rate": 4.850943396226415e-05,
      "loss": 1.8367,
      "step": 23930
    },
    {
      "epoch": 4.516981132075472,
      "grad_norm": 0.8661306500434875,
      "learning_rate": 4.8320754716981136e-05,
      "loss": 1.8387,
      "step": 23940
    },
    {
      "epoch": 4.518867924528302,
      "grad_norm": 0.970544159412384,
      "learning_rate": 4.813207547169812e-05,
      "loss": 1.8367,
      "step": 23950
    },
    {
      "epoch": 4.520754716981132,
      "grad_norm": 0.8973297476768494,
      "learning_rate": 4.794339622641509e-05,
      "loss": 1.8158,
      "step": 23960
    },
    {
      "epoch": 4.522641509433962,
      "grad_norm": 0.8114812970161438,
      "learning_rate": 4.7754716981132075e-05,
      "loss": 1.8353,
      "step": 23970
    },
    {
      "epoch": 4.524528301886792,
      "grad_norm": 0.7986583113670349,
      "learning_rate": 4.756603773584906e-05,
      "loss": 1.8233,
      "step": 23980
    },
    {
      "epoch": 4.526415094339622,
      "grad_norm": 0.8218985795974731,
      "learning_rate": 4.737735849056604e-05,
      "loss": 1.8465,
      "step": 23990
    },
    {
      "epoch": 4.528301886792453,
      "grad_norm": 0.8906697034835815,
      "learning_rate": 4.718867924528302e-05,
      "loss": 1.8636,
      "step": 24000
    },
    {
      "epoch": 4.530188679245283,
      "grad_norm": 0.8484621644020081,
      "learning_rate": 4.7000000000000004e-05,
      "loss": 1.937,
      "step": 24010
    },
    {
      "epoch": 4.532075471698113,
      "grad_norm": 0.8363631963729858,
      "learning_rate": 4.681132075471698e-05,
      "loss": 1.8187,
      "step": 24020
    },
    {
      "epoch": 4.533962264150944,
      "grad_norm": 0.8139674067497253,
      "learning_rate": 4.662264150943396e-05,
      "loss": 1.816,
      "step": 24030
    },
    {
      "epoch": 4.535849056603774,
      "grad_norm": 0.814483642578125,
      "learning_rate": 4.643396226415095e-05,
      "loss": 1.8932,
      "step": 24040
    },
    {
      "epoch": 4.537735849056604,
      "grad_norm": 0.7627200484275818,
      "learning_rate": 4.6245283018867926e-05,
      "loss": 1.8649,
      "step": 24050
    },
    {
      "epoch": 4.539622641509434,
      "grad_norm": 0.8123758435249329,
      "learning_rate": 4.605660377358491e-05,
      "loss": 1.8736,
      "step": 24060
    },
    {
      "epoch": 4.541509433962264,
      "grad_norm": 0.7629175186157227,
      "learning_rate": 4.5867924528301883e-05,
      "loss": 1.8745,
      "step": 24070
    },
    {
      "epoch": 4.543396226415094,
      "grad_norm": 0.8702850937843323,
      "learning_rate": 4.5679245283018866e-05,
      "loss": 1.8253,
      "step": 24080
    },
    {
      "epoch": 4.5452830188679245,
      "grad_norm": 0.8934721946716309,
      "learning_rate": 4.5490566037735855e-05,
      "loss": 1.8897,
      "step": 24090
    },
    {
      "epoch": 4.547169811320755,
      "grad_norm": 0.8285563588142395,
      "learning_rate": 4.530188679245283e-05,
      "loss": 1.8771,
      "step": 24100
    },
    {
      "epoch": 4.549056603773585,
      "grad_norm": 0.8263369798660278,
      "learning_rate": 4.511320754716981e-05,
      "loss": 1.832,
      "step": 24110
    },
    {
      "epoch": 4.550943396226415,
      "grad_norm": 0.8523149490356445,
      "learning_rate": 4.4924528301886794e-05,
      "loss": 1.8294,
      "step": 24120
    },
    {
      "epoch": 4.552830188679245,
      "grad_norm": 0.8357957601547241,
      "learning_rate": 4.473584905660377e-05,
      "loss": 1.8538,
      "step": 24130
    },
    {
      "epoch": 4.554716981132075,
      "grad_norm": 0.8224046230316162,
      "learning_rate": 4.454716981132076e-05,
      "loss": 1.8886,
      "step": 24140
    },
    {
      "epoch": 4.556603773584905,
      "grad_norm": 0.8627557754516602,
      "learning_rate": 4.435849056603774e-05,
      "loss": 1.8511,
      "step": 24150
    },
    {
      "epoch": 4.558490566037736,
      "grad_norm": 0.8926449418067932,
      "learning_rate": 4.4169811320754716e-05,
      "loss": 1.9149,
      "step": 24160
    },
    {
      "epoch": 4.560377358490566,
      "grad_norm": 0.911345899105072,
      "learning_rate": 4.39811320754717e-05,
      "loss": 1.8447,
      "step": 24170
    },
    {
      "epoch": 4.562264150943396,
      "grad_norm": 0.9036703109741211,
      "learning_rate": 4.379245283018868e-05,
      "loss": 1.9003,
      "step": 24180
    },
    {
      "epoch": 4.564150943396227,
      "grad_norm": 0.8132584691047668,
      "learning_rate": 4.3603773584905656e-05,
      "loss": 1.7872,
      "step": 24190
    },
    {
      "epoch": 4.566037735849057,
      "grad_norm": 0.8326340317726135,
      "learning_rate": 4.3415094339622645e-05,
      "loss": 1.819,
      "step": 24200
    },
    {
      "epoch": 4.567924528301887,
      "grad_norm": 0.8054159283638,
      "learning_rate": 4.322641509433963e-05,
      "loss": 1.9085,
      "step": 24210
    },
    {
      "epoch": 4.569811320754717,
      "grad_norm": 0.7212397456169128,
      "learning_rate": 4.30377358490566e-05,
      "loss": 1.854,
      "step": 24220
    },
    {
      "epoch": 4.571698113207547,
      "grad_norm": 0.8474342226982117,
      "learning_rate": 4.2849056603773585e-05,
      "loss": 1.8789,
      "step": 24230
    },
    {
      "epoch": 4.573584905660377,
      "grad_norm": 0.8208396434783936,
      "learning_rate": 4.266037735849057e-05,
      "loss": 1.8986,
      "step": 24240
    },
    {
      "epoch": 4.5754716981132075,
      "grad_norm": 0.7820250391960144,
      "learning_rate": 4.247169811320755e-05,
      "loss": 1.8972,
      "step": 24250
    },
    {
      "epoch": 4.577358490566038,
      "grad_norm": 0.8778645992279053,
      "learning_rate": 4.228301886792453e-05,
      "loss": 1.8363,
      "step": 24260
    },
    {
      "epoch": 4.579245283018868,
      "grad_norm": 0.8240214586257935,
      "learning_rate": 4.209433962264151e-05,
      "loss": 1.8252,
      "step": 24270
    },
    {
      "epoch": 4.581132075471698,
      "grad_norm": 0.8219183683395386,
      "learning_rate": 4.190566037735849e-05,
      "loss": 1.8802,
      "step": 24280
    },
    {
      "epoch": 4.583018867924528,
      "grad_norm": 0.7736845016479492,
      "learning_rate": 4.171698113207547e-05,
      "loss": 1.82,
      "step": 24290
    },
    {
      "epoch": 4.584905660377358,
      "grad_norm": 0.7947128415107727,
      "learning_rate": 4.152830188679245e-05,
      "loss": 1.7943,
      "step": 24300
    },
    {
      "epoch": 4.586792452830188,
      "grad_norm": 0.8329797387123108,
      "learning_rate": 4.1339622641509435e-05,
      "loss": 1.8861,
      "step": 24310
    },
    {
      "epoch": 4.588679245283019,
      "grad_norm": 0.8799788355827332,
      "learning_rate": 4.115094339622642e-05,
      "loss": 1.8646,
      "step": 24320
    },
    {
      "epoch": 4.590566037735849,
      "grad_norm": 0.778907060623169,
      "learning_rate": 4.096226415094339e-05,
      "loss": 1.8596,
      "step": 24330
    },
    {
      "epoch": 4.592452830188679,
      "grad_norm": 0.832582950592041,
      "learning_rate": 4.0773584905660375e-05,
      "loss": 1.8714,
      "step": 24340
    },
    {
      "epoch": 4.59433962264151,
      "grad_norm": 0.7641430497169495,
      "learning_rate": 4.0584905660377364e-05,
      "loss": 1.799,
      "step": 24350
    },
    {
      "epoch": 4.59622641509434,
      "grad_norm": 0.7768657207489014,
      "learning_rate": 4.039622641509434e-05,
      "loss": 1.7862,
      "step": 24360
    },
    {
      "epoch": 4.59811320754717,
      "grad_norm": 0.8001080751419067,
      "learning_rate": 4.020754716981132e-05,
      "loss": 1.8348,
      "step": 24370
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.8293516039848328,
      "learning_rate": 4.0018867924528304e-05,
      "loss": 1.8846,
      "step": 24380
    },
    {
      "epoch": 4.60188679245283,
      "grad_norm": 0.7351784706115723,
      "learning_rate": 3.983018867924528e-05,
      "loss": 1.8301,
      "step": 24390
    },
    {
      "epoch": 4.60377358490566,
      "grad_norm": 0.7870386242866516,
      "learning_rate": 3.964150943396227e-05,
      "loss": 1.8774,
      "step": 24400
    },
    {
      "epoch": 4.6056603773584905,
      "grad_norm": 0.810552179813385,
      "learning_rate": 3.945283018867925e-05,
      "loss": 1.8669,
      "step": 24410
    },
    {
      "epoch": 4.607547169811321,
      "grad_norm": 0.9478758573532104,
      "learning_rate": 3.9264150943396226e-05,
      "loss": 1.7658,
      "step": 24420
    },
    {
      "epoch": 4.609433962264151,
      "grad_norm": 0.7770474553108215,
      "learning_rate": 3.907547169811321e-05,
      "loss": 1.8038,
      "step": 24430
    },
    {
      "epoch": 4.611320754716981,
      "grad_norm": 0.7942537069320679,
      "learning_rate": 3.888679245283019e-05,
      "loss": 1.858,
      "step": 24440
    },
    {
      "epoch": 4.613207547169811,
      "grad_norm": 0.8915954232215881,
      "learning_rate": 3.869811320754717e-05,
      "loss": 1.7828,
      "step": 24450
    },
    {
      "epoch": 4.615094339622641,
      "grad_norm": 0.8175638318061829,
      "learning_rate": 3.8509433962264154e-05,
      "loss": 1.8472,
      "step": 24460
    },
    {
      "epoch": 4.6169811320754715,
      "grad_norm": 0.8970417380332947,
      "learning_rate": 3.8320754716981136e-05,
      "loss": 1.924,
      "step": 24470
    },
    {
      "epoch": 4.618867924528302,
      "grad_norm": 0.8162537217140198,
      "learning_rate": 3.813207547169811e-05,
      "loss": 1.8906,
      "step": 24480
    },
    {
      "epoch": 4.620754716981132,
      "grad_norm": 0.7595642805099487,
      "learning_rate": 3.7943396226415094e-05,
      "loss": 1.8367,
      "step": 24490
    },
    {
      "epoch": 4.622641509433962,
      "grad_norm": 0.7816939949989319,
      "learning_rate": 3.7754716981132076e-05,
      "loss": 1.7434,
      "step": 24500
    },
    {
      "epoch": 4.624528301886793,
      "grad_norm": 0.8313173651695251,
      "learning_rate": 3.756603773584906e-05,
      "loss": 1.7823,
      "step": 24510
    },
    {
      "epoch": 4.626415094339623,
      "grad_norm": 0.8637905716896057,
      "learning_rate": 3.737735849056604e-05,
      "loss": 1.8073,
      "step": 24520
    },
    {
      "epoch": 4.628301886792453,
      "grad_norm": 0.8548377156257629,
      "learning_rate": 3.7188679245283016e-05,
      "loss": 1.722,
      "step": 24530
    },
    {
      "epoch": 4.630188679245283,
      "grad_norm": 0.879370391368866,
      "learning_rate": 3.7e-05,
      "loss": 1.8447,
      "step": 24540
    },
    {
      "epoch": 4.632075471698113,
      "grad_norm": 0.9159022569656372,
      "learning_rate": 3.681132075471698e-05,
      "loss": 1.8778,
      "step": 24550
    },
    {
      "epoch": 4.633962264150943,
      "grad_norm": 0.7911200523376465,
      "learning_rate": 3.662264150943396e-05,
      "loss": 1.7467,
      "step": 24560
    },
    {
      "epoch": 4.6358490566037736,
      "grad_norm": 0.937264084815979,
      "learning_rate": 3.6433962264150945e-05,
      "loss": 1.8507,
      "step": 24570
    },
    {
      "epoch": 4.637735849056604,
      "grad_norm": 0.9217870235443115,
      "learning_rate": 3.624528301886793e-05,
      "loss": 1.7778,
      "step": 24580
    },
    {
      "epoch": 4.639622641509434,
      "grad_norm": 0.8250731229782104,
      "learning_rate": 3.60566037735849e-05,
      "loss": 1.8659,
      "step": 24590
    },
    {
      "epoch": 4.6415094339622645,
      "grad_norm": 0.8374814391136169,
      "learning_rate": 3.5867924528301884e-05,
      "loss": 1.7655,
      "step": 24600
    },
    {
      "epoch": 4.643396226415094,
      "grad_norm": 0.8574469089508057,
      "learning_rate": 3.567924528301887e-05,
      "loss": 1.7566,
      "step": 24610
    },
    {
      "epoch": 4.645283018867924,
      "grad_norm": 0.7795727252960205,
      "learning_rate": 3.549056603773585e-05,
      "loss": 1.794,
      "step": 24620
    },
    {
      "epoch": 4.6471698113207545,
      "grad_norm": 0.8877094984054565,
      "learning_rate": 3.530188679245283e-05,
      "loss": 1.8522,
      "step": 24630
    },
    {
      "epoch": 4.649056603773585,
      "grad_norm": 0.7792847752571106,
      "learning_rate": 3.511320754716981e-05,
      "loss": 1.7638,
      "step": 24640
    },
    {
      "epoch": 4.650943396226415,
      "grad_norm": 0.7512099742889404,
      "learning_rate": 3.492452830188679e-05,
      "loss": 1.8403,
      "step": 24650
    },
    {
      "epoch": 4.652830188679245,
      "grad_norm": 0.7598662376403809,
      "learning_rate": 3.473584905660378e-05,
      "loss": 1.7585,
      "step": 24660
    },
    {
      "epoch": 4.654716981132076,
      "grad_norm": 0.7527235150337219,
      "learning_rate": 3.454716981132076e-05,
      "loss": 1.8523,
      "step": 24670
    },
    {
      "epoch": 4.656603773584906,
      "grad_norm": 0.7924087643623352,
      "learning_rate": 3.4358490566037735e-05,
      "loss": 1.84,
      "step": 24680
    },
    {
      "epoch": 4.658490566037736,
      "grad_norm": 0.7976658344268799,
      "learning_rate": 3.416981132075472e-05,
      "loss": 1.9687,
      "step": 24690
    },
    {
      "epoch": 4.660377358490566,
      "grad_norm": 1.2001748085021973,
      "learning_rate": 3.39811320754717e-05,
      "loss": 1.7891,
      "step": 24700
    },
    {
      "epoch": 4.662264150943396,
      "grad_norm": 0.9494526982307434,
      "learning_rate": 3.379245283018868e-05,
      "loss": 1.907,
      "step": 24710
    },
    {
      "epoch": 4.664150943396226,
      "grad_norm": 0.8306671380996704,
      "learning_rate": 3.3603773584905664e-05,
      "loss": 1.8714,
      "step": 24720
    },
    {
      "epoch": 4.666037735849057,
      "grad_norm": 0.8394619226455688,
      "learning_rate": 3.3415094339622646e-05,
      "loss": 1.7626,
      "step": 24730
    },
    {
      "epoch": 4.667924528301887,
      "grad_norm": 0.7703278064727783,
      "learning_rate": 3.322641509433962e-05,
      "loss": 1.7876,
      "step": 24740
    },
    {
      "epoch": 4.669811320754717,
      "grad_norm": 0.8595234751701355,
      "learning_rate": 3.30377358490566e-05,
      "loss": 1.9366,
      "step": 24750
    },
    {
      "epoch": 4.6716981132075475,
      "grad_norm": 0.7907372713088989,
      "learning_rate": 3.2849056603773586e-05,
      "loss": 1.836,
      "step": 24760
    },
    {
      "epoch": 4.673584905660377,
      "grad_norm": 0.9392917156219482,
      "learning_rate": 3.266037735849057e-05,
      "loss": 1.8211,
      "step": 24770
    },
    {
      "epoch": 4.675471698113207,
      "grad_norm": 0.7680258750915527,
      "learning_rate": 3.247169811320755e-05,
      "loss": 1.7943,
      "step": 24780
    },
    {
      "epoch": 4.6773584905660375,
      "grad_norm": 0.8551594614982605,
      "learning_rate": 3.2283018867924525e-05,
      "loss": 1.8486,
      "step": 24790
    },
    {
      "epoch": 4.679245283018868,
      "grad_norm": 0.8401094675064087,
      "learning_rate": 3.209433962264151e-05,
      "loss": 1.8303,
      "step": 24800
    },
    {
      "epoch": 4.681132075471698,
      "grad_norm": 0.8384758830070496,
      "learning_rate": 3.1905660377358496e-05,
      "loss": 1.8581,
      "step": 24810
    },
    {
      "epoch": 4.683018867924528,
      "grad_norm": 0.9614781141281128,
      "learning_rate": 3.171698113207547e-05,
      "loss": 1.8983,
      "step": 24820
    },
    {
      "epoch": 4.684905660377359,
      "grad_norm": 0.7898456454277039,
      "learning_rate": 3.1528301886792454e-05,
      "loss": 1.8708,
      "step": 24830
    },
    {
      "epoch": 4.686792452830189,
      "grad_norm": 0.7827144861221313,
      "learning_rate": 3.1339622641509436e-05,
      "loss": 1.8686,
      "step": 24840
    },
    {
      "epoch": 4.688679245283019,
      "grad_norm": 0.773356556892395,
      "learning_rate": 3.115094339622642e-05,
      "loss": 1.7087,
      "step": 24850
    },
    {
      "epoch": 4.690566037735849,
      "grad_norm": 0.8753179311752319,
      "learning_rate": 3.0962264150943394e-05,
      "loss": 1.827,
      "step": 24860
    },
    {
      "epoch": 4.692452830188679,
      "grad_norm": 0.8898637294769287,
      "learning_rate": 3.0773584905660376e-05,
      "loss": 1.8511,
      "step": 24870
    },
    {
      "epoch": 4.694339622641509,
      "grad_norm": 0.9529023170471191,
      "learning_rate": 3.058490566037736e-05,
      "loss": 1.7955,
      "step": 24880
    },
    {
      "epoch": 4.69622641509434,
      "grad_norm": 0.7647320032119751,
      "learning_rate": 3.039622641509434e-05,
      "loss": 1.7975,
      "step": 24890
    },
    {
      "epoch": 4.69811320754717,
      "grad_norm": 0.7946781516075134,
      "learning_rate": 3.020754716981132e-05,
      "loss": 1.8822,
      "step": 24900
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.7917987704277039,
      "learning_rate": 3.0018867924528305e-05,
      "loss": 1.8696,
      "step": 24910
    },
    {
      "epoch": 4.7018867924528305,
      "grad_norm": 0.7572168707847595,
      "learning_rate": 2.9830188679245283e-05,
      "loss": 1.8054,
      "step": 24920
    },
    {
      "epoch": 4.70377358490566,
      "grad_norm": 0.8351590037345886,
      "learning_rate": 2.9641509433962266e-05,
      "loss": 1.8917,
      "step": 24930
    },
    {
      "epoch": 4.70566037735849,
      "grad_norm": 0.8720479011535645,
      "learning_rate": 2.9452830188679244e-05,
      "loss": 1.8203,
      "step": 24940
    },
    {
      "epoch": 4.7075471698113205,
      "grad_norm": 0.7758538126945496,
      "learning_rate": 2.9264150943396226e-05,
      "loss": 1.841,
      "step": 24950
    },
    {
      "epoch": 4.709433962264151,
      "grad_norm": 0.9251312613487244,
      "learning_rate": 2.907547169811321e-05,
      "loss": 1.8406,
      "step": 24960
    },
    {
      "epoch": 4.711320754716981,
      "grad_norm": 0.8487018942832947,
      "learning_rate": 2.8886792452830187e-05,
      "loss": 1.8488,
      "step": 24970
    },
    {
      "epoch": 4.713207547169811,
      "grad_norm": 0.9799215197563171,
      "learning_rate": 2.8698113207547173e-05,
      "loss": 1.8427,
      "step": 24980
    },
    {
      "epoch": 4.715094339622642,
      "grad_norm": 0.8641600012779236,
      "learning_rate": 2.8509433962264152e-05,
      "loss": 1.8934,
      "step": 24990
    },
    {
      "epoch": 4.716981132075472,
      "grad_norm": 0.8489611148834229,
      "learning_rate": 2.832075471698113e-05,
      "loss": 1.8542,
      "step": 25000
    },
    {
      "epoch": 4.718867924528302,
      "grad_norm": 0.8195838928222656,
      "learning_rate": 2.8132075471698116e-05,
      "loss": 1.8979,
      "step": 25010
    },
    {
      "epoch": 4.720754716981132,
      "grad_norm": 0.8476022481918335,
      "learning_rate": 2.7943396226415095e-05,
      "loss": 1.8268,
      "step": 25020
    },
    {
      "epoch": 4.722641509433962,
      "grad_norm": 0.9282123446464539,
      "learning_rate": 2.7754716981132077e-05,
      "loss": 1.8949,
      "step": 25030
    },
    {
      "epoch": 4.724528301886792,
      "grad_norm": 0.7618269920349121,
      "learning_rate": 2.7566037735849056e-05,
      "loss": 1.8074,
      "step": 25040
    },
    {
      "epoch": 4.726415094339623,
      "grad_norm": 0.8344789147377014,
      "learning_rate": 2.7377358490566038e-05,
      "loss": 1.817,
      "step": 25050
    },
    {
      "epoch": 4.728301886792453,
      "grad_norm": 0.768207311630249,
      "learning_rate": 2.718867924528302e-05,
      "loss": 1.7969,
      "step": 25060
    },
    {
      "epoch": 4.730188679245283,
      "grad_norm": 0.8071860671043396,
      "learning_rate": 2.7e-05,
      "loss": 1.8583,
      "step": 25070
    },
    {
      "epoch": 4.7320754716981135,
      "grad_norm": 0.7969166040420532,
      "learning_rate": 2.681132075471698e-05,
      "loss": 1.8339,
      "step": 25080
    },
    {
      "epoch": 4.733962264150943,
      "grad_norm": 0.8658750653266907,
      "learning_rate": 2.6622641509433963e-05,
      "loss": 1.7662,
      "step": 25090
    },
    {
      "epoch": 4.735849056603773,
      "grad_norm": 0.8783439993858337,
      "learning_rate": 2.6433962264150942e-05,
      "loss": 1.7558,
      "step": 25100
    },
    {
      "epoch": 4.7377358490566035,
      "grad_norm": 0.892192542552948,
      "learning_rate": 2.6245283018867928e-05,
      "loss": 1.9413,
      "step": 25110
    },
    {
      "epoch": 4.739622641509434,
      "grad_norm": 0.9117266535758972,
      "learning_rate": 2.6056603773584906e-05,
      "loss": 1.8277,
      "step": 25120
    },
    {
      "epoch": 4.741509433962264,
      "grad_norm": 0.8262156844139099,
      "learning_rate": 2.5867924528301885e-05,
      "loss": 1.8453,
      "step": 25130
    },
    {
      "epoch": 4.743396226415094,
      "grad_norm": 0.7578004598617554,
      "learning_rate": 2.567924528301887e-05,
      "loss": 1.8656,
      "step": 25140
    },
    {
      "epoch": 4.745283018867925,
      "grad_norm": 0.9347866177558899,
      "learning_rate": 2.549056603773585e-05,
      "loss": 1.8772,
      "step": 25150
    },
    {
      "epoch": 4.747169811320755,
      "grad_norm": 0.897415041923523,
      "learning_rate": 2.5301886792452832e-05,
      "loss": 1.8055,
      "step": 25160
    },
    {
      "epoch": 4.749056603773585,
      "grad_norm": 0.8841238021850586,
      "learning_rate": 2.511320754716981e-05,
      "loss": 1.8356,
      "step": 25170
    },
    {
      "epoch": 4.750943396226415,
      "grad_norm": 0.8177404403686523,
      "learning_rate": 2.4924528301886793e-05,
      "loss": 1.9049,
      "step": 25180
    },
    {
      "epoch": 4.752830188679245,
      "grad_norm": 0.8903051614761353,
      "learning_rate": 2.4735849056603775e-05,
      "loss": 1.8834,
      "step": 25190
    },
    {
      "epoch": 4.754716981132075,
      "grad_norm": 1.0455268621444702,
      "learning_rate": 2.4547169811320754e-05,
      "loss": 1.8254,
      "step": 25200
    },
    {
      "epoch": 4.756603773584906,
      "grad_norm": 0.7925352454185486,
      "learning_rate": 2.435849056603774e-05,
      "loss": 1.8069,
      "step": 25210
    },
    {
      "epoch": 4.758490566037736,
      "grad_norm": 0.7626885175704956,
      "learning_rate": 2.4169811320754718e-05,
      "loss": 1.8415,
      "step": 25220
    },
    {
      "epoch": 4.760377358490566,
      "grad_norm": 0.8564548492431641,
      "learning_rate": 2.3981132075471697e-05,
      "loss": 1.8269,
      "step": 25230
    },
    {
      "epoch": 4.7622641509433965,
      "grad_norm": 0.7298012971878052,
      "learning_rate": 2.3792452830188682e-05,
      "loss": 1.8822,
      "step": 25240
    },
    {
      "epoch": 4.764150943396227,
      "grad_norm": 0.7756919264793396,
      "learning_rate": 2.360377358490566e-05,
      "loss": 1.8048,
      "step": 25250
    },
    {
      "epoch": 4.766037735849057,
      "grad_norm": 0.8873265981674194,
      "learning_rate": 2.341509433962264e-05,
      "loss": 1.8803,
      "step": 25260
    },
    {
      "epoch": 4.7679245283018865,
      "grad_norm": 0.863833487033844,
      "learning_rate": 2.3226415094339622e-05,
      "loss": 1.8689,
      "step": 25270
    },
    {
      "epoch": 4.769811320754717,
      "grad_norm": 0.9063484072685242,
      "learning_rate": 2.3037735849056604e-05,
      "loss": 1.8795,
      "step": 25280
    },
    {
      "epoch": 4.771698113207547,
      "grad_norm": 0.7634154558181763,
      "learning_rate": 2.2849056603773586e-05,
      "loss": 1.8743,
      "step": 25290
    },
    {
      "epoch": 4.773584905660377,
      "grad_norm": 0.8049344420433044,
      "learning_rate": 2.2660377358490565e-05,
      "loss": 1.842,
      "step": 25300
    },
    {
      "epoch": 4.775471698113208,
      "grad_norm": 0.8097347617149353,
      "learning_rate": 2.2471698113207547e-05,
      "loss": 1.7395,
      "step": 25310
    },
    {
      "epoch": 4.777358490566038,
      "grad_norm": 0.7978386282920837,
      "learning_rate": 2.228301886792453e-05,
      "loss": 1.8064,
      "step": 25320
    },
    {
      "epoch": 4.779245283018868,
      "grad_norm": 0.879819393157959,
      "learning_rate": 2.209433962264151e-05,
      "loss": 1.8685,
      "step": 25330
    },
    {
      "epoch": 4.781132075471698,
      "grad_norm": 0.8289180994033813,
      "learning_rate": 2.1905660377358494e-05,
      "loss": 1.8945,
      "step": 25340
    },
    {
      "epoch": 4.783018867924528,
      "grad_norm": 0.8255693912506104,
      "learning_rate": 2.1716981132075473e-05,
      "loss": 1.748,
      "step": 25350
    },
    {
      "epoch": 4.784905660377358,
      "grad_norm": 0.8332407474517822,
      "learning_rate": 2.152830188679245e-05,
      "loss": 1.8664,
      "step": 25360
    },
    {
      "epoch": 4.786792452830189,
      "grad_norm": 0.7904760837554932,
      "learning_rate": 2.1339622641509437e-05,
      "loss": 1.8634,
      "step": 25370
    },
    {
      "epoch": 4.788679245283019,
      "grad_norm": 0.8387383818626404,
      "learning_rate": 2.1150943396226416e-05,
      "loss": 1.8454,
      "step": 25380
    },
    {
      "epoch": 4.790566037735849,
      "grad_norm": 0.8800026774406433,
      "learning_rate": 2.0962264150943398e-05,
      "loss": 1.8148,
      "step": 25390
    },
    {
      "epoch": 4.7924528301886795,
      "grad_norm": 0.9665449857711792,
      "learning_rate": 2.0773584905660377e-05,
      "loss": 1.8873,
      "step": 25400
    },
    {
      "epoch": 4.79433962264151,
      "grad_norm": 0.9088499546051025,
      "learning_rate": 2.058490566037736e-05,
      "loss": 1.9181,
      "step": 25410
    },
    {
      "epoch": 4.79622641509434,
      "grad_norm": 0.8447815179824829,
      "learning_rate": 2.039622641509434e-05,
      "loss": 1.9024,
      "step": 25420
    },
    {
      "epoch": 4.7981132075471695,
      "grad_norm": 0.8539092540740967,
      "learning_rate": 2.020754716981132e-05,
      "loss": 1.87,
      "step": 25430
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.8367786407470703,
      "learning_rate": 2.0018867924528302e-05,
      "loss": 1.8682,
      "step": 25440
    },
    {
      "epoch": 4.80188679245283,
      "grad_norm": 0.8726165890693665,
      "learning_rate": 1.9830188679245284e-05,
      "loss": 1.7839,
      "step": 25450
    },
    {
      "epoch": 4.80377358490566,
      "grad_norm": 0.8480677604675293,
      "learning_rate": 1.9641509433962263e-05,
      "loss": 1.7906,
      "step": 25460
    },
    {
      "epoch": 4.805660377358491,
      "grad_norm": 0.7666023373603821,
      "learning_rate": 1.945283018867925e-05,
      "loss": 1.8389,
      "step": 25470
    },
    {
      "epoch": 4.807547169811321,
      "grad_norm": 0.8808642625808716,
      "learning_rate": 1.9264150943396227e-05,
      "loss": 1.808,
      "step": 25480
    },
    {
      "epoch": 4.809433962264151,
      "grad_norm": 0.8284617066383362,
      "learning_rate": 1.9075471698113206e-05,
      "loss": 1.7276,
      "step": 25490
    },
    {
      "epoch": 4.811320754716981,
      "grad_norm": 0.8363348245620728,
      "learning_rate": 1.888679245283019e-05,
      "loss": 1.894,
      "step": 25500
    },
    {
      "epoch": 4.813207547169811,
      "grad_norm": 0.7385274767875671,
      "learning_rate": 1.869811320754717e-05,
      "loss": 1.8639,
      "step": 25510
    },
    {
      "epoch": 4.815094339622641,
      "grad_norm": 1.0050822496414185,
      "learning_rate": 1.8509433962264153e-05,
      "loss": 1.881,
      "step": 25520
    },
    {
      "epoch": 4.816981132075472,
      "grad_norm": 0.793065071105957,
      "learning_rate": 1.832075471698113e-05,
      "loss": 1.8213,
      "step": 25530
    },
    {
      "epoch": 4.818867924528302,
      "grad_norm": 0.9663183093070984,
      "learning_rate": 1.8132075471698114e-05,
      "loss": 1.8781,
      "step": 25540
    },
    {
      "epoch": 4.820754716981132,
      "grad_norm": 0.8046610355377197,
      "learning_rate": 1.7943396226415096e-05,
      "loss": 1.8697,
      "step": 25550
    },
    {
      "epoch": 4.8226415094339625,
      "grad_norm": 0.8941308259963989,
      "learning_rate": 1.7754716981132075e-05,
      "loss": 1.9083,
      "step": 25560
    },
    {
      "epoch": 4.824528301886793,
      "grad_norm": 0.812618613243103,
      "learning_rate": 1.7566037735849057e-05,
      "loss": 1.8378,
      "step": 25570
    },
    {
      "epoch": 4.826415094339623,
      "grad_norm": 0.8862343430519104,
      "learning_rate": 1.737735849056604e-05,
      "loss": 1.8034,
      "step": 25580
    },
    {
      "epoch": 4.8283018867924525,
      "grad_norm": 0.7873861789703369,
      "learning_rate": 1.7188679245283018e-05,
      "loss": 1.8252,
      "step": 25590
    },
    {
      "epoch": 4.830188679245283,
      "grad_norm": 0.8257601261138916,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.8167,
      "step": 25600
    },
    {
      "epoch": 4.832075471698113,
      "grad_norm": 0.8425606489181519,
      "learning_rate": 1.6811320754716982e-05,
      "loss": 1.7994,
      "step": 25610
    },
    {
      "epoch": 4.833962264150943,
      "grad_norm": 0.7694969177246094,
      "learning_rate": 1.662264150943396e-05,
      "loss": 1.8482,
      "step": 25620
    },
    {
      "epoch": 4.835849056603774,
      "grad_norm": 0.8048649430274963,
      "learning_rate": 1.6433962264150943e-05,
      "loss": 1.873,
      "step": 25630
    },
    {
      "epoch": 4.837735849056604,
      "grad_norm": 0.8008745312690735,
      "learning_rate": 1.6245283018867925e-05,
      "loss": 1.9156,
      "step": 25640
    },
    {
      "epoch": 4.839622641509434,
      "grad_norm": 0.9098978042602539,
      "learning_rate": 1.6056603773584907e-05,
      "loss": 1.8217,
      "step": 25650
    },
    {
      "epoch": 4.841509433962264,
      "grad_norm": 0.8147541284561157,
      "learning_rate": 1.5867924528301886e-05,
      "loss": 1.8802,
      "step": 25660
    },
    {
      "epoch": 4.843396226415094,
      "grad_norm": 0.9093730449676514,
      "learning_rate": 1.5679245283018868e-05,
      "loss": 1.9482,
      "step": 25670
    },
    {
      "epoch": 4.845283018867924,
      "grad_norm": 0.792123019695282,
      "learning_rate": 1.549056603773585e-05,
      "loss": 1.8193,
      "step": 25680
    },
    {
      "epoch": 4.847169811320755,
      "grad_norm": 0.9375708103179932,
      "learning_rate": 1.530188679245283e-05,
      "loss": 1.8714,
      "step": 25690
    },
    {
      "epoch": 4.849056603773585,
      "grad_norm": 0.89569491147995,
      "learning_rate": 1.5113207547169811e-05,
      "loss": 1.7292,
      "step": 25700
    },
    {
      "epoch": 4.850943396226415,
      "grad_norm": 0.8573721647262573,
      "learning_rate": 1.4924528301886794e-05,
      "loss": 1.7906,
      "step": 25710
    },
    {
      "epoch": 4.8528301886792455,
      "grad_norm": 0.8895979523658752,
      "learning_rate": 1.4735849056603774e-05,
      "loss": 1.8461,
      "step": 25720
    },
    {
      "epoch": 4.854716981132076,
      "grad_norm": 0.9102539420127869,
      "learning_rate": 1.4547169811320755e-05,
      "loss": 1.8415,
      "step": 25730
    },
    {
      "epoch": 4.856603773584906,
      "grad_norm": 0.8592724204063416,
      "learning_rate": 1.4358490566037737e-05,
      "loss": 1.8407,
      "step": 25740
    },
    {
      "epoch": 4.8584905660377355,
      "grad_norm": 0.8296239376068115,
      "learning_rate": 1.4169811320754717e-05,
      "loss": 1.859,
      "step": 25750
    },
    {
      "epoch": 4.860377358490566,
      "grad_norm": 0.8181354999542236,
      "learning_rate": 1.39811320754717e-05,
      "loss": 1.7459,
      "step": 25760
    },
    {
      "epoch": 4.862264150943396,
      "grad_norm": 0.7771655917167664,
      "learning_rate": 1.3792452830188678e-05,
      "loss": 1.8761,
      "step": 25770
    },
    {
      "epoch": 4.8641509433962264,
      "grad_norm": 0.8675657510757446,
      "learning_rate": 1.360377358490566e-05,
      "loss": 1.7665,
      "step": 25780
    },
    {
      "epoch": 4.866037735849057,
      "grad_norm": 0.8105357885360718,
      "learning_rate": 1.3415094339622642e-05,
      "loss": 1.8887,
      "step": 25790
    },
    {
      "epoch": 4.867924528301887,
      "grad_norm": 0.9104196429252625,
      "learning_rate": 1.3226415094339623e-05,
      "loss": 1.8773,
      "step": 25800
    },
    {
      "epoch": 4.869811320754717,
      "grad_norm": 0.9352724552154541,
      "learning_rate": 1.3037735849056605e-05,
      "loss": 1.7858,
      "step": 25810
    },
    {
      "epoch": 4.871698113207547,
      "grad_norm": 0.9086334109306335,
      "learning_rate": 1.2849056603773584e-05,
      "loss": 1.8703,
      "step": 25820
    },
    {
      "epoch": 4.873584905660377,
      "grad_norm": 0.9241464734077454,
      "learning_rate": 1.2660377358490566e-05,
      "loss": 1.8201,
      "step": 25830
    },
    {
      "epoch": 4.875471698113207,
      "grad_norm": 0.8308390974998474,
      "learning_rate": 1.2471698113207548e-05,
      "loss": 1.8315,
      "step": 25840
    },
    {
      "epoch": 4.877358490566038,
      "grad_norm": 0.8501255512237549,
      "learning_rate": 1.2283018867924529e-05,
      "loss": 1.8753,
      "step": 25850
    },
    {
      "epoch": 4.879245283018868,
      "grad_norm": 0.8888262510299683,
      "learning_rate": 1.209433962264151e-05,
      "loss": 1.9153,
      "step": 25860
    },
    {
      "epoch": 4.881132075471698,
      "grad_norm": 0.8735302686691284,
      "learning_rate": 1.190566037735849e-05,
      "loss": 1.756,
      "step": 25870
    },
    {
      "epoch": 4.8830188679245285,
      "grad_norm": 0.7994402647018433,
      "learning_rate": 1.1716981132075472e-05,
      "loss": 1.8192,
      "step": 25880
    },
    {
      "epoch": 4.884905660377359,
      "grad_norm": 0.7877300977706909,
      "learning_rate": 1.1528301886792454e-05,
      "loss": 1.8086,
      "step": 25890
    },
    {
      "epoch": 4.886792452830189,
      "grad_norm": 0.782275378704071,
      "learning_rate": 1.1339622641509435e-05,
      "loss": 1.8284,
      "step": 25900
    },
    {
      "epoch": 4.888679245283019,
      "grad_norm": 0.8432804346084595,
      "learning_rate": 1.1150943396226415e-05,
      "loss": 1.8962,
      "step": 25910
    },
    {
      "epoch": 4.890566037735849,
      "grad_norm": 0.8031221032142639,
      "learning_rate": 1.0962264150943395e-05,
      "loss": 1.7538,
      "step": 25920
    },
    {
      "epoch": 4.892452830188679,
      "grad_norm": 0.7670063376426697,
      "learning_rate": 1.0773584905660378e-05,
      "loss": 1.7797,
      "step": 25930
    },
    {
      "epoch": 4.8943396226415095,
      "grad_norm": 0.8622255921363831,
      "learning_rate": 1.058490566037736e-05,
      "loss": 1.7666,
      "step": 25940
    },
    {
      "epoch": 4.89622641509434,
      "grad_norm": 0.950615406036377,
      "learning_rate": 1.0396226415094339e-05,
      "loss": 1.8572,
      "step": 25950
    },
    {
      "epoch": 4.89811320754717,
      "grad_norm": 0.8477962613105774,
      "learning_rate": 1.020754716981132e-05,
      "loss": 1.917,
      "step": 25960
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.9009524583816528,
      "learning_rate": 1.0018867924528301e-05,
      "loss": 1.8211,
      "step": 25970
    },
    {
      "epoch": 4.90188679245283,
      "grad_norm": 0.8065115809440613,
      "learning_rate": 9.830188679245283e-06,
      "loss": 1.8335,
      "step": 25980
    },
    {
      "epoch": 4.90377358490566,
      "grad_norm": 0.8685621619224548,
      "learning_rate": 9.641509433962266e-06,
      "loss": 1.8087,
      "step": 25990
    },
    {
      "epoch": 4.90566037735849,
      "grad_norm": 0.8343715667724609,
      "learning_rate": 9.452830188679244e-06,
      "loss": 1.8104,
      "step": 26000
    },
    {
      "epoch": 4.907547169811321,
      "grad_norm": 0.8847424387931824,
      "learning_rate": 9.264150943396227e-06,
      "loss": 1.7976,
      "step": 26010
    },
    {
      "epoch": 4.909433962264151,
      "grad_norm": 0.8419348001480103,
      "learning_rate": 9.075471698113209e-06,
      "loss": 1.7867,
      "step": 26020
    },
    {
      "epoch": 4.911320754716981,
      "grad_norm": 0.8679512143135071,
      "learning_rate": 8.88679245283019e-06,
      "loss": 1.8125,
      "step": 26030
    },
    {
      "epoch": 4.913207547169812,
      "grad_norm": 0.84597247838974,
      "learning_rate": 8.69811320754717e-06,
      "loss": 1.7564,
      "step": 26040
    },
    {
      "epoch": 4.915094339622642,
      "grad_norm": 0.7901466488838196,
      "learning_rate": 8.50943396226415e-06,
      "loss": 1.9036,
      "step": 26050
    },
    {
      "epoch": 4.916981132075472,
      "grad_norm": 0.7565051913261414,
      "learning_rate": 8.320754716981132e-06,
      "loss": 1.8298,
      "step": 26060
    },
    {
      "epoch": 4.918867924528302,
      "grad_norm": 0.8806421160697937,
      "learning_rate": 8.132075471698114e-06,
      "loss": 1.8893,
      "step": 26070
    },
    {
      "epoch": 4.920754716981132,
      "grad_norm": 0.9168633222579956,
      "learning_rate": 7.943396226415095e-06,
      "loss": 1.834,
      "step": 26080
    },
    {
      "epoch": 4.922641509433962,
      "grad_norm": 0.7739551067352295,
      "learning_rate": 7.754716981132075e-06,
      "loss": 1.872,
      "step": 26090
    },
    {
      "epoch": 4.9245283018867925,
      "grad_norm": 0.7848264575004578,
      "learning_rate": 7.566037735849057e-06,
      "loss": 1.8691,
      "step": 26100
    },
    {
      "epoch": 4.926415094339623,
      "grad_norm": 0.826474130153656,
      "learning_rate": 7.377358490566038e-06,
      "loss": 1.91,
      "step": 26110
    },
    {
      "epoch": 4.928301886792453,
      "grad_norm": 0.8357343077659607,
      "learning_rate": 7.1886792452830186e-06,
      "loss": 1.944,
      "step": 26120
    },
    {
      "epoch": 4.930188679245283,
      "grad_norm": 0.9551620483398438,
      "learning_rate": 7e-06,
      "loss": 1.8523,
      "step": 26130
    },
    {
      "epoch": 4.932075471698113,
      "grad_norm": 0.9044644236564636,
      "learning_rate": 6.811320754716981e-06,
      "loss": 1.8711,
      "step": 26140
    },
    {
      "epoch": 4.933962264150943,
      "grad_norm": 0.7792763113975525,
      "learning_rate": 6.6226415094339625e-06,
      "loss": 1.8278,
      "step": 26150
    },
    {
      "epoch": 4.935849056603773,
      "grad_norm": 0.7529667615890503,
      "learning_rate": 6.433962264150943e-06,
      "loss": 1.8852,
      "step": 26160
    },
    {
      "epoch": 4.937735849056604,
      "grad_norm": 0.7886598706245422,
      "learning_rate": 6.245283018867924e-06,
      "loss": 1.8504,
      "step": 26170
    },
    {
      "epoch": 4.939622641509434,
      "grad_norm": 0.8611592650413513,
      "learning_rate": 6.0566037735849065e-06,
      "loss": 1.8257,
      "step": 26180
    },
    {
      "epoch": 4.941509433962264,
      "grad_norm": 0.927436888217926,
      "learning_rate": 5.867924528301887e-06,
      "loss": 1.8093,
      "step": 26190
    },
    {
      "epoch": 4.943396226415095,
      "grad_norm": 0.7811133861541748,
      "learning_rate": 5.679245283018868e-06,
      "loss": 1.832,
      "step": 26200
    },
    {
      "epoch": 4.945283018867925,
      "grad_norm": 0.8564581274986267,
      "learning_rate": 5.490566037735849e-06,
      "loss": 1.7997,
      "step": 26210
    },
    {
      "epoch": 4.947169811320755,
      "grad_norm": 0.8284679055213928,
      "learning_rate": 5.30188679245283e-06,
      "loss": 1.8704,
      "step": 26220
    },
    {
      "epoch": 4.949056603773585,
      "grad_norm": 0.7844123840332031,
      "learning_rate": 5.1132075471698114e-06,
      "loss": 1.7938,
      "step": 26230
    },
    {
      "epoch": 4.950943396226415,
      "grad_norm": 0.8362401127815247,
      "learning_rate": 4.924528301886793e-06,
      "loss": 1.8591,
      "step": 26240
    },
    {
      "epoch": 4.952830188679245,
      "grad_norm": 0.8280125856399536,
      "learning_rate": 4.735849056603773e-06,
      "loss": 1.7673,
      "step": 26250
    },
    {
      "epoch": 4.9547169811320755,
      "grad_norm": 0.8886858224868774,
      "learning_rate": 4.5471698113207546e-06,
      "loss": 1.8513,
      "step": 26260
    },
    {
      "epoch": 4.956603773584906,
      "grad_norm": 0.8083602786064148,
      "learning_rate": 4.358490566037737e-06,
      "loss": 1.8445,
      "step": 26270
    },
    {
      "epoch": 4.958490566037736,
      "grad_norm": 0.9105982184410095,
      "learning_rate": 4.169811320754717e-06,
      "loss": 1.848,
      "step": 26280
    },
    {
      "epoch": 4.960377358490566,
      "grad_norm": 0.9117575287818909,
      "learning_rate": 3.9811320754716985e-06,
      "loss": 1.832,
      "step": 26290
    },
    {
      "epoch": 4.962264150943396,
      "grad_norm": 0.8072582483291626,
      "learning_rate": 3.792452830188679e-06,
      "loss": 1.9147,
      "step": 26300
    },
    {
      "epoch": 4.964150943396226,
      "grad_norm": 0.8904500603675842,
      "learning_rate": 3.6037735849056608e-06,
      "loss": 1.8044,
      "step": 26310
    },
    {
      "epoch": 4.966037735849056,
      "grad_norm": 0.8398637175559998,
      "learning_rate": 3.4150943396226417e-06,
      "loss": 1.8595,
      "step": 26320
    },
    {
      "epoch": 4.967924528301887,
      "grad_norm": 0.8212645053863525,
      "learning_rate": 3.226415094339623e-06,
      "loss": 1.8164,
      "step": 26330
    },
    {
      "epoch": 4.969811320754717,
      "grad_norm": 0.7362319231033325,
      "learning_rate": 3.037735849056604e-06,
      "loss": 1.8969,
      "step": 26340
    },
    {
      "epoch": 4.971698113207547,
      "grad_norm": 0.8410064578056335,
      "learning_rate": 2.849056603773585e-06,
      "loss": 1.8514,
      "step": 26350
    },
    {
      "epoch": 4.973584905660378,
      "grad_norm": 0.8351640701293945,
      "learning_rate": 2.660377358490566e-06,
      "loss": 1.869,
      "step": 26360
    },
    {
      "epoch": 4.975471698113208,
      "grad_norm": 0.8289386630058289,
      "learning_rate": 2.471698113207547e-06,
      "loss": 1.8453,
      "step": 26370
    },
    {
      "epoch": 4.977358490566038,
      "grad_norm": 0.8611980080604553,
      "learning_rate": 2.2830188679245283e-06,
      "loss": 1.9118,
      "step": 26380
    },
    {
      "epoch": 4.979245283018868,
      "grad_norm": 0.8039102554321289,
      "learning_rate": 2.0943396226415092e-06,
      "loss": 1.8848,
      "step": 26390
    },
    {
      "epoch": 4.981132075471698,
      "grad_norm": 0.8956910967826843,
      "learning_rate": 1.9056603773584906e-06,
      "loss": 1.7789,
      "step": 26400
    },
    {
      "epoch": 4.983018867924528,
      "grad_norm": 0.7669048309326172,
      "learning_rate": 1.7169811320754717e-06,
      "loss": 1.8034,
      "step": 26410
    },
    {
      "epoch": 4.9849056603773585,
      "grad_norm": 0.8014086484909058,
      "learning_rate": 1.5283018867924528e-06,
      "loss": 1.7409,
      "step": 26420
    },
    {
      "epoch": 4.986792452830189,
      "grad_norm": 0.8429880142211914,
      "learning_rate": 1.3396226415094341e-06,
      "loss": 1.8813,
      "step": 26430
    },
    {
      "epoch": 4.988679245283019,
      "grad_norm": 0.9116052389144897,
      "learning_rate": 1.1509433962264152e-06,
      "loss": 1.802,
      "step": 26440
    },
    {
      "epoch": 4.990566037735849,
      "grad_norm": 0.8488928079605103,
      "learning_rate": 9.622641509433963e-07,
      "loss": 1.8742,
      "step": 26450
    },
    {
      "epoch": 4.992452830188679,
      "grad_norm": 1.001542329788208,
      "learning_rate": 7.735849056603774e-07,
      "loss": 1.8555,
      "step": 26460
    },
    {
      "epoch": 4.994339622641509,
      "grad_norm": 0.8162131309509277,
      "learning_rate": 5.849056603773586e-07,
      "loss": 1.8909,
      "step": 26470
    },
    {
      "epoch": 4.996226415094339,
      "grad_norm": 0.8160637617111206,
      "learning_rate": 3.9622641509433963e-07,
      "loss": 1.8684,
      "step": 26480
    },
    {
      "epoch": 4.99811320754717,
      "grad_norm": 0.8421993255615234,
      "learning_rate": 2.0754716981132074e-07,
      "loss": 1.9045,
      "step": 26490
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.6669981479644775,
      "learning_rate": 1.8867924528301887e-08,
      "loss": 1.9329,
      "step": 26500
    }
  ],
  "logging_steps": 10,
  "max_steps": 26500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6947263801589760.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
